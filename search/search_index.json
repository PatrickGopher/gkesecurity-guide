{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Unofficial Google Kubernetes Engine (GKE) Security Guide Introduction Disclaimer This guide is not an official guide, is not endorsed by Google , and is comprised entirely from publicly available information. The information is provided \"as-is\", without warranty or fit for a particular purpose. You should fully understand the security implications of each recommendation before applying it in your environment. There are several books on the topic of getting Kubernetes Up and Running and even one specific to Kubernetes Security . If you are deploying in multiple platform environments like on-premise or across multiple cloud providers, those resources will be very useful. However, if you are deploying GKE clusters in GCP, you'll notice that there are a wide array of GCP and GKE-specific choices that need to be made. The goal of this guide is to help you prioritize and implement a security posture that meets your organization's needs while taking advantage of all the benefits of the GKE service. For each control and approach, the guide will attempt to help you understand the relative risks, relative likelihood, upfront cost and complexity, and ongoing cost and complexity of each security-related decision. Deep links to official GCP/GKE and Kubernetes documentation will be provided to guide further reading. Finally, links to hand-selected tools, books, and talk recordings will give additional support to the concepts for further exploration and understanding. Audience This guide is focused specifically on the following roles working with a GKE deployment: Operations - Those responsible for deploying and maintaining the GCP Project, Networking, and GKE Cluster lifecycle. Security - Those responsible for ensuring the operational environment adequately meets the organization's security standards and posture. Security-minded developers - Those deploying operational workloads into the GKE cluster looking to follow security best practices. Team Leads - Those managing teams implementing GKE clusters and responsible for prioritizing security with operational velocity. Structure of this Guide As each layer of a system builds upon the layer underneath, this guide begins with the foundation of all GCP services and explains how the structure of folders and projects in a GCP organization can help organize and guide security settings and permissions management. Next, it covers the types of network configurations that support GKE clusters for different isolation strategies. The core of the guide gets into cluster configuration, lifecycle and operations, observability, and the management of custom additions in the form of addons. With a solid base in place, the guide covers the deployment and configuration of workloads and container images in terms of security best practices. Finally, it covers certain security-related workflows such as auditing for conformance and handling security issues when an incident occurs. Each topic area will cover: A few sentences to paragraphs on what the topic or setting is and does. Why the topic or setting is important and the risks associated with it. The versions it applies to, where applicable. When you might want to prioritize adoption or implementation. The upfront cost and complexity of deploying the change or approach. The longer term cost and complexity of maintaining the system with this change or approach implemented. GKE vs Building Kubernetes Yourself For first time Kubernetes operators, it's a fantastic idea to follow Kubernetes the Hard Way by Kelsey Hightower and build a cluster from scratch. It will give you a deep understanding of the components, their configuration settings, and an appreciation for the complexity of the system. The very next thing you should do is leverage an approach to Kubernetes that ensures you and your team do as little of that work as possible. In many cases, this is best done by working with a managed Kubernetes service like GKE . Successful Kubernetes cluster deployments require solid execution in wide range of areas, and it's impossible to be execellent in all of them at once. From a business standpoint, you want your teams working on the problems that matter to your organization and to leave the boring operational work to the cloud service provider. Kubernetes version upgrades, security patches, operating system maintenance, logging and monitoring infrastructure, and more are all things you want to be building \"on top of\" and not having to \"build\" yourself first. Refer to the benefits of GKE for further reading. Kubernetes Security Maturity Throughout this guide, there will be dozens of security decisions and configuration tasks that you will be faced with prioritizing and implementing. It's important to have some higher level structure and frame of reference for which items are required early or later on in your journey to Kubernetes security maturity. Kubernetes for Enterprise Security Requirements really helps visually explain the progression path well. Summarizing the key areas from this talk: Infrastructure Security - Does the surrounding environment and Kubernetes cluster provide a solid foundation for running workloads securely? Software Supply Chain - Are the container images running in the cluster built from trusted sources and free of vulnerabilities? Container Runtime Security - Are there capabilities in place to monitor what is happening inside the containers? Additionally: Security Observability and Response - Are there capabilities for knowing when a security incident occurs and processes in place to respond and remediate? As you progress through this guide, each of the topic areas should map back to one or more of these key areas and help align your decision along these lines. Contributing This guide is a living document, and contributions in the form of issues and PRs are welcomed. If you are considering writing new content, please open an issue outlining what you'd like to write about, where it might fit in, and other details. If you found this guide useful, please consider donating your time attending and supporting your local cloud-native and Kubernetes-related meetups. The success or failure of Kubernetes and the CNCF ecosystem is largely dependent on you and how you help elevate others with your compassion, assistance, and inclusivity. About the Author(s) @BradGeesaman is an Independent Security Consultant currently focused on helping teams secure their GKE clusters in harmony with their desired operational velocity.","title":"Introduction"},{"location":"#the-unofficial-google-kubernetes-engine-gke-security-guide","text":"","title":"The Unofficial Google Kubernetes Engine (GKE) Security Guide"},{"location":"#introduction","text":"Disclaimer This guide is not an official guide, is not endorsed by Google , and is comprised entirely from publicly available information. The information is provided \"as-is\", without warranty or fit for a particular purpose. You should fully understand the security implications of each recommendation before applying it in your environment. There are several books on the topic of getting Kubernetes Up and Running and even one specific to Kubernetes Security . If you are deploying in multiple platform environments like on-premise or across multiple cloud providers, those resources will be very useful. However, if you are deploying GKE clusters in GCP, you'll notice that there are a wide array of GCP and GKE-specific choices that need to be made. The goal of this guide is to help you prioritize and implement a security posture that meets your organization's needs while taking advantage of all the benefits of the GKE service. For each control and approach, the guide will attempt to help you understand the relative risks, relative likelihood, upfront cost and complexity, and ongoing cost and complexity of each security-related decision. Deep links to official GCP/GKE and Kubernetes documentation will be provided to guide further reading. Finally, links to hand-selected tools, books, and talk recordings will give additional support to the concepts for further exploration and understanding.","title":"Introduction"},{"location":"#audience","text":"This guide is focused specifically on the following roles working with a GKE deployment: Operations - Those responsible for deploying and maintaining the GCP Project, Networking, and GKE Cluster lifecycle. Security - Those responsible for ensuring the operational environment adequately meets the organization's security standards and posture. Security-minded developers - Those deploying operational workloads into the GKE cluster looking to follow security best practices. Team Leads - Those managing teams implementing GKE clusters and responsible for prioritizing security with operational velocity.","title":"Audience"},{"location":"#structure-of-this-guide","text":"As each layer of a system builds upon the layer underneath, this guide begins with the foundation of all GCP services and explains how the structure of folders and projects in a GCP organization can help organize and guide security settings and permissions management. Next, it covers the types of network configurations that support GKE clusters for different isolation strategies. The core of the guide gets into cluster configuration, lifecycle and operations, observability, and the management of custom additions in the form of addons. With a solid base in place, the guide covers the deployment and configuration of workloads and container images in terms of security best practices. Finally, it covers certain security-related workflows such as auditing for conformance and handling security issues when an incident occurs. Each topic area will cover: A few sentences to paragraphs on what the topic or setting is and does. Why the topic or setting is important and the risks associated with it. The versions it applies to, where applicable. When you might want to prioritize adoption or implementation. The upfront cost and complexity of deploying the change or approach. The longer term cost and complexity of maintaining the system with this change or approach implemented.","title":"Structure of this Guide"},{"location":"#gke-vs-building-kubernetes-yourself","text":"For first time Kubernetes operators, it's a fantastic idea to follow Kubernetes the Hard Way by Kelsey Hightower and build a cluster from scratch. It will give you a deep understanding of the components, their configuration settings, and an appreciation for the complexity of the system. The very next thing you should do is leverage an approach to Kubernetes that ensures you and your team do as little of that work as possible. In many cases, this is best done by working with a managed Kubernetes service like GKE . Successful Kubernetes cluster deployments require solid execution in wide range of areas, and it's impossible to be execellent in all of them at once. From a business standpoint, you want your teams working on the problems that matter to your organization and to leave the boring operational work to the cloud service provider. Kubernetes version upgrades, security patches, operating system maintenance, logging and monitoring infrastructure, and more are all things you want to be building \"on top of\" and not having to \"build\" yourself first. Refer to the benefits of GKE for further reading.","title":"GKE vs Building Kubernetes Yourself"},{"location":"#kubernetes-security-maturity","text":"Throughout this guide, there will be dozens of security decisions and configuration tasks that you will be faced with prioritizing and implementing. It's important to have some higher level structure and frame of reference for which items are required early or later on in your journey to Kubernetes security maturity. Kubernetes for Enterprise Security Requirements really helps visually explain the progression path well. Summarizing the key areas from this talk: Infrastructure Security - Does the surrounding environment and Kubernetes cluster provide a solid foundation for running workloads securely? Software Supply Chain - Are the container images running in the cluster built from trusted sources and free of vulnerabilities? Container Runtime Security - Are there capabilities in place to monitor what is happening inside the containers? Additionally: Security Observability and Response - Are there capabilities for knowing when a security incident occurs and processes in place to respond and remediate? As you progress through this guide, each of the topic areas should map back to one or more of these key areas and help align your decision along these lines.","title":"Kubernetes Security Maturity"},{"location":"#contributing","text":"This guide is a living document, and contributions in the form of issues and PRs are welcomed. If you are considering writing new content, please open an issue outlining what you'd like to write about, where it might fit in, and other details. If you found this guide useful, please consider donating your time attending and supporting your local cloud-native and Kubernetes-related meetups. The success or failure of Kubernetes and the CNCF ecosystem is largely dependent on you and how you help elevate others with your compassion, assistance, and inclusivity.","title":"Contributing"},{"location":"#about-the-authors","text":"@BradGeesaman is an Independent Security Consultant currently focused on helping teams secure their GKE clusters in harmony with their desired operational velocity.","title":"About the Author(s)"},{"location":"audit_and_compliance/","text":"Audit and Compliance CIS Benchmarking Inspec-GCP and Inspec-K8s Cloud Asset Inventory Forseti Security CSCC Common Misconfigurations","title":"Audit and Compliance"},{"location":"audit_and_compliance/#audit-and-compliance","text":"","title":"Audit and Compliance"},{"location":"audit_and_compliance/#cis-benchmarking","text":"","title":"CIS Benchmarking"},{"location":"audit_and_compliance/#inspec-gcp-and-inspec-k8s","text":"","title":"Inspec-GCP and Inspec-K8s"},{"location":"audit_and_compliance/#cloud-asset-inventory","text":"","title":"Cloud Asset Inventory"},{"location":"audit_and_compliance/#forseti-security","text":"","title":"Forseti Security"},{"location":"audit_and_compliance/#cscc","text":"","title":"CSCC"},{"location":"audit_and_compliance/#common-misconfigurations","text":"","title":"Common Misconfigurations"},{"location":"cloud_environment/","text":"Cloud Environment Identity and IAM Hierarchy At the foundation of your Google Cloud Platform (GCP) organization is a relationship with your trusted identity provider. Commonly, this is a Gsuite domain or a Cloud Identity installation. It's the basis of trust for how users and groups are managed throughout the GCP environments. It's where you audit logins/logouts, enforce MFA and session duration, and more. It's important to understand that in a GCP organization , where a user is granted permissions is just as important as what permissions are granted. For example, granting a user or service account a binding to an IAM role for Storage Admin at the project level allows that account to have all those permissions for all Google Cloud Storage (GCS) buckets in that project . Consider a somewhat realistic scenario: That project has GCS buckets for storing audit logs, VPC flow logs, and firewall logs. The security team is given Storage Admin permissions to manage these. Also in that same project , a development team wants to store some data used by their application. They too, get assigned Storage Admin permissions to that project . There are a few problems: Unless the security team is the one that made the IAM permissions assignment, they might not be aware of the exposure of their security log and event data. This jeopardizes the integrity of the information if needed for incident response and guaranteeing chain of custody. The development team now has access to this sensitive security information. Even though they may never access it or need to, this violates the principle of least privilege. Any time a new GCS bucket is added to this project , both teams will have access to it automatically. This can often be undesired behavior and tricky to untangle. Things get even more interesting when you consider a group of projects organized into a GCP Folder. Permissions granted to a Folder \"trickle down\" to the Folders and projects inside them. Because permissions are additive , a descendent project can't block or override those permissions. Therefore, careful consideration must be given to access granted at each level, and special attention is needed for IAM roles bound to users/groups/service accounts at the Folder or Organization levels. Some examples to consider: If a user is assigned the primitive role Owner at the Organization level, they will be Owner in every single project in the entire organization. The risk of compromise of those specific credentials (and by extension, that user's primary computing devices) is immediately increased to undesired levels. Granting Viewer at the Organization level allows reading all GCS buckets, all StackDriver logs, and viewing the configurations of most resources. In many cases, sensitive data is found in any or all of those places that can be used to escalate privileges. IAM \"Overcrowding\" The IAM page in the GCP Console tends to get \"crowded\" and harder to visually reason about when there are lots of individual bindings. Each IAM Role binding increases the cognitive load when reviewing permissions. Over time, this can get unwieldy and difficult to consolidate once systems and services are running in production. Best Practices Use a single Identity Provider - If all users and groups are originating from a single domain and are managed centrally, maintaining the proper IAM permissions throughout the GCP Organization is greatly simplified. Ensure all accounts use Multi-Factor Authentication (MFA) - Protecting the theft and misuse of GCP account credentials is the first and most important step to protecting your GCP infrastructure. If a set of credentials is stolen or a laptop lost, the window for those credentials to be valid is greatly reduced. Consolidate Users into Groups - Instead of granting individual users access, consider using a group, placing that user in that group, and granting access to the group. This way, if that person leaves the organization, it doesn't leave orphaned/unused accounts clogging up the IAM permissions listings that have to be manually removed using gcloud/UI. Also, when replacing that person who left, it's a simpler process of just adding the new person(s) to the same groups. No gcloud/UI changes are then necessary. Follow the Principle of Least Privilege - Take care when granting access Project-wide and even greater care when granting access at the Folder and Organization levels. There are often unintended consequences of over-granting permissions. Additionally, it's much harder to safely remove that binding as time goes on because inherited permissions might be required in a descendent project. Resources Creating and Managing GCP Organizations GCP Resource Hierarchy GSuite Security Cloud Identity Overview Cloud IAM Overview Organization, Folder, and Project Structure Resource hierarchy decisions made early on in your GCP journey tend to have a \"inertia\". Meaning, once they are configured a certain way, they become entrenched and harder to change as time goes on. If configured well, they can reduce administrative and security toil in managing resources and access to those resources in a clean, manageable way. If configured sub-optimally, you might feel like making administrative changes is overly complex and burdensome. Often, these decisions are often made in haste during design and implementation efforts \"just to get things working\". It might seem that there is pressure to get things perfect from the outset, but there is some level of built-in flexibility that you can take advantage of -- provided certain steps are taken. The two components of the GCP resource hierarchy that are trickiest to change are the organization and the non-empty project . The organization is the root of all resources, so it establishes the hierarchy. Projects cannot be renamed, and resources inside projects cannot be moved outside of projects . Folders can be made up to three levels deep, and projects can be moved around inside folders as needed. What this means: Permissions assigned at the organization level descend to everything below regardless of folder and project structure. Renaming projects or moving resources out of a project requires migration of resources, so careful thought to their name and what runs inside them is needed. Folders are a powerful tool in designing and adapting your IAM permissons hierarchy as needs change. Consider the following diagram of a GCP Organization: The organization is named example.com , and that is the root of the IAM hierarchy. Skipping over the folders tier for a second -- the projects are organized by operational environment which is very common and a best practice for keeping resources completely separated. Resources in the example-dev project have no access by default to example-test or example-prod . \"Fantastic!\" you might say, and copy this entire approach. However, depending on your organization's needs, this hierarchy might not be ideal! Let's break down the potential issues with the folders and the resources inside each project : In the example-dev project, instance_a and service_a may or may not be part of the same overall \"service\". If they both operate on data in bucket_a , for example, this makes sense. But if instance_a is managed by a separate team and shares nothing with service_a or bucket_a , you have a permissions scoping and \"attack blast radius\" problem. Permissions assigned on the example-dev project might be shared in undesirable ways between instance_a and service_a . In the event of a security compromise, attackers will be much more likely to obtain credentials with shared permissions and access nearby resources inside that same project. Finally, unless additional measures are taken with custom log sinks, the logs and metrics in Stackdriver will be going to the same project. Administrators and developers of the instance_a and service_a would be able to see each other's logs and metrics. In the example-test project, two compute instances and two GCS buckets are in place. If bucket_b and bucket_c are part of the same overall service, this makes sense. But if bucket_b serves data with PII in it and bucket_c is where VPC flow logs are stored, this increases the chance of accidental exposure if the Storage Viewer IAM Role is granted at the project level. The service accounts given the ability to read VPC flow logs may be able to read from the PII bucket and vice versa. A simple question to ask is, \"If the credentials for this user/group/service account were stolen without us knowing, what would they have access to?\" A service account with Storage Viewer granted at the example-test project means its credentials, if stolen or misused, would allow viewing the VPC flow logs and PII data in both GCS buckets. At the folder and project level, it may seem reasonable to name the folder after the team name. e.g. team-a-app1-dev . However, in practice, team names and organization charts in the real world tend to change more often than GCP resources. To remain flexible, it's much easier to use GCP groups to combine users into teams and then assign IAM permissions for that group at the desired level. Otherwise, you'll end up having team_a managing the team_b projects which will get very confusing. If your organization becomes large and has lots of geographic regions, a common practice is to have a top-level folder for regions like US , EU , etc. While this places no technical boundary on where the resources are deployed in descendent projects, it may make compliance auditing more straightforward where data must remain in certain regions or countries. Certain services, like GCS and Google Container Registry (GCR), don't have very granular IAM permissions available to them unless you want to maintain per-object ACLs. To avoid some of that additional overhead, a common practice is to make dedicated GCP projects just for certain types of common GCS buckets. Similarly, with GCR, container image push/pull permissions are coarse GCS \"storage\" permissions. Dedicated projects are therefore a near requirement for separate GCR registries. Highly privileged administrative components that have organization-wide permissions should always be in dedicated projects near the top of the organization . A common example is the project where Forseti Security components are installed. If this project was nested inside several levels of folders , an IAM Role granted higher in that structure would unintentially provide access to those systems. Best Practices Certain Decisions Have inertia - Some decisions like organization domain name and project names carry a lot of inertia and are hard to change without migrating resources, so it's important to decide on a naming convention for projects early and stick with it. Consider Central Control of Project Creation - If project creation is to be controlled centrally, you can use an Organization Access Policy to set which group has the Project Creator IAM Role. Avoid Embedding Team Names in GCP Resource Names - Map folder and project names more closely with service offerings and operational environments. Use groups to establish the concept of \"teams\" and assign groups the IAM permissions to the folders and projects they need. Use Folder Names at the Top Level Carefully - Consider using high level folders to group sub- folders and projects along boundaries that shouldn't change often but help guide organization and auditing. e.g. Geographic region. Use separate projects for each operational environment. It's common to append -dev , -test , and -prod to project names and then group them into a folder named for that service . e.g. The folder b2b-crm holds the projects named b2b-crm-dev , b2b-crm-test , and b2b-crm-prod . Resources Using Resource Hierarchy for Access Control Using IAM Securely Organization Access Policy Forseti Security Cluster Networking VPC (shared vs peering based on administration model) Once the GCP organization , folder , and project structure is organized according to the desired hierarchy, the next step is to structure the networking and IP address management to support your use case. As a small organization, it may make sense to have a single project with a single GKE cluster to start. Inside this project, a VPC network is created, a few subnets are declared, and a GKE cluster is deployed. If another cluster is needed for a completely separate purpose, another project can be created (with another VPC and set of subnets). If these clusters need to communicate privately and avoid egress network charges, a common choice is to use VPC peering which makes a two-way routing path between each VPC ( A - B ). When the third VPC/Cluster comes along, another VPC/Subnet/Cluster is needed. Now, to VPC peer all three VPCs, three total VPC peering connections are needed. A - B , B - C , and A - C . When the fourth cluster is desired, a total of six VPC peering configurations are needed. This becomes difficult to manage correctly and adds to ongoing maintenance costs for troubleshooting and configuration for each additional VPC. The most common solution to centralizing the management and configuration of VPC networks across multiple projects is the Shared VPC model. One project is defined as the \"host\" project (where the VPC and Subnets are managed) and other projects are defined as \"service\" projects (where the services/apps/GKE clusters are deployed). The \"service\" projects no longer have VPCs and Subnets in them and instead are only allowed to \"use\" those VPCs/subnets defined in the \"host\" project. This has a few advantages for security-conscious organizations: Using IAM permissions, it's now possible to granularly assign the ability to create projects, attach them to the Shared VPC, create/manage subnets and firewall rules, and the ability to \"use\" or \"attach to\" the subnets to different users/groups in each of the \"service\" projects. With centralized control of the networking components, it's easier to manage IP address space, avoid CIDR overlap problems, and maintain consistent and correct configurations. Owners of resources in the various \"service\" projects can simply leverage the networking infrastructure provided to them. In practice, this helps curtail \"project-network\" sprawl which is very hard to reign in after the fact. With the organization hierarchy set to have one project for each environment type to separate IAM permissions, a best practice GKE deployment has a GKE cluster for each environment type in its respective project. The natural tendency is to make a Shared VPC for a service offering (e.g. b2b-crm ) and attach the three projects for b2b-crm-dev , b2b-crm-test , and b2b-crm-prod to it. From an operational perspective, this might facilitate simpler environment-to-environment communications to move data between clusters or share hybrid/VPN connectivity needs. However, from a security perspective, this is akin to running dev , test , and prod right next to each other on the same shared network. This can unintentionally increase the \"blast radius\" of an attack as a compromised test cluster would have more possible avenues to pivot and communicate directly with the prod cluster, for instance. While it's possible to do this with sufficient security controls in place, the chance for misconfiguration is much higher. If the entire point of a Shared VPC is to reduce the number of VPCs to manage, how does that work when each environment and cluster shouldn't share a VPC? One method is to create a Shared VPC for related dev projects and clusters, one for test projects , and one for prod projects and clusters that need to communicate with each other. For instance, a project holding GCE instances sharing a VPC with a project running a GKE cluster as a part of the same total application \"stack\". GKE is known for using a large amount of private RFC 1918 IP address space, and for proper operation of routing and interconnectivity features, CIDR ranges should not be reused or overlap. Node IPs - Each GKE \"node\" (GCE instance) needs one IP for administration and control plane communications, and it uses the \"primary\" range in the subnet declared. If the cluster isn't to grow beyond 250 nodes, this can be a /24 . For ~1000 nodes, a /22 is needed. This CIDR range should be unique and not overlap or be reused anywhere. Pod IPs - By default, each node can run a max of 110 Pods. Each pod needs an IP that is unique, and so GKE slices up /24 CIDR ranges from the \"secondary\" subnet range to assign to each node. A 250 node cluster will use 250 x /24 CIDR blocks. In this case, a /16 is needed to handle this cluster (250 x 256 = 64K, closest is 2^8 = 65535). A ~1000 node cluster will need a /14 all to itself (1000 x 256 = 256K, closest is 2^18 = 262K). Cluster/Service IPs - The only CIDR range that can be reused is the Service CIDR. This is because it is never meant to be routed outside the GKE cluster. It's what is assigned to ClusterIP Service objects in Kubernetes which can be used to map friendly, internal DNS names to Pod IPs matching certain labels. A range of /20 provides just over 4,090 possible services in a single cluster. This should be more than sufficient for clusters below 500 nodes. If the prospect of assigning a /16 per cluster has the network team of a large organization nervous, consider using \"Flexible Pod CIDR\". In many situations, the resources needed by Pods and the size of the GKE Nodes makes the practical limit of pods per node far fewer than the max of 110. If the workloads for a cluster are such that no more than 50-60 pods will ever run on a node, the node will only use a max of half of the /24 CIDR assigned to it. The \"wasted\" 128 addresses per node can add up quickly. When creating a GKE Node Pool, specifying a MaxPodsPerNode of 64 or fewer will trigger GKE to assign a /25 from the secondary range instead of a /24 . The reason for not going to a /26 is because of the natural lifecycle of a Pod IP assignment during a deployment is that greater than 64 IPs might be in use for short periods as some pods are starting while others are still terminating. Best Practices Do not overlap CIDRs - In almost every GCP organization , there exists a need to route to each GKE cluster and between clusters, and the cleanest way to do this with the least amount of engineering is to plan ahead and avoid CIDR overlap. The only known exception is the GKE Service CIDR range. Use VPC Aliasing - Currently, this is in the process of becoming the default option. Enabling IP aliasing means that the GKE CNI plugin places Pods IPs on an \"alias\" interface of the underlying node. This reduces latency by reducing network \"hops\" as the GCP Network infrastructure is used to route Pod-to-Pod traffic and LoadBalancer-to-Pod traffic instead of always involving kube-proxy . Consider Using Flexible Pod CIDR - Especially on larger cluster sizes, this can greatly reduce \"CIDR waste\" and allow for future scaling and expansion of node pools with half or more of the original IP space needed with the default configuration. Take Note of Quotas/Limits - When it comes to Shared VPCs, a recent quota/limit on the number of secondary address ranges per VPC went from 5 to 30. This had an effect of limiting the total number of GKE clusters that could be deployed in a Shared VPC as the quotas/limits for the networking components are shared by the \"host\" project. Large organizations with 1000s of instances will want to be aware of the 15,000 VM per Network when combining large projects with large instance counts and GKE node counts. Resources Shared VPC Overview GKE with Shared VPC GKE Networking Overview GKE Alias IP Cluster Shared VPC Quotas and Limits GKE Flexible Pod CIDR","title":"Cloud Environment"},{"location":"cloud_environment/#cloud-environment","text":"","title":"Cloud Environment"},{"location":"cloud_environment/#identity-and-iam-hierarchy","text":"At the foundation of your Google Cloud Platform (GCP) organization is a relationship with your trusted identity provider. Commonly, this is a Gsuite domain or a Cloud Identity installation. It's the basis of trust for how users and groups are managed throughout the GCP environments. It's where you audit logins/logouts, enforce MFA and session duration, and more. It's important to understand that in a GCP organization , where a user is granted permissions is just as important as what permissions are granted. For example, granting a user or service account a binding to an IAM role for Storage Admin at the project level allows that account to have all those permissions for all Google Cloud Storage (GCS) buckets in that project . Consider a somewhat realistic scenario: That project has GCS buckets for storing audit logs, VPC flow logs, and firewall logs. The security team is given Storage Admin permissions to manage these. Also in that same project , a development team wants to store some data used by their application. They too, get assigned Storage Admin permissions to that project . There are a few problems: Unless the security team is the one that made the IAM permissions assignment, they might not be aware of the exposure of their security log and event data. This jeopardizes the integrity of the information if needed for incident response and guaranteeing chain of custody. The development team now has access to this sensitive security information. Even though they may never access it or need to, this violates the principle of least privilege. Any time a new GCS bucket is added to this project , both teams will have access to it automatically. This can often be undesired behavior and tricky to untangle. Things get even more interesting when you consider a group of projects organized into a GCP Folder. Permissions granted to a Folder \"trickle down\" to the Folders and projects inside them. Because permissions are additive , a descendent project can't block or override those permissions. Therefore, careful consideration must be given to access granted at each level, and special attention is needed for IAM roles bound to users/groups/service accounts at the Folder or Organization levels. Some examples to consider: If a user is assigned the primitive role Owner at the Organization level, they will be Owner in every single project in the entire organization. The risk of compromise of those specific credentials (and by extension, that user's primary computing devices) is immediately increased to undesired levels. Granting Viewer at the Organization level allows reading all GCS buckets, all StackDriver logs, and viewing the configurations of most resources. In many cases, sensitive data is found in any or all of those places that can be used to escalate privileges. IAM \"Overcrowding\" The IAM page in the GCP Console tends to get \"crowded\" and harder to visually reason about when there are lots of individual bindings. Each IAM Role binding increases the cognitive load when reviewing permissions. Over time, this can get unwieldy and difficult to consolidate once systems and services are running in production.","title":"Identity and IAM Hierarchy"},{"location":"cloud_environment/#best-practices","text":"Use a single Identity Provider - If all users and groups are originating from a single domain and are managed centrally, maintaining the proper IAM permissions throughout the GCP Organization is greatly simplified. Ensure all accounts use Multi-Factor Authentication (MFA) - Protecting the theft and misuse of GCP account credentials is the first and most important step to protecting your GCP infrastructure. If a set of credentials is stolen or a laptop lost, the window for those credentials to be valid is greatly reduced. Consolidate Users into Groups - Instead of granting individual users access, consider using a group, placing that user in that group, and granting access to the group. This way, if that person leaves the organization, it doesn't leave orphaned/unused accounts clogging up the IAM permissions listings that have to be manually removed using gcloud/UI. Also, when replacing that person who left, it's a simpler process of just adding the new person(s) to the same groups. No gcloud/UI changes are then necessary. Follow the Principle of Least Privilege - Take care when granting access Project-wide and even greater care when granting access at the Folder and Organization levels. There are often unintended consequences of over-granting permissions. Additionally, it's much harder to safely remove that binding as time goes on because inherited permissions might be required in a descendent project.","title":"Best Practices"},{"location":"cloud_environment/#resources","text":"Creating and Managing GCP Organizations GCP Resource Hierarchy GSuite Security Cloud Identity Overview Cloud IAM Overview","title":"Resources"},{"location":"cloud_environment/#organization-folder-and-project-structure","text":"Resource hierarchy decisions made early on in your GCP journey tend to have a \"inertia\". Meaning, once they are configured a certain way, they become entrenched and harder to change as time goes on. If configured well, they can reduce administrative and security toil in managing resources and access to those resources in a clean, manageable way. If configured sub-optimally, you might feel like making administrative changes is overly complex and burdensome. Often, these decisions are often made in haste during design and implementation efforts \"just to get things working\". It might seem that there is pressure to get things perfect from the outset, but there is some level of built-in flexibility that you can take advantage of -- provided certain steps are taken. The two components of the GCP resource hierarchy that are trickiest to change are the organization and the non-empty project . The organization is the root of all resources, so it establishes the hierarchy. Projects cannot be renamed, and resources inside projects cannot be moved outside of projects . Folders can be made up to three levels deep, and projects can be moved around inside folders as needed. What this means: Permissions assigned at the organization level descend to everything below regardless of folder and project structure. Renaming projects or moving resources out of a project requires migration of resources, so careful thought to their name and what runs inside them is needed. Folders are a powerful tool in designing and adapting your IAM permissons hierarchy as needs change. Consider the following diagram of a GCP Organization: The organization is named example.com , and that is the root of the IAM hierarchy. Skipping over the folders tier for a second -- the projects are organized by operational environment which is very common and a best practice for keeping resources completely separated. Resources in the example-dev project have no access by default to example-test or example-prod . \"Fantastic!\" you might say, and copy this entire approach. However, depending on your organization's needs, this hierarchy might not be ideal! Let's break down the potential issues with the folders and the resources inside each project : In the example-dev project, instance_a and service_a may or may not be part of the same overall \"service\". If they both operate on data in bucket_a , for example, this makes sense. But if instance_a is managed by a separate team and shares nothing with service_a or bucket_a , you have a permissions scoping and \"attack blast radius\" problem. Permissions assigned on the example-dev project might be shared in undesirable ways between instance_a and service_a . In the event of a security compromise, attackers will be much more likely to obtain credentials with shared permissions and access nearby resources inside that same project. Finally, unless additional measures are taken with custom log sinks, the logs and metrics in Stackdriver will be going to the same project. Administrators and developers of the instance_a and service_a would be able to see each other's logs and metrics. In the example-test project, two compute instances and two GCS buckets are in place. If bucket_b and bucket_c are part of the same overall service, this makes sense. But if bucket_b serves data with PII in it and bucket_c is where VPC flow logs are stored, this increases the chance of accidental exposure if the Storage Viewer IAM Role is granted at the project level. The service accounts given the ability to read VPC flow logs may be able to read from the PII bucket and vice versa. A simple question to ask is, \"If the credentials for this user/group/service account were stolen without us knowing, what would they have access to?\" A service account with Storage Viewer granted at the example-test project means its credentials, if stolen or misused, would allow viewing the VPC flow logs and PII data in both GCS buckets. At the folder and project level, it may seem reasonable to name the folder after the team name. e.g. team-a-app1-dev . However, in practice, team names and organization charts in the real world tend to change more often than GCP resources. To remain flexible, it's much easier to use GCP groups to combine users into teams and then assign IAM permissions for that group at the desired level. Otherwise, you'll end up having team_a managing the team_b projects which will get very confusing. If your organization becomes large and has lots of geographic regions, a common practice is to have a top-level folder for regions like US , EU , etc. While this places no technical boundary on where the resources are deployed in descendent projects, it may make compliance auditing more straightforward where data must remain in certain regions or countries. Certain services, like GCS and Google Container Registry (GCR), don't have very granular IAM permissions available to them unless you want to maintain per-object ACLs. To avoid some of that additional overhead, a common practice is to make dedicated GCP projects just for certain types of common GCS buckets. Similarly, with GCR, container image push/pull permissions are coarse GCS \"storage\" permissions. Dedicated projects are therefore a near requirement for separate GCR registries. Highly privileged administrative components that have organization-wide permissions should always be in dedicated projects near the top of the organization . A common example is the project where Forseti Security components are installed. If this project was nested inside several levels of folders , an IAM Role granted higher in that structure would unintentially provide access to those systems.","title":"Organization, Folder, and Project Structure"},{"location":"cloud_environment/#best-practices_1","text":"Certain Decisions Have inertia - Some decisions like organization domain name and project names carry a lot of inertia and are hard to change without migrating resources, so it's important to decide on a naming convention for projects early and stick with it. Consider Central Control of Project Creation - If project creation is to be controlled centrally, you can use an Organization Access Policy to set which group has the Project Creator IAM Role. Avoid Embedding Team Names in GCP Resource Names - Map folder and project names more closely with service offerings and operational environments. Use groups to establish the concept of \"teams\" and assign groups the IAM permissions to the folders and projects they need. Use Folder Names at the Top Level Carefully - Consider using high level folders to group sub- folders and projects along boundaries that shouldn't change often but help guide organization and auditing. e.g. Geographic region. Use separate projects for each operational environment. It's common to append -dev , -test , and -prod to project names and then group them into a folder named for that service . e.g. The folder b2b-crm holds the projects named b2b-crm-dev , b2b-crm-test , and b2b-crm-prod .","title":"Best Practices"},{"location":"cloud_environment/#resources_1","text":"Using Resource Hierarchy for Access Control Using IAM Securely Organization Access Policy Forseti Security","title":"Resources"},{"location":"cloud_environment/#cluster-networking","text":"VPC (shared vs peering based on administration model) Once the GCP organization , folder , and project structure is organized according to the desired hierarchy, the next step is to structure the networking and IP address management to support your use case. As a small organization, it may make sense to have a single project with a single GKE cluster to start. Inside this project, a VPC network is created, a few subnets are declared, and a GKE cluster is deployed. If another cluster is needed for a completely separate purpose, another project can be created (with another VPC and set of subnets). If these clusters need to communicate privately and avoid egress network charges, a common choice is to use VPC peering which makes a two-way routing path between each VPC ( A - B ). When the third VPC/Cluster comes along, another VPC/Subnet/Cluster is needed. Now, to VPC peer all three VPCs, three total VPC peering connections are needed. A - B , B - C , and A - C . When the fourth cluster is desired, a total of six VPC peering configurations are needed. This becomes difficult to manage correctly and adds to ongoing maintenance costs for troubleshooting and configuration for each additional VPC. The most common solution to centralizing the management and configuration of VPC networks across multiple projects is the Shared VPC model. One project is defined as the \"host\" project (where the VPC and Subnets are managed) and other projects are defined as \"service\" projects (where the services/apps/GKE clusters are deployed). The \"service\" projects no longer have VPCs and Subnets in them and instead are only allowed to \"use\" those VPCs/subnets defined in the \"host\" project. This has a few advantages for security-conscious organizations: Using IAM permissions, it's now possible to granularly assign the ability to create projects, attach them to the Shared VPC, create/manage subnets and firewall rules, and the ability to \"use\" or \"attach to\" the subnets to different users/groups in each of the \"service\" projects. With centralized control of the networking components, it's easier to manage IP address space, avoid CIDR overlap problems, and maintain consistent and correct configurations. Owners of resources in the various \"service\" projects can simply leverage the networking infrastructure provided to them. In practice, this helps curtail \"project-network\" sprawl which is very hard to reign in after the fact. With the organization hierarchy set to have one project for each environment type to separate IAM permissions, a best practice GKE deployment has a GKE cluster for each environment type in its respective project. The natural tendency is to make a Shared VPC for a service offering (e.g. b2b-crm ) and attach the three projects for b2b-crm-dev , b2b-crm-test , and b2b-crm-prod to it. From an operational perspective, this might facilitate simpler environment-to-environment communications to move data between clusters or share hybrid/VPN connectivity needs. However, from a security perspective, this is akin to running dev , test , and prod right next to each other on the same shared network. This can unintentionally increase the \"blast radius\" of an attack as a compromised test cluster would have more possible avenues to pivot and communicate directly with the prod cluster, for instance. While it's possible to do this with sufficient security controls in place, the chance for misconfiguration is much higher. If the entire point of a Shared VPC is to reduce the number of VPCs to manage, how does that work when each environment and cluster shouldn't share a VPC? One method is to create a Shared VPC for related dev projects and clusters, one for test projects , and one for prod projects and clusters that need to communicate with each other. For instance, a project holding GCE instances sharing a VPC with a project running a GKE cluster as a part of the same total application \"stack\". GKE is known for using a large amount of private RFC 1918 IP address space, and for proper operation of routing and interconnectivity features, CIDR ranges should not be reused or overlap. Node IPs - Each GKE \"node\" (GCE instance) needs one IP for administration and control plane communications, and it uses the \"primary\" range in the subnet declared. If the cluster isn't to grow beyond 250 nodes, this can be a /24 . For ~1000 nodes, a /22 is needed. This CIDR range should be unique and not overlap or be reused anywhere. Pod IPs - By default, each node can run a max of 110 Pods. Each pod needs an IP that is unique, and so GKE slices up /24 CIDR ranges from the \"secondary\" subnet range to assign to each node. A 250 node cluster will use 250 x /24 CIDR blocks. In this case, a /16 is needed to handle this cluster (250 x 256 = 64K, closest is 2^8 = 65535). A ~1000 node cluster will need a /14 all to itself (1000 x 256 = 256K, closest is 2^18 = 262K). Cluster/Service IPs - The only CIDR range that can be reused is the Service CIDR. This is because it is never meant to be routed outside the GKE cluster. It's what is assigned to ClusterIP Service objects in Kubernetes which can be used to map friendly, internal DNS names to Pod IPs matching certain labels. A range of /20 provides just over 4,090 possible services in a single cluster. This should be more than sufficient for clusters below 500 nodes. If the prospect of assigning a /16 per cluster has the network team of a large organization nervous, consider using \"Flexible Pod CIDR\". In many situations, the resources needed by Pods and the size of the GKE Nodes makes the practical limit of pods per node far fewer than the max of 110. If the workloads for a cluster are such that no more than 50-60 pods will ever run on a node, the node will only use a max of half of the /24 CIDR assigned to it. The \"wasted\" 128 addresses per node can add up quickly. When creating a GKE Node Pool, specifying a MaxPodsPerNode of 64 or fewer will trigger GKE to assign a /25 from the secondary range instead of a /24 . The reason for not going to a /26 is because of the natural lifecycle of a Pod IP assignment during a deployment is that greater than 64 IPs might be in use for short periods as some pods are starting while others are still terminating.","title":"Cluster Networking"},{"location":"cloud_environment/#best-practices_2","text":"Do not overlap CIDRs - In almost every GCP organization , there exists a need to route to each GKE cluster and between clusters, and the cleanest way to do this with the least amount of engineering is to plan ahead and avoid CIDR overlap. The only known exception is the GKE Service CIDR range. Use VPC Aliasing - Currently, this is in the process of becoming the default option. Enabling IP aliasing means that the GKE CNI plugin places Pods IPs on an \"alias\" interface of the underlying node. This reduces latency by reducing network \"hops\" as the GCP Network infrastructure is used to route Pod-to-Pod traffic and LoadBalancer-to-Pod traffic instead of always involving kube-proxy . Consider Using Flexible Pod CIDR - Especially on larger cluster sizes, this can greatly reduce \"CIDR waste\" and allow for future scaling and expansion of node pools with half or more of the original IP space needed with the default configuration. Take Note of Quotas/Limits - When it comes to Shared VPCs, a recent quota/limit on the number of secondary address ranges per VPC went from 5 to 30. This had an effect of limiting the total number of GKE clusters that could be deployed in a Shared VPC as the quotas/limits for the networking components are shared by the \"host\" project. Large organizations with 1000s of instances will want to be aware of the 15,000 VM per Network when combining large projects with large instance counts and GKE node counts.","title":"Best Practices"},{"location":"cloud_environment/#resources_2","text":"Shared VPC Overview GKE with Shared VPC GKE Networking Overview GKE Alias IP Cluster Shared VPC Quotas and Limits GKE Flexible Pod CIDR","title":"Resources"},{"location":"cluster_addons/","text":"Cluster Add-ons Cloud Run on GKE Anthos Configuration Management Istio GPUs","title":"Cluster Add-ons"},{"location":"cluster_addons/#cluster-add-ons","text":"","title":"Cluster Add-ons"},{"location":"cluster_addons/#cloud-run-on-gke","text":"","title":"Cloud Run on GKE"},{"location":"cluster_addons/#anthos-configuration-management","text":"","title":"Anthos Configuration Management"},{"location":"cluster_addons/#istio","text":"","title":"Istio"},{"location":"cluster_addons/#gpus","text":"","title":"GPUs"},{"location":"cluster_configuration/","text":"Cluster Configuration The GKE Cluster Architecture Google Kubernetes Engine makes several key architectural decisions for you that may differ from other Kubernetes cluster installations. These choices are made in the interest of ease of use, operational simplicity, and security: There is no direct access to the control plane systems - GKE manages the GCE instances running etcd and the api server and supporting components, and does not expose them to you or an attacker via SSH, for example. In exchange for giving up direct control over all API server configuration flags, your GKE cluster offers very strong protection of the core components and sensitive data in etcd automatically. This means your control over the control plane is restricted to the configuration options on the GKE cluster object. Upgrades are handled via GKE Operations - Upgrades and downgrades are handled via GKE operations through API calls. GKE versions (which map to Kubernetes versions) and worker node OS security and version upgrades are all core features of the built-in upgrade process. The default worker node operating system is COS - Container-optimized OS is a hardened, minimal operating system engineered specifically to run containers and to function as a Kubernetes worker node . While SSH is enabled by default and integrates with GCP SSH functionality, it's goal is to make SSHing into each worker node unnecessary by handling patching and upgrades as a part of the node pool lifecycle. There is an Ubuntu worker node option for supporting unique use cases, but it requires additional administration in terms of upgrades and security patches. The CNI is not configurable - The native GKE CNI is used to connect worker nodes and pods to the GCP Network and is not currently replacable. Network Policy enforcement is done via Calico - Enabling Network Policy enforcement installs a Calico deployment and daemonset as a managed addon for you. Logging and Monitoring is done via Stackdriver - By default, all logs and performance metrics are sent to Stackdriver in the current GCP project . Resources GKE Architecture GKE Upgrades Container-Optimized OS GKE CNI GKE Stackdriver Geographical Placement and Cluster Types GKE clusters are by default \"zonal\" clusters. That is, a single GCE instance running the control plane components is deployed in the same GCP zone as the node-pool nodes. \"Regional\" GKE clusters are deployed across three zones in the same region. Three GCE instances running the control plane components are deployed (one per zone ) behind a single IP and load balancer . The node-pools spread evenly across the zones with a minimum of one node per zone . The key benefits to a \"regional\" cluster : The cluster control plane can handle a single GCP zone failure more gracefully. When the control plane is being upgraded, only one instance is down at a time which leaves two remaining instances. Upgrades on \"zonal\" clusters means a 4-10 minute downtime of the API/control plane while that takes place where you can't use kubectl to interact with your cluster . Since there is no additional charge for running a \"regional\" GKE cluster with a highly-available control plane, why use a \"zonal\" cluster ? The two primary reasons are to save on \"cross-zone\" network traffic costs and to support specific GPU node-pool needs. Persistent Disks The default StorageClass defines persistent disks that are \"zonal\". You may want to add a new StorageClass that makes \"regional\" persistent disks available for pod workloads that get rescheduled on another node in a different zone to still be able to access them. Best Practices Use Regional Clusters - Unless you have specific needs that force you to use a \"zonal\" cluster , using \"regional\" clusters offers the best redundancy and availablility for a minor increase in network traffic costs for the majority of use cases. Offer a Regional Persistent Disk StorageClass - Allows pods to attach and access persistent disk volumes regardless of where they are scheduled inside the cluster. This prevents a zone failure from allowing a pod to be rescheduled and mount that disk on a node in another zone . Resources GKE Regional Clusters Google Cloud IAM and Kubernetes RBAC Google Cloud Identity and Access Management (IAM) - The system in GCP that grants permissions via GCP IAM Roles to users and service accounts to access GCP APIs. Kubernetes Role-Based Access Control (RBAC) - The native system inside Kubernetes that grants permissions via Kubernetes roles and clusterroles to users and service accounts to access the Kubernetes API Server. The two APIs that can be used to interact with the GKE service and a GKE cluster are: GCP GKE API (container.googleapis.com) - Used to create/update/delete the cluster and node pools that comprise the GKE cluster and to obtain connection and credential information for how to access a given cluster . Kubernetes API of the cluster - The unique Kubernetes API Server endpoint running on the GKE Control Plane systems for a specific cluster . It controls resources and access to resources running inside the cluster. This means that there are two main categories of permissions that you have to consider: Cluster Administration - Permissions associated with administering the cluster itself. Cluster Usage - Permissions associated with granting what is allowed to run inside the cluster. There are a couple key points to understand about how Cloud IAM and Kubernetes RBAC can be used to grant permissions in GKE: GCP Cloud IAM is administered at the project level, and the granted permissions apply to all GKE clusters in the project . They remain in-place even if a cluster is deleted from the project . Kubernetes RBAC permissions are administered per- cluster and are stored inside each cluster . When that cluster is destroyed, those permissions are also destroyed. GCP Cloud IAM permissions have no concept of Kubernetes namespaces , so the permissions granted apply for all namespaces . Kubernetes RBAC can be used to grant permissions to access resources in all namespaces or only in specific namespaces . Kubernetes RBAC cannot be used to grant permissions to the GCP GKE API (container.googleapis.com). This is performed solely by Cloud IAM. Both systems are additive in that they only grant permissions. They cannot remove or negate permissions from themselves or each other. IAM and RBAC combine inside the cluster When accessing a resource (e.g. list pods in the default namespace) via the Kubernetes API of a GKE cluster , it can be granted via IAM or RBAC. They are effectively combined. If either grants access, the request is permitted. Best Practices There are several predefined IAM roles for GKE clusters that can ease administration as you are getting started with GKE. However, the permissions they grant are often overly broad and violate the priciple of least privileges. You will want to create custom IAM roles with just the permissions needed, but remember that IAM permissions apply to all namespaces and cannot be limited to a single namespace . To maintain least privilege, it's recommended to leverage a minimal IAM role to gain access to the GKE API and then use Kubernetes RBAC roles and clusterroles to define what resources they can access inside the cluster . In order for users and service accounts to perform a gcloud container clusters get-credentials call to generate a valid kubeconfig for a GKE cluster , they need to have the following permissions: container.apiServices.get container.apiServices.list container.clusters.get container.clusters.list container.clusters.getCredentials If these permissions are grouped into a custom IAM role , that IAM role can be conveniently bound to a Gsuite/Cloud Identity group which includes all users that need access to the cluster . From this point, the users and service accounts can be granted access to resources as needed with cluster -wide or per- namespace granularity. This has the benefit of minimizing IAM changes needed, ensuring access granted is per- cluster , giving the highest granularity, and making troubleshooting permissions an RBAC-only process. With the above approach of deferring nearly all permissions to in- cluster RBAC instead of IAM, there is one exception: assigning the Kubernetes Engine Admin predefined IAM role at the project or folder level to a small number of trusted administrators to ensure that they don't accidentally remove their own access via an RBAC configuration mistake. Resources GKE IAM and RBAC Predefined GKE Roles GKE RBAC Cluster Access Cluster Settings Node Pool Settings","title":"Cluster Configuration"},{"location":"cluster_configuration/#cluster-configuration","text":"","title":"Cluster Configuration"},{"location":"cluster_configuration/#the-gke-cluster-architecture","text":"Google Kubernetes Engine makes several key architectural decisions for you that may differ from other Kubernetes cluster installations. These choices are made in the interest of ease of use, operational simplicity, and security: There is no direct access to the control plane systems - GKE manages the GCE instances running etcd and the api server and supporting components, and does not expose them to you or an attacker via SSH, for example. In exchange for giving up direct control over all API server configuration flags, your GKE cluster offers very strong protection of the core components and sensitive data in etcd automatically. This means your control over the control plane is restricted to the configuration options on the GKE cluster object. Upgrades are handled via GKE Operations - Upgrades and downgrades are handled via GKE operations through API calls. GKE versions (which map to Kubernetes versions) and worker node OS security and version upgrades are all core features of the built-in upgrade process. The default worker node operating system is COS - Container-optimized OS is a hardened, minimal operating system engineered specifically to run containers and to function as a Kubernetes worker node . While SSH is enabled by default and integrates with GCP SSH functionality, it's goal is to make SSHing into each worker node unnecessary by handling patching and upgrades as a part of the node pool lifecycle. There is an Ubuntu worker node option for supporting unique use cases, but it requires additional administration in terms of upgrades and security patches. The CNI is not configurable - The native GKE CNI is used to connect worker nodes and pods to the GCP Network and is not currently replacable. Network Policy enforcement is done via Calico - Enabling Network Policy enforcement installs a Calico deployment and daemonset as a managed addon for you. Logging and Monitoring is done via Stackdriver - By default, all logs and performance metrics are sent to Stackdriver in the current GCP project .","title":"The GKE Cluster Architecture"},{"location":"cluster_configuration/#resources","text":"GKE Architecture GKE Upgrades Container-Optimized OS GKE CNI GKE Stackdriver","title":"Resources"},{"location":"cluster_configuration/#geographical-placement-and-cluster-types","text":"GKE clusters are by default \"zonal\" clusters. That is, a single GCE instance running the control plane components is deployed in the same GCP zone as the node-pool nodes. \"Regional\" GKE clusters are deployed across three zones in the same region. Three GCE instances running the control plane components are deployed (one per zone ) behind a single IP and load balancer . The node-pools spread evenly across the zones with a minimum of one node per zone . The key benefits to a \"regional\" cluster : The cluster control plane can handle a single GCP zone failure more gracefully. When the control plane is being upgraded, only one instance is down at a time which leaves two remaining instances. Upgrades on \"zonal\" clusters means a 4-10 minute downtime of the API/control plane while that takes place where you can't use kubectl to interact with your cluster . Since there is no additional charge for running a \"regional\" GKE cluster with a highly-available control plane, why use a \"zonal\" cluster ? The two primary reasons are to save on \"cross-zone\" network traffic costs and to support specific GPU node-pool needs. Persistent Disks The default StorageClass defines persistent disks that are \"zonal\". You may want to add a new StorageClass that makes \"regional\" persistent disks available for pod workloads that get rescheduled on another node in a different zone to still be able to access them.","title":"Geographical Placement and Cluster Types"},{"location":"cluster_configuration/#best-practices","text":"Use Regional Clusters - Unless you have specific needs that force you to use a \"zonal\" cluster , using \"regional\" clusters offers the best redundancy and availablility for a minor increase in network traffic costs for the majority of use cases. Offer a Regional Persistent Disk StorageClass - Allows pods to attach and access persistent disk volumes regardless of where they are scheduled inside the cluster. This prevents a zone failure from allowing a pod to be rescheduled and mount that disk on a node in another zone .","title":"Best Practices"},{"location":"cluster_configuration/#resources_1","text":"GKE Regional Clusters","title":"Resources"},{"location":"cluster_configuration/#google-cloud-iam-and-kubernetes-rbac","text":"Google Cloud Identity and Access Management (IAM) - The system in GCP that grants permissions via GCP IAM Roles to users and service accounts to access GCP APIs. Kubernetes Role-Based Access Control (RBAC) - The native system inside Kubernetes that grants permissions via Kubernetes roles and clusterroles to users and service accounts to access the Kubernetes API Server. The two APIs that can be used to interact with the GKE service and a GKE cluster are: GCP GKE API (container.googleapis.com) - Used to create/update/delete the cluster and node pools that comprise the GKE cluster and to obtain connection and credential information for how to access a given cluster . Kubernetes API of the cluster - The unique Kubernetes API Server endpoint running on the GKE Control Plane systems for a specific cluster . It controls resources and access to resources running inside the cluster. This means that there are two main categories of permissions that you have to consider: Cluster Administration - Permissions associated with administering the cluster itself. Cluster Usage - Permissions associated with granting what is allowed to run inside the cluster. There are a couple key points to understand about how Cloud IAM and Kubernetes RBAC can be used to grant permissions in GKE: GCP Cloud IAM is administered at the project level, and the granted permissions apply to all GKE clusters in the project . They remain in-place even if a cluster is deleted from the project . Kubernetes RBAC permissions are administered per- cluster and are stored inside each cluster . When that cluster is destroyed, those permissions are also destroyed. GCP Cloud IAM permissions have no concept of Kubernetes namespaces , so the permissions granted apply for all namespaces . Kubernetes RBAC can be used to grant permissions to access resources in all namespaces or only in specific namespaces . Kubernetes RBAC cannot be used to grant permissions to the GCP GKE API (container.googleapis.com). This is performed solely by Cloud IAM. Both systems are additive in that they only grant permissions. They cannot remove or negate permissions from themselves or each other. IAM and RBAC combine inside the cluster When accessing a resource (e.g. list pods in the default namespace) via the Kubernetes API of a GKE cluster , it can be granted via IAM or RBAC. They are effectively combined. If either grants access, the request is permitted.","title":"Google Cloud IAM and Kubernetes RBAC"},{"location":"cluster_configuration/#best-practices_1","text":"There are several predefined IAM roles for GKE clusters that can ease administration as you are getting started with GKE. However, the permissions they grant are often overly broad and violate the priciple of least privileges. You will want to create custom IAM roles with just the permissions needed, but remember that IAM permissions apply to all namespaces and cannot be limited to a single namespace . To maintain least privilege, it's recommended to leverage a minimal IAM role to gain access to the GKE API and then use Kubernetes RBAC roles and clusterroles to define what resources they can access inside the cluster . In order for users and service accounts to perform a gcloud container clusters get-credentials call to generate a valid kubeconfig for a GKE cluster , they need to have the following permissions: container.apiServices.get container.apiServices.list container.clusters.get container.clusters.list container.clusters.getCredentials If these permissions are grouped into a custom IAM role , that IAM role can be conveniently bound to a Gsuite/Cloud Identity group which includes all users that need access to the cluster . From this point, the users and service accounts can be granted access to resources as needed with cluster -wide or per- namespace granularity. This has the benefit of minimizing IAM changes needed, ensuring access granted is per- cluster , giving the highest granularity, and making troubleshooting permissions an RBAC-only process. With the above approach of deferring nearly all permissions to in- cluster RBAC instead of IAM, there is one exception: assigning the Kubernetes Engine Admin predefined IAM role at the project or folder level to a small number of trusted administrators to ensure that they don't accidentally remove their own access via an RBAC configuration mistake.","title":"Best Practices"},{"location":"cluster_configuration/#resources_2","text":"GKE IAM and RBAC Predefined GKE Roles GKE RBAC","title":"Resources"},{"location":"cluster_configuration/#cluster-access","text":"","title":"Cluster Access"},{"location":"cluster_configuration/#cluster-settings","text":"","title":"Cluster Settings"},{"location":"cluster_configuration/#node-pool-settings","text":"","title":"Node Pool Settings"},{"location":"cluster_lifecycle/","text":"Cluster Lifecycle Infrastructure as Code Upgrades Scaling Security Bulletins and Patching Maintenance Windows","title":"Cluster Lifecycle"},{"location":"cluster_lifecycle/#cluster-lifecycle","text":"","title":"Cluster Lifecycle"},{"location":"cluster_lifecycle/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"cluster_lifecycle/#upgrades","text":"","title":"Upgrades"},{"location":"cluster_lifecycle/#scaling","text":"","title":"Scaling"},{"location":"cluster_lifecycle/#security-bulletins-and-patching","text":"","title":"Security Bulletins and Patching"},{"location":"cluster_lifecycle/#maintenance-windows","text":"","title":"Maintenance Windows"},{"location":"cluster_observability/","text":"Cluster Observability Audit Logs Control Plane Logs Worker Node Logs Pod Logs Node Metrics Kubernetes State Metrics Workload Metrics","title":"Cluster Observability"},{"location":"cluster_observability/#cluster-observability","text":"","title":"Cluster Observability"},{"location":"cluster_observability/#audit-logs","text":"","title":"Audit Logs"},{"location":"cluster_observability/#control-plane-logs","text":"","title":"Control Plane Logs"},{"location":"cluster_observability/#worker-node-logs","text":"","title":"Worker Node Logs"},{"location":"cluster_observability/#pod-logs","text":"","title":"Pod Logs"},{"location":"cluster_observability/#node-metrics","text":"","title":"Node Metrics"},{"location":"cluster_observability/#kubernetes-state-metrics","text":"","title":"Kubernetes State Metrics"},{"location":"cluster_observability/#workload-metrics","text":"","title":"Workload Metrics"},{"location":"container_images/","text":"Container Images Dockerfile Best Practices Base Images Package Vulnerabilities Application Vulnerabilities","title":"Container Images"},{"location":"container_images/#container-images","text":"","title":"Container Images"},{"location":"container_images/#dockerfile-best-practices","text":"","title":"Dockerfile Best Practices"},{"location":"container_images/#base-images","text":"","title":"Base Images"},{"location":"container_images/#package-vulnerabilities","text":"","title":"Package Vulnerabilities"},{"location":"container_images/#application-vulnerabilities","text":"","title":"Application Vulnerabilities"},{"location":"detection_and_response/","text":"Detection and Response Runtime Detection Incident Response Forensics Credential Revocation and Rotation","title":"Detection and Response"},{"location":"detection_and_response/#detection-and-response","text":"","title":"Detection and Response"},{"location":"detection_and_response/#runtime-detection","text":"","title":"Runtime Detection"},{"location":"detection_and_response/#incident-response","text":"","title":"Incident Response"},{"location":"detection_and_response/#forensics","text":"","title":"Forensics"},{"location":"detection_and_response/#credential-revocation-and-rotation","text":"","title":"Credential Revocation and Rotation"},{"location":"project_organization/","text":"Project Organization Project and Environment Separation Using the Hipster Shop as an example workload of eleven coordinating microservices that form a \"service offering\", there are a few different approaches for how to organize GCP projects and GKE clusters . Each has pros and cons from a security perspective. Single GCP Project, Single GKE Cluster, Namespaces per Environment - A single project named my-hipster-shop with a GKE cluster named gke-hipster-shop and three Kubernetes namespaces : hipster-shop-dev , hipster-shop-test , and hipster-shop-prod . Pros Simplest project strategy Simplest network strategy Least expensive cluster strategy Cons Weakest \"isolation\" strategy Weakest \"defense in depth\" strategy Weakest \"resource contention\" strategy Single GCP Project, Three GKE Clusters - A single project named my-hipster-shop with three GKE clusters named gke-hipster-shop-dev , gke-hipster-shop-test , and gke-hipster-shop-prod . A single namespace named hipster-shop is in each cluster . Pros Good \"isolation\" strategy Good \"defense in depth\" strategy Good \"resource contention\" strategy Cons Most complex project strategy Simple network strategy Most expensive cluster strategy Three GCP Projects, Single GKE Cluster per Project - Three projects named my-hipster-shop-dev , my-hipster-shop-test , and my-hipster-shop-prod . In each project , a GKE cluster named gke-hipster-shop- env , and a single namespace named hipster-shop in each cluster . Pros Strongest \"isolation\" strategy Strongest \"defense in depth\" strategy Strongest \"resource contention\" strategy Cons Most complex project strategy Most complex network strategy Most expensive cluster strategy Strategy Descriptions Project Complexity - Although GCP projects are \"free\", this refers to the ongoing maintenance and operational overhead of managing and caring for GCP projects . Managing 5 projects vs 15 vs 150 has challenges that require more sophisticated tooling and processes to do well. Network Complexity - Having all GKE clusters on the same shared VPC network or in separate VPC networks will vary the amount of ongoing maintenance of IP CIDR allocation/consumption, Interconnect/Cloud VPN configuration complexity, and complexity of collecting network related flow logs. Cluster Cost - While the control plane of GKE is free, the additional GKE worker nodes in three clusters vs one plus the additional maintenance cost of managing (upgrading, securing, monitoring) more clusters increases the overall cost. Isolation - Does the configuration leverage harder boundary mechanisms like GCP projects , separate GKE clusters on separate GCE instances , or does it rely on softer boundaries like Kubernetes namespaces ? Defense in Depth - Should a security incident occur, which strategy serves to reduce the available attack surface by default, make lateral movement more difficult to perform successfully, and make malicious activity easier to identify vs normal activity? Resource Contention - Are workloads competing for resources on the same nodes , clusters , or projects ? Is it possible for a single workload to over-consume resources such that the nodes evict pods ? If the cluster autoscales to add more nodes , does that consume all available cpu / memory / disk quota in the project and prevent other clusters from having available resources to autoscale if they need to grow? Best Practices Standardize Early - Understand that you will be building tools and processes implicitly and explicitly around your approach, so choose the one that meets your requirements and be consistent across all your GKE deployments. Automation with infrastructure-as-code tools like Terraform for standardizing configuration and naming conventions of projects and clusters is strongly encouraged. One Cluster per Project - While it is the most expensive approach to operate, it offers the best permissions isolation, defense-in-depth, and resource contention strategy by default. If you are serious about service offering and environment separation and have any compliance requirements, this is the best way to achieve those objectives. One Cluster per Service Offering - Running one cluster per colocated set of services with similar data gravity and redundancy needs is ideal for reducing the \"blast radius\" of an incident. It might seem convenient to place three or four smaller \"production\" services into a single GKE cluster , but consider the scope of an investigation should a compromise occur. All workloads in that cluster and all the data they touch would have to be \"in scope\" for the remediation efforts. Resources Preparing GKE Environments for Production GCP Cost Calculator GKE Pricing Hipster Shop Terraform Separating Tenants The previous section covers the \"cluster per project\" approaches, and this section attempts to guide you through the \"workloads per cluster\" decisions. Two important definitions to cover first are Hard and Soft tenancy as written by Jessie Frazelle: Soft multi-tenancy - multiple users within the same organization in the same cluster . Soft multi-tenancy could have possible bad actors such as people leaving the company, etc. Users are not thought to be actively malicious since they are within the same organization, but potential for accidents or \"evil leaving employees.\" A large focus of soft multi-tenancy is to prevent accidents. Hard multi-tenancy - multiple users, from various places, in the same cluster . Hard multi-tenancy means that anyone on the cluster is thought to be potentially malicious and therefore should not have access to any other tenants resources. From experience, building and operating a cluster with a hard-tenancy use case in mind is very difficult. The tools and capabilties are improving in this area, but it requires extreme attention to detail, careful planning, 100% visibility into activity, and near-hyper active monitoring. For these reasons, your journey with Kubernetes and GKE should first solve for the soft-tenancy use case. The understanding and lessons learned will overlap nearly 100% if you decide to go for hard-tenancy and will absolutely give you the proper frame of reference to decide if your organization can tackle the added challenges. There is no such thing as a \"single-tenant\" cluster When it comes to workloads, there are always a minimum of two classes: \"System\" and \"User\" workloads. Workloads that are responsible for the operation of the cluster (CNI, log export, metrics export, etc) should be isolated from workloads that run actual applications and vice versa. In Kubernetes, the default separation between these workload types is likely not sufficient for production needs. System components run on the same physical resources as user workloads, share a common administrative mechanism, share a common layer 3 network with no default access controls, and often run with higher privileges. Even in GKE, you will want to take steps to address these concerns. API Isolation - Using separate Kubernetes namespaces to segment workloads for different purposes when it comes to how those resources interact with the Kubernetes API only. Service accounts per namespace tied to granular RBAC policies are the primary approach. Network Isolation - Using Kubernetes namespaces as an anchor point, defining which pods are allowed to talk with each other explicitly via NetworkPolicy objects is the primary approach. For instance, preventing all ingress traffic from non- kube-system namespaces with the exception of udp/53 for kube-dns . Privilege Isolation - Leveraging well-formed containers running as non-privileged users in combination with PodSecurityPolicies to prevent user workloads from being able to access sensitive or privileged resources on the worker node and undermining the security of all workloads. Resource Isolation - Using features like ResourceQuotas to cap overal cpu/memory/persistent disk resource consumption, resources requests and limits to ensure pods are given the resources they need without overcrowding other workloads, and separating security or performance sensitive workloads on separate Node Pools . Whether you are a single developer running a couple microservices in one cluster or a large organization with many teams sharing large clusters , these concerns are important and should not be overlooked. The remainder of this guide will attempt to show you how to implement these features in combination and give you confidence in the decisions you make for your use case. Best Practices Tenants per Cluster - From a security perspective, having a single tenant per cluster provides the highest degree of separation among tenants, but it is common to allow multiple workloads from different users/teams of similar trust levels to share a cluster for cost and operational efficiency. Multiple Untrusted Tenants in a Single Cluster - This approach is not generally recommended as the level of effort to sufficiently isolate workloads is high and the risk of a vulnerability or mistake leading to a tenant escape is much higher. Separate the System from the Workloads - No matter which approach is taken, you should take steps to properly isolate the pods and services that control and manage your cluster from the workloads that operate in them. The system components can have permissions to GCP resources outside your cluster , and it's important that an incident with a user workload can't escape to the system workloads and then escape \"outside\" the cluster . Resources Hard Multi-Tenancy in Kubernetes Multi-Tenancy Design Space GKE Cluster Multi-Tenancy Project Quotas GCP projects have quotas or limits on how many resources are available for potential use. This doesn't mean that there is available capacity to fulfill the request, and on rare occasions, a particular zone might not be able to provide another GKE worker right away when your cluster workloads need more capacity and node-pool autoscaling kicks in. GKE uses several GCE related quotas like cpu , memory , disk , and gpu of a particular type, but it also uses lesser known quotas like \"Number of secondary ranges per VPC\". The resource related quotas are per region and sometimes per zone, so it's important to monitor your quota usage to avoid quota-capped resource exhaustion scenarios from negatively affecting your application's performance during a scale-up event or from preventing certain types of upgrade scenarios. Best Practices Monitor Quota Consumption - Smaller GKE clusters will most likely fit into the project quota defaults, but clusters of a dozen nodes or more may start to bump into the limits. Request Quota Increases Ahead of Time - Quota increases can take as much as 48 hrs to be approved, so it's best to plan ahead and ask early. Aim for 110% - If the single GKE cluster in a project uses 10, 32-core nodes , the total cpu cores needed is 320 or more. To give enough headroom to perform a \"blue/green\" cluster upgrade if needed (bringing up an identical cluster in parallel), the project quota should be at least 640 cpu cores in that region to facilitate that approach. Following the 110% guideline, this would actually be more like 700 cpus . This allows for two, full-sized clusters to be possible for a short duration while the workloads are migrated between the clusters . GCP APIs have Rate Limits - If your application makes 100s of requests per second or more to say, GCS or GCR, you may run into rate limits designed to protect overuse of the GCP APIs from a single customer affecting all customers. If you run into these, you may or may not be able to get them increased. Consider working with GCP support and implementing a different approach with your application. Resources GCE Quotas and Limits Working with GCP Quotas","title":"Project Organization"},{"location":"project_organization/#project-organization","text":"","title":"Project Organization"},{"location":"project_organization/#project-and-environment-separation","text":"Using the Hipster Shop as an example workload of eleven coordinating microservices that form a \"service offering\", there are a few different approaches for how to organize GCP projects and GKE clusters . Each has pros and cons from a security perspective. Single GCP Project, Single GKE Cluster, Namespaces per Environment - A single project named my-hipster-shop with a GKE cluster named gke-hipster-shop and three Kubernetes namespaces : hipster-shop-dev , hipster-shop-test , and hipster-shop-prod . Pros Simplest project strategy Simplest network strategy Least expensive cluster strategy Cons Weakest \"isolation\" strategy Weakest \"defense in depth\" strategy Weakest \"resource contention\" strategy Single GCP Project, Three GKE Clusters - A single project named my-hipster-shop with three GKE clusters named gke-hipster-shop-dev , gke-hipster-shop-test , and gke-hipster-shop-prod . A single namespace named hipster-shop is in each cluster . Pros Good \"isolation\" strategy Good \"defense in depth\" strategy Good \"resource contention\" strategy Cons Most complex project strategy Simple network strategy Most expensive cluster strategy Three GCP Projects, Single GKE Cluster per Project - Three projects named my-hipster-shop-dev , my-hipster-shop-test , and my-hipster-shop-prod . In each project , a GKE cluster named gke-hipster-shop- env , and a single namespace named hipster-shop in each cluster . Pros Strongest \"isolation\" strategy Strongest \"defense in depth\" strategy Strongest \"resource contention\" strategy Cons Most complex project strategy Most complex network strategy Most expensive cluster strategy","title":"Project and Environment Separation"},{"location":"project_organization/#strategy-descriptions","text":"Project Complexity - Although GCP projects are \"free\", this refers to the ongoing maintenance and operational overhead of managing and caring for GCP projects . Managing 5 projects vs 15 vs 150 has challenges that require more sophisticated tooling and processes to do well. Network Complexity - Having all GKE clusters on the same shared VPC network or in separate VPC networks will vary the amount of ongoing maintenance of IP CIDR allocation/consumption, Interconnect/Cloud VPN configuration complexity, and complexity of collecting network related flow logs. Cluster Cost - While the control plane of GKE is free, the additional GKE worker nodes in three clusters vs one plus the additional maintenance cost of managing (upgrading, securing, monitoring) more clusters increases the overall cost. Isolation - Does the configuration leverage harder boundary mechanisms like GCP projects , separate GKE clusters on separate GCE instances , or does it rely on softer boundaries like Kubernetes namespaces ? Defense in Depth - Should a security incident occur, which strategy serves to reduce the available attack surface by default, make lateral movement more difficult to perform successfully, and make malicious activity easier to identify vs normal activity? Resource Contention - Are workloads competing for resources on the same nodes , clusters , or projects ? Is it possible for a single workload to over-consume resources such that the nodes evict pods ? If the cluster autoscales to add more nodes , does that consume all available cpu / memory / disk quota in the project and prevent other clusters from having available resources to autoscale if they need to grow?","title":"Strategy Descriptions"},{"location":"project_organization/#best-practices","text":"Standardize Early - Understand that you will be building tools and processes implicitly and explicitly around your approach, so choose the one that meets your requirements and be consistent across all your GKE deployments. Automation with infrastructure-as-code tools like Terraform for standardizing configuration and naming conventions of projects and clusters is strongly encouraged. One Cluster per Project - While it is the most expensive approach to operate, it offers the best permissions isolation, defense-in-depth, and resource contention strategy by default. If you are serious about service offering and environment separation and have any compliance requirements, this is the best way to achieve those objectives. One Cluster per Service Offering - Running one cluster per colocated set of services with similar data gravity and redundancy needs is ideal for reducing the \"blast radius\" of an incident. It might seem convenient to place three or four smaller \"production\" services into a single GKE cluster , but consider the scope of an investigation should a compromise occur. All workloads in that cluster and all the data they touch would have to be \"in scope\" for the remediation efforts.","title":"Best Practices"},{"location":"project_organization/#resources","text":"Preparing GKE Environments for Production GCP Cost Calculator GKE Pricing Hipster Shop Terraform","title":"Resources"},{"location":"project_organization/#separating-tenants","text":"The previous section covers the \"cluster per project\" approaches, and this section attempts to guide you through the \"workloads per cluster\" decisions. Two important definitions to cover first are Hard and Soft tenancy as written by Jessie Frazelle: Soft multi-tenancy - multiple users within the same organization in the same cluster . Soft multi-tenancy could have possible bad actors such as people leaving the company, etc. Users are not thought to be actively malicious since they are within the same organization, but potential for accidents or \"evil leaving employees.\" A large focus of soft multi-tenancy is to prevent accidents. Hard multi-tenancy - multiple users, from various places, in the same cluster . Hard multi-tenancy means that anyone on the cluster is thought to be potentially malicious and therefore should not have access to any other tenants resources. From experience, building and operating a cluster with a hard-tenancy use case in mind is very difficult. The tools and capabilties are improving in this area, but it requires extreme attention to detail, careful planning, 100% visibility into activity, and near-hyper active monitoring. For these reasons, your journey with Kubernetes and GKE should first solve for the soft-tenancy use case. The understanding and lessons learned will overlap nearly 100% if you decide to go for hard-tenancy and will absolutely give you the proper frame of reference to decide if your organization can tackle the added challenges. There is no such thing as a \"single-tenant\" cluster When it comes to workloads, there are always a minimum of two classes: \"System\" and \"User\" workloads. Workloads that are responsible for the operation of the cluster (CNI, log export, metrics export, etc) should be isolated from workloads that run actual applications and vice versa. In Kubernetes, the default separation between these workload types is likely not sufficient for production needs. System components run on the same physical resources as user workloads, share a common administrative mechanism, share a common layer 3 network with no default access controls, and often run with higher privileges. Even in GKE, you will want to take steps to address these concerns. API Isolation - Using separate Kubernetes namespaces to segment workloads for different purposes when it comes to how those resources interact with the Kubernetes API only. Service accounts per namespace tied to granular RBAC policies are the primary approach. Network Isolation - Using Kubernetes namespaces as an anchor point, defining which pods are allowed to talk with each other explicitly via NetworkPolicy objects is the primary approach. For instance, preventing all ingress traffic from non- kube-system namespaces with the exception of udp/53 for kube-dns . Privilege Isolation - Leveraging well-formed containers running as non-privileged users in combination with PodSecurityPolicies to prevent user workloads from being able to access sensitive or privileged resources on the worker node and undermining the security of all workloads. Resource Isolation - Using features like ResourceQuotas to cap overal cpu/memory/persistent disk resource consumption, resources requests and limits to ensure pods are given the resources they need without overcrowding other workloads, and separating security or performance sensitive workloads on separate Node Pools . Whether you are a single developer running a couple microservices in one cluster or a large organization with many teams sharing large clusters , these concerns are important and should not be overlooked. The remainder of this guide will attempt to show you how to implement these features in combination and give you confidence in the decisions you make for your use case.","title":"Separating Tenants"},{"location":"project_organization/#best-practices_1","text":"Tenants per Cluster - From a security perspective, having a single tenant per cluster provides the highest degree of separation among tenants, but it is common to allow multiple workloads from different users/teams of similar trust levels to share a cluster for cost and operational efficiency. Multiple Untrusted Tenants in a Single Cluster - This approach is not generally recommended as the level of effort to sufficiently isolate workloads is high and the risk of a vulnerability or mistake leading to a tenant escape is much higher. Separate the System from the Workloads - No matter which approach is taken, you should take steps to properly isolate the pods and services that control and manage your cluster from the workloads that operate in them. The system components can have permissions to GCP resources outside your cluster , and it's important that an incident with a user workload can't escape to the system workloads and then escape \"outside\" the cluster .","title":"Best Practices"},{"location":"project_organization/#resources_1","text":"Hard Multi-Tenancy in Kubernetes Multi-Tenancy Design Space GKE Cluster Multi-Tenancy","title":"Resources"},{"location":"project_organization/#project-quotas","text":"GCP projects have quotas or limits on how many resources are available for potential use. This doesn't mean that there is available capacity to fulfill the request, and on rare occasions, a particular zone might not be able to provide another GKE worker right away when your cluster workloads need more capacity and node-pool autoscaling kicks in. GKE uses several GCE related quotas like cpu , memory , disk , and gpu of a particular type, but it also uses lesser known quotas like \"Number of secondary ranges per VPC\". The resource related quotas are per region and sometimes per zone, so it's important to monitor your quota usage to avoid quota-capped resource exhaustion scenarios from negatively affecting your application's performance during a scale-up event or from preventing certain types of upgrade scenarios.","title":"Project Quotas"},{"location":"project_organization/#best-practices_2","text":"Monitor Quota Consumption - Smaller GKE clusters will most likely fit into the project quota defaults, but clusters of a dozen nodes or more may start to bump into the limits. Request Quota Increases Ahead of Time - Quota increases can take as much as 48 hrs to be approved, so it's best to plan ahead and ask early. Aim for 110% - If the single GKE cluster in a project uses 10, 32-core nodes , the total cpu cores needed is 320 or more. To give enough headroom to perform a \"blue/green\" cluster upgrade if needed (bringing up an identical cluster in parallel), the project quota should be at least 640 cpu cores in that region to facilitate that approach. Following the 110% guideline, this would actually be more like 700 cpus . This allows for two, full-sized clusters to be possible for a short duration while the workloads are migrated between the clusters . GCP APIs have Rate Limits - If your application makes 100s of requests per second or more to say, GCS or GCR, you may run into rate limits designed to protect overuse of the GCP APIs from a single customer affecting all customers. If you run into these, you may or may not be able to get them increased. Consider working with GCP support and implementing a different approach with your application.","title":"Best Practices"},{"location":"project_organization/#resources_2","text":"GCE Quotas and Limits Working with GCP Quotas","title":"Resources"},{"location":"workload_configuration/","text":"Workload Configuration Trusted Image Repos Deployment Settings and Best Practices Pod Security Policy Network Policy RBAC Dynamic Admission Control Secrets Management Pod Disruption Budgets LimitRanges Resource Control","title":"Workload Configuration"},{"location":"workload_configuration/#workload-configuration","text":"","title":"Workload Configuration"},{"location":"workload_configuration/#trusted-image-repos","text":"","title":"Trusted Image Repos"},{"location":"workload_configuration/#deployment-settings-and-best-practices","text":"","title":"Deployment Settings and Best Practices"},{"location":"workload_configuration/#pod-security-policy","text":"","title":"Pod Security Policy"},{"location":"workload_configuration/#network-policy","text":"","title":"Network Policy"},{"location":"workload_configuration/#rbac","text":"","title":"RBAC"},{"location":"workload_configuration/#dynamic-admission-control","text":"","title":"Dynamic Admission Control"},{"location":"workload_configuration/#secrets-management","text":"","title":"Secrets Management"},{"location":"workload_configuration/#pod-disruption-budgets","text":"","title":"Pod Disruption Budgets"},{"location":"workload_configuration/#limitranges","text":"","title":"LimitRanges"},{"location":"workload_configuration/#resource-control","text":"","title":"Resource Control"}]}