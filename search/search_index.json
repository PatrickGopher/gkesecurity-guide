{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Unofficial Google Kubernetes Engine (GKE) Security Guide Introduction Disclaimer This guide is not an official guide, is not endorsed by Google , and is comprised entirely from publicly available information. The information is provided \"as-is\", without warranty or fit for a particular purpose. You should fully understand the security implications of each recommendation before applying it in your environment. There are several books on the topic of getting Kubernetes Up and Running and even one specific to Kubernetes Security . If you are deploying in multiple platform environments like on-premise or across multiple cloud providers, those resources will be very useful. However, if you are deploying GKE clusters in GCP, you'll notice that there are a wide array of GCP and GKE-specific choices that need to be made. The goal of this guide is to help you prioritize and implement a security posture that meets your organization's needs while taking advantage of all the benefits of the GKE service. For each control and approach, the guide will attempt to help you understand the relative risks, relative likelihood, upfront cost and complexity, and ongoing cost and complexity of each security-related decision. Deep links to official GCP/GKE and Kubernetes documentation will be provided to guide further reading. Finally, links to hand-selected tools, books, and talk recordings will give additional support to the concepts for further exploration and understanding. Audience This guide is focused specifically on the following roles working with a GKE deployment: Operations - Those responsible for deploying and maintaining the GCP Project, Networking, and GKE Cluster lifecycle. Security - Those responsible for ensuring the operational environment adequately meets the organization's security standards and posture. Security-minded developers - Those deploying operational workloads into the GKE cluster looking to follow security best practices. Team Leads - Those managing teams implementing GKE clusters and responsible for prioritizing security with operational velocity. Structure of this Guide As each layer of a system builds upon the layer underneath, this guide begins with the foundation of all GCP services and explains how the structure of folders and projects in a GCP organization can help organize and guide security settings and permissions management. Next, it covers the types of network configurations that support GKE clusters for different isolation strategies. The core of the guide gets into cluster configuration, lifecycle and operations, observability, and the management of custom additions in the form of addons. With a solid base in place, the guide covers the deployment and configuration of workloads and container images in terms of security best practices. Finally, it covers certain security-related workflows such as auditing for conformance and handling security issues when an incident occurs. Each topic area will cover: A few sentences to paragraphs on what the topic or setting is and does. Why the topic or setting is important and the risks associated with it. The versions it applies to, where applicable. When you might want to prioritize adoption or implementation. The upfront cost and complexity of deploying the change or approach. The longer term cost and complexity of maintaining the system with this change or approach implemented. GKE vs Building Kubernetes Yourself For first time Kubernetes operators, it's a fantastic idea to follow Kubernetes the Hard Way by Kelsey Hightower and build a cluster from scratch. It will give you a deep understanding of the components, their configuration settings, and an appreciation for the complexity of the system. The very next thing you should do is leverage an approach to Kubernetes that ensures you and your team do as little of that work as possible. In many cases, this is best done by working with a managed Kubernetes service like GKE . Successful Kubernetes cluster deployments require solid execution in wide range of areas, and it's impossible to be execellent in all of them at once. From a business standpoint, you want your teams working on the problems that matter to your organization and to leave the boring operational work to the cloud service provider. Kubernetes version upgrades, security patches, operating system maintenance, logging and monitoring infrastructure, and more are all things you want to be building \"on top of\" and not having to \"build\" yourself first. Refer to the benefits of GKE for further reading. Kubernetes Security Maturity Throughout this guide, there will be dozens of security decisions and configuration tasks that you will be faced with prioritizing and implementing. It's important to have some higher level structure and frame of reference for which items are required early or later on in your journey to Kubernetes security maturity. Kubernetes for Enterprise Security Requirements really helps visually explain the progression path well. Summarizing the key areas from this talk: Infrastructure Security - Does the surrounding environment and Kubernetes cluster provide a solid foundation for running workloads securely? Software Supply Chain - Are the container images running in the cluster built from trusted sources and free of vulnerabilities? Container Runtime Security - Are there capabilities in place to monitor what is happening inside the containers? Additionally: Security Observability and Response - Are there capabilities for knowing when a security incident occurs and processes in place to respond and remediate? As you progress through this guide, each of the topic areas should map back to one or more of these key areas and help align your decision along these lines. Contributing This guide is a living document, and contributions in the form of issues and PRs are welcomed. If you are considering writing new content, please open an issue outlining what you'd like to write about, where it might fit in, and other details. If you found this guide useful, please consider donating your time attending and supporting your local cloud-native and Kubernetes-related meetups. The success or failure of Kubernetes and the CNCF ecosystem is largely dependent on you and how you help elevate others with your compassion, assistance, and inclusivity. About the Author(s) @BradGeesaman is an Independent Security Consultant currently focused on helping teams secure their GKE clusters in harmony with their desired operational velocity.","title":"Introduction"},{"location":"#the-unofficial-google-kubernetes-engine-gke-security-guide","text":"","title":"The Unofficial Google Kubernetes Engine (GKE) Security Guide"},{"location":"#introduction","text":"Disclaimer This guide is not an official guide, is not endorsed by Google , and is comprised entirely from publicly available information. The information is provided \"as-is\", without warranty or fit for a particular purpose. You should fully understand the security implications of each recommendation before applying it in your environment. There are several books on the topic of getting Kubernetes Up and Running and even one specific to Kubernetes Security . If you are deploying in multiple platform environments like on-premise or across multiple cloud providers, those resources will be very useful. However, if you are deploying GKE clusters in GCP, you'll notice that there are a wide array of GCP and GKE-specific choices that need to be made. The goal of this guide is to help you prioritize and implement a security posture that meets your organization's needs while taking advantage of all the benefits of the GKE service. For each control and approach, the guide will attempt to help you understand the relative risks, relative likelihood, upfront cost and complexity, and ongoing cost and complexity of each security-related decision. Deep links to official GCP/GKE and Kubernetes documentation will be provided to guide further reading. Finally, links to hand-selected tools, books, and talk recordings will give additional support to the concepts for further exploration and understanding.","title":"Introduction"},{"location":"#audience","text":"This guide is focused specifically on the following roles working with a GKE deployment: Operations - Those responsible for deploying and maintaining the GCP Project, Networking, and GKE Cluster lifecycle. Security - Those responsible for ensuring the operational environment adequately meets the organization's security standards and posture. Security-minded developers - Those deploying operational workloads into the GKE cluster looking to follow security best practices. Team Leads - Those managing teams implementing GKE clusters and responsible for prioritizing security with operational velocity.","title":"Audience"},{"location":"#structure-of-this-guide","text":"As each layer of a system builds upon the layer underneath, this guide begins with the foundation of all GCP services and explains how the structure of folders and projects in a GCP organization can help organize and guide security settings and permissions management. Next, it covers the types of network configurations that support GKE clusters for different isolation strategies. The core of the guide gets into cluster configuration, lifecycle and operations, observability, and the management of custom additions in the form of addons. With a solid base in place, the guide covers the deployment and configuration of workloads and container images in terms of security best practices. Finally, it covers certain security-related workflows such as auditing for conformance and handling security issues when an incident occurs. Each topic area will cover: A few sentences to paragraphs on what the topic or setting is and does. Why the topic or setting is important and the risks associated with it. The versions it applies to, where applicable. When you might want to prioritize adoption or implementation. The upfront cost and complexity of deploying the change or approach. The longer term cost and complexity of maintaining the system with this change or approach implemented.","title":"Structure of this Guide"},{"location":"#gke-vs-building-kubernetes-yourself","text":"For first time Kubernetes operators, it's a fantastic idea to follow Kubernetes the Hard Way by Kelsey Hightower and build a cluster from scratch. It will give you a deep understanding of the components, their configuration settings, and an appreciation for the complexity of the system. The very next thing you should do is leverage an approach to Kubernetes that ensures you and your team do as little of that work as possible. In many cases, this is best done by working with a managed Kubernetes service like GKE . Successful Kubernetes cluster deployments require solid execution in wide range of areas, and it's impossible to be execellent in all of them at once. From a business standpoint, you want your teams working on the problems that matter to your organization and to leave the boring operational work to the cloud service provider. Kubernetes version upgrades, security patches, operating system maintenance, logging and monitoring infrastructure, and more are all things you want to be building \"on top of\" and not having to \"build\" yourself first. Refer to the benefits of GKE for further reading.","title":"GKE vs Building Kubernetes Yourself"},{"location":"#kubernetes-security-maturity","text":"Throughout this guide, there will be dozens of security decisions and configuration tasks that you will be faced with prioritizing and implementing. It's important to have some higher level structure and frame of reference for which items are required early or later on in your journey to Kubernetes security maturity. Kubernetes for Enterprise Security Requirements really helps visually explain the progression path well. Summarizing the key areas from this talk: Infrastructure Security - Does the surrounding environment and Kubernetes cluster provide a solid foundation for running workloads securely? Software Supply Chain - Are the container images running in the cluster built from trusted sources and free of vulnerabilities? Container Runtime Security - Are there capabilities in place to monitor what is happening inside the containers? Additionally: Security Observability and Response - Are there capabilities for knowing when a security incident occurs and processes in place to respond and remediate? As you progress through this guide, each of the topic areas should map back to one or more of these key areas and help align your decision along these lines.","title":"Kubernetes Security Maturity"},{"location":"#contributing","text":"This guide is a living document, and contributions in the form of issues and PRs are welcomed. If you are considering writing new content, please open an issue outlining what you'd like to write about, where it might fit in, and other details. If you found this guide useful, please consider donating your time attending and supporting your local cloud-native and Kubernetes-related meetups. The success or failure of Kubernetes and the CNCF ecosystem is largely dependent on you and how you help elevate others with your compassion, assistance, and inclusivity.","title":"Contributing"},{"location":"#about-the-authors","text":"@BradGeesaman is an Independent Security Consultant currently focused on helping teams secure their GKE clusters in harmony with their desired operational velocity.","title":"About the Author(s)"},{"location":"audit_and_compliance/","text":"Audit and Compliance CIS Benchmarking Inspec-GCP and Inspec-K8s Cloud Asset Inventory Forseti Security CSCC","title":"Audit and Compliance"},{"location":"audit_and_compliance/#audit-and-compliance","text":"","title":"Audit and Compliance"},{"location":"audit_and_compliance/#cis-benchmarking","text":"","title":"CIS Benchmarking"},{"location":"audit_and_compliance/#inspec-gcp-and-inspec-k8s","text":"","title":"Inspec-GCP and Inspec-K8s"},{"location":"audit_and_compliance/#cloud-asset-inventory","text":"","title":"Cloud Asset Inventory"},{"location":"audit_and_compliance/#forseti-security","text":"","title":"Forseti Security"},{"location":"audit_and_compliance/#cscc","text":"","title":"CSCC"},{"location":"cloud_environment/","text":"Cloud Environment Identity and IAM Hierarchy At the foundation of your Google Cloud Platform (GCP) organization is a relationship with your trusted identity provider. Commonly, this is a Gsuite domain or a Cloud Identity installation. It's the basis of trust for how users and groups are managed throughout the GCP environments. It's where you audit logins/logouts, enforce MFA and session duration, and more. It's important to understand that in a GCP organization , where a user is granted permissions is just as important as what permissions are granted. For example, granting a user or service account a binding to an IAM role for Storage Admin at the project level allows that account to have all those permissions for all Google Cloud Storage (GCS) buckets in that project . Consider a somewhat realistic scenario: That project has GCS buckets for storing audit logs, VPC flow logs, and firewall logs. The security team is given Storage Admin permissions to manage these. Also in that same project , a development team wants to store some data used by their application. They too, get assigned Storage Admin permissions to that project . There are a few problems: Unless the security team is the one that made the IAM permissions assignment, they might not be aware of the exposure of their security log and event data. This jeopardizes the integrity of the information if needed for incident response and guaranteeing chain of custody. The development team now has access to this sensitive security information. Even though they may never access it or need to, this violates the principle of least privilege. Any time a new GCS bucket is added to this project , both teams will have access to it automatically. This can often be undesired behavior and tricky to untangle. Things get even more interesting when you consider a group of projects organized into a GCP Folder. Permissions granted to a Folder \"trickle down\" to the Folders and projects inside them. Because permissions are additive , a descendent project can't block or override those permissions. Therefore, careful consideration must be given to access granted at each level, and special attention is needed for IAM roles bound to users/groups/service accounts at the Folder or Organization levels. Some examples to consider: If a user is assigned the primitive role Owner at the Organization level, they will be Owner in every single project in the entire organization. The risk of compromise of those specific credentials (and by extension, that user's primary computing devices) is immediately increased to undesired levels. Granting Viewer at the Organization level allows reading all GCS buckets, all StackDriver logs, and viewing the configurations of most resources. In many cases, sensitive data is found in any or all of those places that can be used to escalate privileges. IAM \"Overcrowding\" The IAM page in the GCP Console tends to get \"crowded\" and harder to visually reason about when there are lots of individual bindings. Each IAM Role binding increases the cognitive load when reviewing permissions. Over time, this can get unwieldy and difficult to consolidate once systems and services are running in production. Best Practices Use a single Identity Provider - If all users and groups are originating from a single domain and are managed centrally, maintaining the proper IAM permissions throughout the GCP Organization is greatly simplified. Ensure all accounts use Multi-Factor Authentication (MFA) - Protecting the theft and misuse of GCP account credentials is the first and most important step to protecting your GCP infrastructure. If a set of credentials is stolen or a laptop lost, the window for those credentials to be valid is greatly reduced. Consolidate Users into Groups - Instead of granting individual users access, consider using a group, placing that user in that group, and granting access to the group. This way, if that person leaves the organization, it doesn't leave orphaned/unused accounts clogging up the IAM permissions listings that have to be manually removed using gcloud/UI. Also, when replacing that person who left, it's a simpler process of just adding the new person(s) to the same groups. No gcloud/UI changes are then necessary. Follow the Principle of Least Privilege - Take care when granting access Project-wide and even greater care when granting access at the Folder and Organization levels. There are often unintended consequences of over-granting permissions. Additionally, it's much harder to safely remove that binding as time goes on because inherited permissions might be required in a descendent project. Resources Creating and Managing GCP Organizations GCP Resource Hierarchy GSuite Security Cloud Identity Overview Cloud IAM Overview Organization, Folder, and Project Structure Resource hierarchy decisions made early on in your GCP journey tend to have a \"inertia\". Meaning, once they are configured a certain way, they become entrenched and harder to change as time goes on. If configured well, they can reduce administrative and security toil in managing resources and access to those resources in a clean, manageable way. If configured sub-optimally, you might feel like making administrative changes is overly complex and burdensome. Often, these decisions are often made in haste during design and implementation efforts \"just to get things working\". It might seem that there is pressure to get things perfect from the outset, but there is some level of built-in flexibility that you can take advantage of -- provided certain steps are taken. The two components of the GCP resource hierarchy that are trickiest to change are the organization and the non-empty project . The organization is the root of all resources, so it establishes the hierarchy. Projects cannot be renamed, and resources inside projects cannot be moved outside of projects . Folders can be made up to three levels deep, and projects can be moved around inside folders as needed. What this means: Permissions assigned at the organization level descend to everything below regardless of folder and project structure. Renaming projects or moving resources out of a project requires migration of resources, so careful thought to their name and what runs inside them is needed. Folders are a powerful tool in designing and adapting your IAM permissons hierarchy as needs change. Consider the following diagram of a GCP Organization: The organization is named example.com , and that is the root of the IAM hierarchy. Skipping over the folders tier for a second -- the projects are organized by operational environment which is very common and a best practice for keeping resources completely separated. Resources in the example-dev project have no access by default to example-test or example-prod . \"Fantastic!\" you might say, and copy this entire approach. However, depending on your organization's needs, this hierarchy might not be ideal! Let's break down the potential issues with the folders and the resources inside each project : In the example-dev project, instance_a and service_a may or may not be part of the same overall \"service\". If they both operate on data in bucket_a , for example, this makes sense. But if instance_a is managed by a separate team and shares nothing with service_a or bucket_a , you have a permissions scoping and \"attack blast radius\" problem. Permissions assigned on the example-dev project might be shared in undesirable ways between instance_a and service_a . In the event of a security compromise, attackers will be much more likely to obtain credentials with shared permissions and access nearby resources inside that same project. Finally, unless additional measures are taken with custom log sinks, the logs and metrics in Stackdriver will be going to the same project. Administrators and developers of the instance_a and service_a would be able to see each other's logs and metrics. In the example-test project, two compute instances and two GCS buckets are in place. If bucket_b and bucket_c are part of the same overall service, this makes sense. But if bucket_b serves data with PII in it and bucket_c is where VPC flow logs are stored, this increases the chance of accidental exposure if the Storage Viewer IAM Role is granted at the project level. The service accounts given the ability to read VPC flow logs may be able to read from the PII bucket and vice versa. A simple question to ask is, \"If the credentials for this user/group/service account were stolen without us knowing, what would they have access to?\" A service account with Storage Viewer granted at the example-test project means its credentials, if stolen or misused, would allow viewing the VPC flow logs and PII data in both GCS buckets. At the folder and project level, it may seem reasonable to name the folder after the team name. e.g. team-a-app1-dev . However, in practice, team names and organization charts in the real world tend to change more often than GCP resources. To remain flexible, it's much easier to use GCP groups to combine users into teams and then assign IAM permissions for that group at the desired level. Otherwise, you'll end up having team_a managing the team_b projects which will get very confusing. If your organization becomes large and has lots of geographic regions, a common practice is to have a top-level folder for regions like US , EU , etc. While this places no technical boundary on where the resources are deployed in descendent projects, it may make compliance auditing more straightforward where data must remain in certain regions or countries. Certain services, like GCS and Google Container Registry (GCR), don't have very granular IAM permissions available to them unless you want to maintain per-object ACLs. To avoid some of that additional overhead, a common practice is to make dedicated GCP projects just for certain types of common GCS buckets. Similarly, with GCR, container image push/pull permissions are coarse GCS \"storage\" permissions. Dedicated projects are therefore a near requirement for separate GCR registries. Highly privileged administrative components that have organization-wide permissions should always be in dedicated projects near the top of the organization . A common example is the project where Forseti Security components are installed. If this project was nested inside several levels of folders , an IAM Role granted higher in that structure would unintentially provide access to those systems. Best Practices Certain Decisions Have inertia - Some decisions like organization domain name and project names carry a lot of inertia and are hard to change without migrating resources, so it's important to decide on a naming convention for projects early and stick with it. Consider Central Control of Project Creation - If project creation is to be controlled centrally, you can use an Organization Access Policy to set which group has the Project Creator IAM Role. Avoid Embedding Team Names in GCP Resource Names - Map folder and project names more closely with service offerings and operational environments. Use groups to establish the concept of \"teams\" and assign groups the IAM permissions to the folders and projects they need. Use Folder Names at the Top Level Carefully - Consider using high level folders to group sub- folders and projects along boundaries that shouldn't change often but help guide organization and auditing. e.g. Geographic region. Use separate projects for each operational environment. It's common to append -dev , -test , and -prod to project names and then group them into a folder named for that service . e.g. The folder b2b-crm holds the projects named b2b-crm-dev , b2b-crm-test , and b2b-crm-prod . Resources Using Resource Hierarchy for Access Control Using IAM Securely Organization Access Policy Forseti Security Cluster Networking VPC (shared vs peering based on administration model) Once the GCP organization , folder , and project structure is organized according to the desired hierarchy, the next step is to structure the networking and IP address management to support your use case. As a small organization, it may make sense to have a single project with a single GKE cluster to start. Inside this project, a VPC network is created, a few subnets are declared, and a GKE cluster is deployed. If another cluster is needed for a completely separate purpose, another project can be created (with another VPC and set of subnets). If these clusters need to communicate privately and avoid egress network charges, a common choice is to use VPC peering which makes a two-way routing path between each VPC ( A - B ). When the third VPC/Cluster comes along, another VPC/Subnet/Cluster is needed. Now, to VPC peer all three VPCs, three total VPC peering connections are needed. A - B , B - C , and A - C . When the fourth cluster is desired, a total of six VPC peering configurations are needed. This becomes difficult to manage correctly and adds to ongoing maintenance costs for troubleshooting and configuration for each additional VPC. The most common solution to centralizing the management and configuration of VPC networks across multiple projects is the Shared VPC model. One project is defined as the \"host\" project (where the VPC and Subnets are managed) and other projects are defined as \"service\" projects (where the services/apps/GKE clusters are deployed). The \"service\" projects no longer have VPCs and Subnets in them and instead are only allowed to \"use\" those VPCs/subnets defined in the \"host\" project. This has a few advantages for security-conscious organizations: Using IAM permissions, it's now possible to granularly assign the ability to create projects, attach them to the Shared VPC, create/manage subnets and firewall rules, and the ability to \"use\" or \"attach to\" the subnets to different users/groups in each of the \"service\" projects. With centralized control of the networking components, it's easier to manage IP address space, avoid CIDR overlap problems, and maintain consistent and correct configurations. Owners of resources in the various \"service\" projects can simply leverage the networking infrastructure provided to them. In practice, this helps curtail \"project-network\" sprawl which is very hard to reign in after the fact. With the organization hierarchy set to have one project for each environment type to separate IAM permissions, a best practice GKE deployment has a GKE cluster for each environment type in its respective project. The natural tendency is to make a Shared VPC for a service offering (e.g. b2b-crm ) and attach the three projects for b2b-crm-dev , b2b-crm-test , and b2b-crm-prod to it. From an operational perspective, this might facilitate simpler environment-to-environment communications to move data between clusters or share hybrid/VPN connectivity needs. However, from a security perspective, this is akin to running dev , test , and prod right next to each other on the same shared network. This can unintentionally increase the \"blast radius\" of an attack as a compromised test cluster would have more possible avenues to pivot and communicate directly with the prod cluster, for instance. While it's possible to do this with sufficient security controls in place, the chance for misconfiguration is much higher. If the entire point of a Shared VPC is to reduce the number of VPCs to manage, how does that work when each environment and cluster shouldn't share a VPC? One method is to create a Shared VPC for related dev projects and clusters, one for test projects , and one for prod projects and clusters that need to communicate with each other. For instance, a project holding GCE instances sharing a VPC with a project running a GKE cluster as a part of the same total application \"stack\". GKE is known for using a large amount of private RFC 1918 IP address space, and for proper operation of routing and interconnectivity features, CIDR ranges should not be reused or overlap. Node IPs - Each GKE \"node\" (GCE instance) needs one IP for administration and control plane communications, and it uses the \"primary\" range in the subnet declared. If the cluster isn't to grow beyond 250 nodes, this can be a /24 . For ~1000 nodes, a /22 is needed. This CIDR range should be unique and not overlap or be reused anywhere. Pod IPs - By default, each node can run a max of 110 Pods. Each pod needs an IP that is unique, and so GKE slices up /24 CIDR ranges from the \"secondary\" subnet range to assign to each node. A 250 node cluster will use 250 x /24 CIDR blocks. In this case, a /16 is needed to handle this cluster (250 x 256 = 64K, closest is 2^8 = 65535). A ~1000 node cluster will need a /14 all to itself (1000 x 256 = 256K, closest is 2^18 = 262K). Cluster/Service IPs - The only CIDR range that can be reused is the Service CIDR. This is because it is never meant to be routed outside the GKE cluster. It's what is assigned to ClusterIP Service objects in Kubernetes which can be used to map friendly, internal DNS names to Pod IPs matching certain labels. A range of /20 provides just over 4,090 possible services in a single cluster. This should be more than sufficient for clusters below 500 nodes. If the prospect of assigning a /16 per cluster has the network team of a large organization nervous, consider using \"Flexible Pod CIDR\". In many situations, the resources needed by Pods and the size of the GKE Nodes makes the practical limit of pods per node far fewer than the max of 110. If the workloads for a cluster are such that no more than 50-60 pods will ever run on a node, the node will only use a max of half of the /24 CIDR assigned to it. The \"wasted\" 128 addresses per node can add up quickly. When creating a GKE Node Pool, specifying a MaxPodsPerNode of 64 or fewer will trigger GKE to assign a /25 from the secondary range instead of a /24 . The reason for not going to a /26 is because of the natural lifecycle of a Pod IP assignment during a deployment is that greater than 64 IPs might be in use for short periods as some pods are starting while others are still terminating. Best Practices Do not overlap CIDRs - In almost every GCP organization , there exists a need to route to each GKE cluster and between clusters, and the cleanest way to do this with the least amount of engineering is to plan ahead and avoid CIDR overlap. The only known exception is the GKE Service CIDR range. Use VPC Aliasing - Currently, this is in the process of becoming the default option. Enabling IP aliasing means that the GKE CNI plugin places Pods IPs on an \"alias\" interface of the underlying node. This reduces latency by reducing network \"hops\" as the GCP Network infrastructure is used to route Pod-to-Pod traffic and LoadBalancer-to-Pod traffic instead of always involving kube-proxy . Consider Using Flexible Pod CIDR - Especially on larger cluster sizes, this can greatly reduce \"CIDR waste\" and allow for future scaling and expansion of node pools with half or more of the original IP space needed with the default configuration. Take Note of Quotas/Limits - When it comes to Shared VPCs, a recent quota/limit on the number of secondary address ranges per VPC went from 5 to 30. This had an effect of limiting the total number of GKE clusters that could be deployed in a Shared VPC as the quotas/limits for the networking components are shared by the \"host\" project. Large organizations with 1000s of instances will want to be aware of the 15,000 VM per Network when combining large projects with large instance counts and GKE node counts. Resources Shared VPC Overview GKE with Shared VPC GKE Networking Overview GKE Alias IP Cluster Shared VPC Quotas and Limits GKE Flexible Pod CIDR","title":"Cloud Environment"},{"location":"cloud_environment/#cloud-environment","text":"","title":"Cloud Environment"},{"location":"cloud_environment/#identity-and-iam-hierarchy","text":"At the foundation of your Google Cloud Platform (GCP) organization is a relationship with your trusted identity provider. Commonly, this is a Gsuite domain or a Cloud Identity installation. It's the basis of trust for how users and groups are managed throughout the GCP environments. It's where you audit logins/logouts, enforce MFA and session duration, and more. It's important to understand that in a GCP organization , where a user is granted permissions is just as important as what permissions are granted. For example, granting a user or service account a binding to an IAM role for Storage Admin at the project level allows that account to have all those permissions for all Google Cloud Storage (GCS) buckets in that project . Consider a somewhat realistic scenario: That project has GCS buckets for storing audit logs, VPC flow logs, and firewall logs. The security team is given Storage Admin permissions to manage these. Also in that same project , a development team wants to store some data used by their application. They too, get assigned Storage Admin permissions to that project . There are a few problems: Unless the security team is the one that made the IAM permissions assignment, they might not be aware of the exposure of their security log and event data. This jeopardizes the integrity of the information if needed for incident response and guaranteeing chain of custody. The development team now has access to this sensitive security information. Even though they may never access it or need to, this violates the principle of least privilege. Any time a new GCS bucket is added to this project , both teams will have access to it automatically. This can often be undesired behavior and tricky to untangle. Things get even more interesting when you consider a group of projects organized into a GCP Folder. Permissions granted to a Folder \"trickle down\" to the Folders and projects inside them. Because permissions are additive , a descendent project can't block or override those permissions. Therefore, careful consideration must be given to access granted at each level, and special attention is needed for IAM roles bound to users/groups/service accounts at the Folder or Organization levels. Some examples to consider: If a user is assigned the primitive role Owner at the Organization level, they will be Owner in every single project in the entire organization. The risk of compromise of those specific credentials (and by extension, that user's primary computing devices) is immediately increased to undesired levels. Granting Viewer at the Organization level allows reading all GCS buckets, all StackDriver logs, and viewing the configurations of most resources. In many cases, sensitive data is found in any or all of those places that can be used to escalate privileges. IAM \"Overcrowding\" The IAM page in the GCP Console tends to get \"crowded\" and harder to visually reason about when there are lots of individual bindings. Each IAM Role binding increases the cognitive load when reviewing permissions. Over time, this can get unwieldy and difficult to consolidate once systems and services are running in production.","title":"Identity and IAM Hierarchy"},{"location":"cloud_environment/#best-practices","text":"Use a single Identity Provider - If all users and groups are originating from a single domain and are managed centrally, maintaining the proper IAM permissions throughout the GCP Organization is greatly simplified. Ensure all accounts use Multi-Factor Authentication (MFA) - Protecting the theft and misuse of GCP account credentials is the first and most important step to protecting your GCP infrastructure. If a set of credentials is stolen or a laptop lost, the window for those credentials to be valid is greatly reduced. Consolidate Users into Groups - Instead of granting individual users access, consider using a group, placing that user in that group, and granting access to the group. This way, if that person leaves the organization, it doesn't leave orphaned/unused accounts clogging up the IAM permissions listings that have to be manually removed using gcloud/UI. Also, when replacing that person who left, it's a simpler process of just adding the new person(s) to the same groups. No gcloud/UI changes are then necessary. Follow the Principle of Least Privilege - Take care when granting access Project-wide and even greater care when granting access at the Folder and Organization levels. There are often unintended consequences of over-granting permissions. Additionally, it's much harder to safely remove that binding as time goes on because inherited permissions might be required in a descendent project.","title":"Best Practices"},{"location":"cloud_environment/#resources","text":"Creating and Managing GCP Organizations GCP Resource Hierarchy GSuite Security Cloud Identity Overview Cloud IAM Overview","title":"Resources"},{"location":"cloud_environment/#organization-folder-and-project-structure","text":"Resource hierarchy decisions made early on in your GCP journey tend to have a \"inertia\". Meaning, once they are configured a certain way, they become entrenched and harder to change as time goes on. If configured well, they can reduce administrative and security toil in managing resources and access to those resources in a clean, manageable way. If configured sub-optimally, you might feel like making administrative changes is overly complex and burdensome. Often, these decisions are often made in haste during design and implementation efforts \"just to get things working\". It might seem that there is pressure to get things perfect from the outset, but there is some level of built-in flexibility that you can take advantage of -- provided certain steps are taken. The two components of the GCP resource hierarchy that are trickiest to change are the organization and the non-empty project . The organization is the root of all resources, so it establishes the hierarchy. Projects cannot be renamed, and resources inside projects cannot be moved outside of projects . Folders can be made up to three levels deep, and projects can be moved around inside folders as needed. What this means: Permissions assigned at the organization level descend to everything below regardless of folder and project structure. Renaming projects or moving resources out of a project requires migration of resources, so careful thought to their name and what runs inside them is needed. Folders are a powerful tool in designing and adapting your IAM permissons hierarchy as needs change. Consider the following diagram of a GCP Organization: The organization is named example.com , and that is the root of the IAM hierarchy. Skipping over the folders tier for a second -- the projects are organized by operational environment which is very common and a best practice for keeping resources completely separated. Resources in the example-dev project have no access by default to example-test or example-prod . \"Fantastic!\" you might say, and copy this entire approach. However, depending on your organization's needs, this hierarchy might not be ideal! Let's break down the potential issues with the folders and the resources inside each project : In the example-dev project, instance_a and service_a may or may not be part of the same overall \"service\". If they both operate on data in bucket_a , for example, this makes sense. But if instance_a is managed by a separate team and shares nothing with service_a or bucket_a , you have a permissions scoping and \"attack blast radius\" problem. Permissions assigned on the example-dev project might be shared in undesirable ways between instance_a and service_a . In the event of a security compromise, attackers will be much more likely to obtain credentials with shared permissions and access nearby resources inside that same project. Finally, unless additional measures are taken with custom log sinks, the logs and metrics in Stackdriver will be going to the same project. Administrators and developers of the instance_a and service_a would be able to see each other's logs and metrics. In the example-test project, two compute instances and two GCS buckets are in place. If bucket_b and bucket_c are part of the same overall service, this makes sense. But if bucket_b serves data with PII in it and bucket_c is where VPC flow logs are stored, this increases the chance of accidental exposure if the Storage Viewer IAM Role is granted at the project level. The service accounts given the ability to read VPC flow logs may be able to read from the PII bucket and vice versa. A simple question to ask is, \"If the credentials for this user/group/service account were stolen without us knowing, what would they have access to?\" A service account with Storage Viewer granted at the example-test project means its credentials, if stolen or misused, would allow viewing the VPC flow logs and PII data in both GCS buckets. At the folder and project level, it may seem reasonable to name the folder after the team name. e.g. team-a-app1-dev . However, in practice, team names and organization charts in the real world tend to change more often than GCP resources. To remain flexible, it's much easier to use GCP groups to combine users into teams and then assign IAM permissions for that group at the desired level. Otherwise, you'll end up having team_a managing the team_b projects which will get very confusing. If your organization becomes large and has lots of geographic regions, a common practice is to have a top-level folder for regions like US , EU , etc. While this places no technical boundary on where the resources are deployed in descendent projects, it may make compliance auditing more straightforward where data must remain in certain regions or countries. Certain services, like GCS and Google Container Registry (GCR), don't have very granular IAM permissions available to them unless you want to maintain per-object ACLs. To avoid some of that additional overhead, a common practice is to make dedicated GCP projects just for certain types of common GCS buckets. Similarly, with GCR, container image push/pull permissions are coarse GCS \"storage\" permissions. Dedicated projects are therefore a near requirement for separate GCR registries. Highly privileged administrative components that have organization-wide permissions should always be in dedicated projects near the top of the organization . A common example is the project where Forseti Security components are installed. If this project was nested inside several levels of folders , an IAM Role granted higher in that structure would unintentially provide access to those systems.","title":"Organization, Folder, and Project Structure"},{"location":"cloud_environment/#best-practices_1","text":"Certain Decisions Have inertia - Some decisions like organization domain name and project names carry a lot of inertia and are hard to change without migrating resources, so it's important to decide on a naming convention for projects early and stick with it. Consider Central Control of Project Creation - If project creation is to be controlled centrally, you can use an Organization Access Policy to set which group has the Project Creator IAM Role. Avoid Embedding Team Names in GCP Resource Names - Map folder and project names more closely with service offerings and operational environments. Use groups to establish the concept of \"teams\" and assign groups the IAM permissions to the folders and projects they need. Use Folder Names at the Top Level Carefully - Consider using high level folders to group sub- folders and projects along boundaries that shouldn't change often but help guide organization and auditing. e.g. Geographic region. Use separate projects for each operational environment. It's common to append -dev , -test , and -prod to project names and then group them into a folder named for that service . e.g. The folder b2b-crm holds the projects named b2b-crm-dev , b2b-crm-test , and b2b-crm-prod .","title":"Best Practices"},{"location":"cloud_environment/#resources_1","text":"Using Resource Hierarchy for Access Control Using IAM Securely Organization Access Policy Forseti Security","title":"Resources"},{"location":"cloud_environment/#cluster-networking","text":"VPC (shared vs peering based on administration model) Once the GCP organization , folder , and project structure is organized according to the desired hierarchy, the next step is to structure the networking and IP address management to support your use case. As a small organization, it may make sense to have a single project with a single GKE cluster to start. Inside this project, a VPC network is created, a few subnets are declared, and a GKE cluster is deployed. If another cluster is needed for a completely separate purpose, another project can be created (with another VPC and set of subnets). If these clusters need to communicate privately and avoid egress network charges, a common choice is to use VPC peering which makes a two-way routing path between each VPC ( A - B ). When the third VPC/Cluster comes along, another VPC/Subnet/Cluster is needed. Now, to VPC peer all three VPCs, three total VPC peering connections are needed. A - B , B - C , and A - C . When the fourth cluster is desired, a total of six VPC peering configurations are needed. This becomes difficult to manage correctly and adds to ongoing maintenance costs for troubleshooting and configuration for each additional VPC. The most common solution to centralizing the management and configuration of VPC networks across multiple projects is the Shared VPC model. One project is defined as the \"host\" project (where the VPC and Subnets are managed) and other projects are defined as \"service\" projects (where the services/apps/GKE clusters are deployed). The \"service\" projects no longer have VPCs and Subnets in them and instead are only allowed to \"use\" those VPCs/subnets defined in the \"host\" project. This has a few advantages for security-conscious organizations: Using IAM permissions, it's now possible to granularly assign the ability to create projects, attach them to the Shared VPC, create/manage subnets and firewall rules, and the ability to \"use\" or \"attach to\" the subnets to different users/groups in each of the \"service\" projects. With centralized control of the networking components, it's easier to manage IP address space, avoid CIDR overlap problems, and maintain consistent and correct configurations. Owners of resources in the various \"service\" projects can simply leverage the networking infrastructure provided to them. In practice, this helps curtail \"project-network\" sprawl which is very hard to reign in after the fact. With the organization hierarchy set to have one project for each environment type to separate IAM permissions, a best practice GKE deployment has a GKE cluster for each environment type in its respective project. The natural tendency is to make a Shared VPC for a service offering (e.g. b2b-crm ) and attach the three projects for b2b-crm-dev , b2b-crm-test , and b2b-crm-prod to it. From an operational perspective, this might facilitate simpler environment-to-environment communications to move data between clusters or share hybrid/VPN connectivity needs. However, from a security perspective, this is akin to running dev , test , and prod right next to each other on the same shared network. This can unintentionally increase the \"blast radius\" of an attack as a compromised test cluster would have more possible avenues to pivot and communicate directly with the prod cluster, for instance. While it's possible to do this with sufficient security controls in place, the chance for misconfiguration is much higher. If the entire point of a Shared VPC is to reduce the number of VPCs to manage, how does that work when each environment and cluster shouldn't share a VPC? One method is to create a Shared VPC for related dev projects and clusters, one for test projects , and one for prod projects and clusters that need to communicate with each other. For instance, a project holding GCE instances sharing a VPC with a project running a GKE cluster as a part of the same total application \"stack\". GKE is known for using a large amount of private RFC 1918 IP address space, and for proper operation of routing and interconnectivity features, CIDR ranges should not be reused or overlap. Node IPs - Each GKE \"node\" (GCE instance) needs one IP for administration and control plane communications, and it uses the \"primary\" range in the subnet declared. If the cluster isn't to grow beyond 250 nodes, this can be a /24 . For ~1000 nodes, a /22 is needed. This CIDR range should be unique and not overlap or be reused anywhere. Pod IPs - By default, each node can run a max of 110 Pods. Each pod needs an IP that is unique, and so GKE slices up /24 CIDR ranges from the \"secondary\" subnet range to assign to each node. A 250 node cluster will use 250 x /24 CIDR blocks. In this case, a /16 is needed to handle this cluster (250 x 256 = 64K, closest is 2^8 = 65535). A ~1000 node cluster will need a /14 all to itself (1000 x 256 = 256K, closest is 2^18 = 262K). Cluster/Service IPs - The only CIDR range that can be reused is the Service CIDR. This is because it is never meant to be routed outside the GKE cluster. It's what is assigned to ClusterIP Service objects in Kubernetes which can be used to map friendly, internal DNS names to Pod IPs matching certain labels. A range of /20 provides just over 4,090 possible services in a single cluster. This should be more than sufficient for clusters below 500 nodes. If the prospect of assigning a /16 per cluster has the network team of a large organization nervous, consider using \"Flexible Pod CIDR\". In many situations, the resources needed by Pods and the size of the GKE Nodes makes the practical limit of pods per node far fewer than the max of 110. If the workloads for a cluster are such that no more than 50-60 pods will ever run on a node, the node will only use a max of half of the /24 CIDR assigned to it. The \"wasted\" 128 addresses per node can add up quickly. When creating a GKE Node Pool, specifying a MaxPodsPerNode of 64 or fewer will trigger GKE to assign a /25 from the secondary range instead of a /24 . The reason for not going to a /26 is because of the natural lifecycle of a Pod IP assignment during a deployment is that greater than 64 IPs might be in use for short periods as some pods are starting while others are still terminating.","title":"Cluster Networking"},{"location":"cloud_environment/#best-practices_2","text":"Do not overlap CIDRs - In almost every GCP organization , there exists a need to route to each GKE cluster and between clusters, and the cleanest way to do this with the least amount of engineering is to plan ahead and avoid CIDR overlap. The only known exception is the GKE Service CIDR range. Use VPC Aliasing - Currently, this is in the process of becoming the default option. Enabling IP aliasing means that the GKE CNI plugin places Pods IPs on an \"alias\" interface of the underlying node. This reduces latency by reducing network \"hops\" as the GCP Network infrastructure is used to route Pod-to-Pod traffic and LoadBalancer-to-Pod traffic instead of always involving kube-proxy . Consider Using Flexible Pod CIDR - Especially on larger cluster sizes, this can greatly reduce \"CIDR waste\" and allow for future scaling and expansion of node pools with half or more of the original IP space needed with the default configuration. Take Note of Quotas/Limits - When it comes to Shared VPCs, a recent quota/limit on the number of secondary address ranges per VPC went from 5 to 30. This had an effect of limiting the total number of GKE clusters that could be deployed in a Shared VPC as the quotas/limits for the networking components are shared by the \"host\" project. Large organizations with 1000s of instances will want to be aware of the 15,000 VM per Network when combining large projects with large instance counts and GKE node counts.","title":"Best Practices"},{"location":"cloud_environment/#resources_2","text":"Shared VPC Overview GKE with Shared VPC GKE Networking Overview GKE Alias IP Cluster Shared VPC Quotas and Limits GKE Flexible Pod CIDR","title":"Resources"},{"location":"cluster_addons/","text":"Cluster Add-ons Anthos Configuration Management Binary Authorization GKE Sandbox (gVisor) Workload Identity","title":"Cluster Add-ons"},{"location":"cluster_addons/#cluster-add-ons","text":"","title":"Cluster Add-ons"},{"location":"cluster_addons/#anthos-configuration-management","text":"","title":"Anthos Configuration Management"},{"location":"cluster_addons/#binary-authorization","text":"","title":"Binary Authorization"},{"location":"cluster_addons/#gke-sandbox-gvisor","text":"","title":"GKE Sandbox (gVisor)"},{"location":"cluster_addons/#workload-identity","text":"","title":"Workload Identity"},{"location":"cluster_configuration/","text":"Cluster Configuration The GKE Cluster Architecture Google Kubernetes Engine makes several key architectural decisions for you that may differ from other Kubernetes cluster installations. These choices are made in the interest of ease of use, operational simplicity, and security: There is no direct access to the control plane systems - GKE manages the GCE instances running etcd and the api server and supporting components, and does not expose them to you or an attacker via SSH, for example. In exchange for giving up direct control over all API server configuration flags, your GKE cluster offers very strong protection of the core components and sensitive data in etcd automatically. This means your control over the control plane is restricted to the configuration options on the GKE cluster object. Upgrades are handled via GKE Operations - Upgrades and downgrades are handled via GKE operations through API calls. GKE versions (which map to Kubernetes versions) and worker node OS security and version upgrades are all core features of the built-in upgrade process. The default worker node operating system is COS - Container-optimized OS is a hardened, minimal operating system engineered specifically to run containers and to function as a Kubernetes worker node . While SSH is enabled by default and integrates with GCP SSH functionality, it's goal is to make SSHing into each worker node unnecessary by handling patching and upgrades as a part of the node pool lifecycle. There is an Ubuntu worker node option for supporting unique use cases, but it requires additional administration in terms of upgrades and security patches. The CNI is not configurable - The native GKE CNI is used to connect worker nodes and pods to the GCP Network and is not currently replacable. Network Policy enforcement is done via Calico - Enabling Network Policy enforcement installs a Calico deployment and daemonset as a managed addon for you. Logging and Monitoring is done via Stackdriver - By default, all logs and performance metrics are sent to Stackdriver in the current GCP project . Resources GKE Architecture GKE Upgrades Container-Optimized OS GKE CNI GKE Stackdriver Geographical Placement and Cluster Types GKE clusters are by default \"zonal\" clusters. That is, a single GCE instance running the control plane components is deployed in the same GCP zone as the node-pool nodes. \"Regional\" GKE clusters are deployed across three zones in the same region. Three GCE instances running the control plane components are deployed (one per zone ) behind a single IP and load balancer . The node-pools spread evenly across the zones with a minimum of one node per zone . The key benefits to a \"regional\" cluster : The cluster control plane can handle a single GCP zone failure more gracefully. When the control plane is being upgraded, only one instance is down at a time which leaves two remaining instances. Upgrades on \"zonal\" clusters means a 4-10 minute downtime of the API/control plane while that takes place where you can't use kubectl to interact with your cluster . Since there is no additional charge for running a \"regional\" GKE cluster with a highly-available control plane, why use a \"zonal\" cluster ? The two primary reasons are to save on \"cross-zone\" network traffic costs and to support specific GPU node-pool needs. Persistent Disks The default StorageClass defines persistent disks that are \"zonal\". You may want to add a new StorageClass that makes \"regional\" persistent disks available for pod workloads that get rescheduled on another node in a different zone to still be able to access them. Best Practices Use Regional Clusters - Unless you have specific needs that force you to use a \"zonal\" cluster , using \"regional\" clusters offers the best redundancy and availablility for a minor increase in network traffic costs for the majority of use cases. Offer a Regional Persistent Disk StorageClass - Allows pods to attach and access persistent disk volumes regardless of where they are scheduled inside the cluster. This prevents a zone failure from allowing a pod to be rescheduled and mount that disk on a node in another zone . Resources GKE Regional Clusters Google Cloud IAM and Kubernetes RBAC Google Cloud Identity and Access Management (IAM) - The system in GCP that grants permissions via GCP IAM Roles to users and service accounts to access GCP APIs. Kubernetes Role-Based Access Control (RBAC) - The native system inside Kubernetes that grants permissions via Kubernetes roles and clusterroles to users and service accounts to access the Kubernetes API Server. The two APIs that can be used to interact with the GKE service and a GKE cluster are: GCP GKE API (container.googleapis.com) - Used to create/update/delete the cluster and node pools that comprise the GKE cluster and to obtain connection and credential information for how to access a given cluster . Kubernetes API of the cluster - The unique Kubernetes API Server endpoint running on the GKE Control Plane systems for a specific cluster . It controls resources and access to resources running inside the cluster. This means that there are two main categories of permissions that you have to consider: Cluster Administration - Permissions associated with administering the cluster itself. Cluster Usage - Permissions associated with granting what is allowed to run inside the cluster. There are a couple key points to understand about how Cloud IAM and Kubernetes RBAC can be used to grant permissions in GKE: GCP Cloud IAM is administered at the project level, and the granted permissions apply to all GKE clusters in the project . They remain in-place even if a cluster is deleted from the project . Kubernetes RBAC permissions are administered per- cluster and are stored inside each cluster . When that cluster is destroyed, those permissions are also destroyed. GCP Cloud IAM permissions have no concept of Kubernetes namespaces , so the permissions granted apply for all namespaces . Kubernetes RBAC can be used to grant permissions to access resources in all namespaces or only in specific namespaces . Kubernetes RBAC cannot be used to grant permissions to the GCP GKE API (container.googleapis.com). This is performed solely by Cloud IAM. Both systems are additive in that they only grant permissions. They cannot remove or negate permissions from themselves or each other. IAM and RBAC combine inside the cluster When accessing a resource (e.g. list pods in the default namespace) via the Kubernetes API of a GKE cluster , it can be granted via IAM or RBAC. They are effectively combined. If either grants access, the request is permitted. Best Practices There are several predefined IAM roles for GKE clusters that can ease administration as you are getting started with GKE. However, the permissions they grant are often overly broad and violate the priciple of least privileges. You will want to create custom IAM roles with just the permissions needed, but remember that IAM permissions apply to all namespaces and cannot be limited to a single namespace . To maintain least privilege, it's recommended to leverage a minimal IAM role to gain access to the GKE API and then use Kubernetes RBAC roles and clusterroles to define what resources they can access inside the cluster . In order for users and service accounts to perform a gcloud container clusters get-credentials call to generate a valid kubeconfig for a GKE cluster , they need to have the following permissions: container.apiServices.get container.apiServices.list container.clusters.get container.clusters.list container.clusters.getCredentials If these permissions are grouped into a custom IAM role , that IAM role can be conveniently bound to a Gsuite/Cloud Identity group which includes all users that need access to the cluster . From this point, the users and service accounts can be granted access to resources as needed with cluster -wide or per- namespace granularity. This has the benefit of minimizing IAM changes needed, ensuring access granted is per- cluster , giving the highest granularity, and making troubleshooting permissions an RBAC-only process. With the above approach of deferring nearly all permissions to in- cluster RBAC instead of IAM, there is one exception: assigning the Kubernetes Engine Admin predefined IAM role at the project or folder level to a small number of trusted administrators to ensure that they don't accidentally remove their own access via an RBAC configuration mistake. Resources GKE IAM and RBAC Predefined GKE Roles GKE RBAC Cluster Access Control Plane Access The GCE instances running the control plane components like the kube-apiserver and etcd are not directly accessible via SSH and the GCE API. They don't appear in gcloud command outputs nor in the GCP console. The GKE control plane is only accessible via the exposed Kubernetes API on tcp/443 . By default, the API is assigned a public IP address and has firewall rules that allow access from 0.0.0.0/0 . While this is conducive to ease of use and access from anywhere, it might not meet your security requirements. Warning Sufficient access to the Kubernetes API equates to root on all the worker nodes . Protecting the Kubernetes API from access misconfiguration, denial-of-service, and exploitation should be high on a security team's priority list. There are controls available to improve on the default configuration, and they are covered in the next section. Worker Node Access Because GKE nodes are simply GCE instances inside instance groups , they are accessible via SSH in accordance with the normal GCE routing and VPC firewall rules. The standard GCE methods for using gcloud compute ssh to gain access to the underlying operating system work as expected. There are two things to consider when granting SSH access to nodes : Does SSH access need to be available publicly? In many cases, additional firewall source restrictions are useful in limiting the allowed subnets from which SSH access can be initiated. For instance, from a set of external office IP addresses or from a specific VPC subnet designated for management purposes. Several primitive IAM roles like Owner , Editor , Dataproc Service Agent , Compute OS Admin Login , Compute Admin , and Compute Instance Admin include the compute.instances.osAdminLogin permission. Users and service accounts with those permissions and network level access can SSH into the worker nodes and gain root permissions at the operating system level. Warning Gaining root access to a GKE worker node allows that user to view all secrets attached to pods running on that node . Those secrets may include tokens that belong to service accounts which have elevated permissions to the Kubernetes API. With that token , that user can access the Kubernetes API as that service account . If that service account has permissions to modify Role-Based Access Control or to view all secrets , that commonly equates to full cluster access (aka \"cluster admin\"). As \"cluster admin\" has full control of the Kubernetes API, that equates to root on all worker nodes . Cluster Settings For a full list of cluster configuration level items, the google_container_cluster terraform provider documentation is a fantastic resource for a list of each setting and its purpose. The list below aims to cover the availability and security-related items with additional supporting guidance: Location - The region (\"us-central1\") or zone (\"us-central1-a\") where the cluster will be located. Clusters cannot span multiple regions . If a region is specified, the control plane will have an instance in each of the three zones . If a zone is specified, the control plane will have a single instance in that zone only. Node Locations - The list of one to three zones where the nodes will be located, and it must be in the same region . Recommend: specifying a region to obtain a fault-tolerant control plane. Mix-and-Match If node locations is specified, it overrides the default behavior. It's possible to have a regional cluster but then use node locations to restrict the zones to only one or two instead of the default of three. It's also possible to create a zonal cluster with a single control plane instance but then use node locations to select one to three zones where the nodes will be located. Typically, this feature is used to help guide clusters into configurations to align with large CPU or GPU quota needs. Addons Config HTTP Load Balancing - Enabled by default, this installs the ingress-gce HTTP/S Load Balancing and Ingress Controller. It can be disabled to allow for installation of a different ingress controller solution or left implemented but unused. Recommend: enabled. Kubernetes Dashboard - Now disabled by default, this installs a Kubernetes dashboard deployment inside the cluster . Historically, the dashboard has been misconfigured and has had a small number of security issues that have led to compromises. With the GCP console providing all the dashboard features needed, this can and should be disabled. Clusters that were originally created in the Kubernetes 1.6 era and upgraded to the present may still have this enabled. Recommend: Disabled. Network Policy - Disabled by default, this installs the Calico CNI components which enables the configuration of networkpolicy resources in the Kubernetes API that can restrict pod'-to- pod communication. It is \"safe\" to enable by default as the default effect of the networkpolicy configuration in Kubernetes is \"allow all traffic\" until a networkpolicy` is defined. Recommend: Enabled. Istio - Disabled by default, this installs the Google-managed deployment of Istio inside the cluster . Istio can provide interesting security-related features like \"mutual TLS\" encryption for traffic between pods in the mesh , deep traffic inspection in the form of network traces, and granular role-based permissions for egress network traffic and Layer 7 access controls. In terms of complexity, Istio adds over 50 custom resource definitions that a security team will need to analyze and understand to have awareness of the security posture of the cluster . Recommend: \"Disabled\" until the features are needed. Database Encryption - Disabled by default, this enables an integration with Cloud KMS to have the API server access a Cloud KMS key to encrypt and decrypt the contents of secrets as they are stored and accessed from etcd . In environments where the control plane nodes are potentially accessible, this makes the compromise of a dedicated etcd system or etcd backup more difficult to extract the secrets in plain-text. In GKE, the control plane systems are not accessible directly by users or attackers, so this adds marginal benefit aside from satisfying regulatory requirements of \"databases must encrypt sensitive records at the row-level\". Recommend: \"Disabled\" unless compliance requirements dictate. Default Max Pods Per Node - Controls how many pods can run on a single node. The default is the hard-coded maximum of 110, but this can be reduced to 64, 32, 16, or 8 if the workload profile is known. This causes the subnet allocated to each GKE node to go from a /24 down to a /25 , /26 , and so on, and it can greatly reduce IP address consumption of precious RFC1918 space. Recommend: the default of 110 unless specific IP space or capacity needs require smaller. Binary Authorization - Disabled by default. This addon enables an admission controller that validates pod specs against a policy that defines which container image repositories are allowed and/or validates that container images have a valid PGP signature before allowing them to run inside the cluster. Can be enabled safely as the default policy is to \"allow all\" until configured. Recommend: Enabled. Kubernetes Alpha - Disabled by default. Do not enable on production clusters as alpha clusters are deleted automatically after 30 days by GKE. Recommend: Disabled. Legacy ABAC - Disabled by default, this is a legacy permissions mechanism that is largely unused in the Kubernetes community. The default role-based access control (RBAC) mechanism is enabled by default and is preferred. Recommend: Disabled. Logging Service - Enabled by default, this installs and manages a daemonset that collects and sends all pod logs to Stackdriver for processing, troubleshooting, and analysis/auditing purposes. Recommend: logging.googleapis.com/kubernetes Maintenance Policy - Defines a preferred 4-hour time window (in UTC) for when GKE should perform automatic or security-related upgrades. Recommend: setting a 4-hour UTC window with the least workload impact and overlap with operational on-call rotations. Master Authorized Networks - By default, the implicit allowed CIDR range is 0.0.0.0/0 for which source IPs can connect to the Kubernetes API. You may specify up to 50 different CIDR ranges. This setting can be changed as needed without affecting the lifecycle of the running cluster , and it is the most common method for reducing the scope of who can reach the Kubernetes cluster API to a smaller set of IPs. In the event of a vulnerability in the API server, enabling a small list of CIDRs to a known set can reduce the risk of an organization delaying maintenance operations to a time that is less busy or risky. Recommend: Configuring a short list of allowed CIDRs. Monitoring Service - Enabled by default, this installs and manages a daemonset that collects and sends all metrics to Stackdriver for troubleshooting and analysis. Recommend: monitoring.googleapis.com/kubernetes Pod Security Policy - Disabled by default, this enables the admission controller used to validate the specifications of pods to prevent insecure or \"privileged\" pods from being created that allow trivial escaping to the underlying host as root and bypassing other security mechanisms like networkpolicy . Recommend: Enabled with extensive testing OR implementing the same features with an Open Policy Agent/Gatekeeper validating admission controller. Authenticator Groups - Disabled by default, this enables RBAC to use Google Groups in RoleBinding and ClusterRoleBinding resources instead of having to specify each user or serviceaccount individually. For example, a Gsuite administrator creates a group called gke-security-groups@mydomain.com and places groups named gke-admins@ and gke-developers@ inside it. Passing gke-security-groups@mydomain.com to this setting allows RBAC to reference/lookup the gke-admins@mydomain.com and/or gke-developers@mydomain.com when evaluating access in the API to resources. Recommend: Enabled if using Gsuite/Google Groups to support easier to manage RBAC policies. Private Cluster - Master IPv4 CIDR Range - The RFC1918 subnet that the control plane should pick an IP address from when assigning a private IP. Recommend: a /28 that does not overlap with any other IP space in use. Enable Private Endpoint - \"false\" by default, setting this to \"true\" instructs the GKE control plane IP to be selected from the master CIDR range and does not expose a public IP address. Recommend: true Enable Private Nodes - \"false\" by default, setting this to \"true\" instructs the GKE nodes to not be assigned a public IP address. Recommend: true Private Clusters There is a common misconception that a \"private cluster\" is one that has worker nodes with private IP addresses. This is only half-correct. A true \"private cluster\" is one that has both the control plane and nodes using private IP addresses, and both of the above settings are necessary to achieve that improved security posture. When these two settings are enabled and combined with a small list of master authorized networks, the attack surface of the Kubernetes API can be significantly reduced. Remove Default Node Pool - By default, GKE deploys a node pool to be able to run pods . However, its lifecycle is tied to that of the cluster object. Meaning, certain changes may cause the entire cluster to be recreated. It's recommended that you disable/remove the default node pool and explicitly declare node pools with the desired settings so that they keep their lifecycles independent from the control plane/cluster. Workload Identity - Disabled by default, this enables the Workload Identity addon and by specifying which Identity \"domain\" to use. Without Workload Identity, pods wanting to reach GCP APIs would be granted the credentials attached to the underlying GCE instance . The underlying GCE instance has permissions to send logs, send metrics, read from Cloud Storage/Container Registries, and more. Historically, the nodes were assigned the default compute service account which was assigned Project Editor permissions. This meant that every pod could potentially have Project Editor access! With Workload Identity, a daemonset on every worker node intercepts all calls for instance credentials to the metadata API. If the Kubernetes service account attached to that pod has been granted a specific binding to a Google service account , then the dynamic crednetials for that specific Google service account will be returned to the pod . In essence, this is a way to map Kubernetes service accounts to Google service accounts and can remove the need for exporting Google service account keys in JSON format and storing them manually inside Kubernetes secrets . Recommend: Enabled. IntraNode Visibility - Disabled by default, this enables the network traffic on the pod network to be viewable by the VPC to be available for capture with VPC flow logs. Recommend: Disabled unless flow logs from pod traffic is needed. Node Pool Settings For a full list of node configuration level items, the google_container_node_pool terraform provider documentation lists each setting and its purpose. The list below aims to cover the availability and security-related items with additional supporting guidance: Auto Repair - If enabled, should the node validate its own health and destroy/recreate the node if it detects a problem. Generally speaking, this can be safely enabled and avoid getting paged for minor issues that can be self-healed. Recommend: Enabled. Auto Upgrade - If enabled, will attempt to keep the node versions in sync with the control plane version during the maintenance window. Depending on how the applications running inside the cluster respond to losing a worker node or how close to maximum capacity the cluster is running, this may or may not affect workload availability. While all applications should be designed to handle this gracefully, it may require additional testing before it is considered appropriate to enable on your production clusters . Recommend: Enabled with proper testing. Node Config Image Type - Default is \"COS\", or \"Container-Optimized OS\". This is a hardened, minimalist distribution of Linux that is designed to run containers securely. Currently, the stable/default version of \"COS\" leverages a full Docker installation. \"COS_containerd\" is available leverages containerd instead. \"Ubuntu\" is available for specific needs where full control over the Operating System and kernel modules is required, but it defers many operational and security-related tasks to the customer. Recommend: \"COS\" until \"COS_containerd\" becomes the default. Metadata - The mechanism for passing attributes to the GCE Instance Metadata of the underlying GCE instances . Perhaps the most important setting which is now default in GKE 1.12+ , is disable-legacy-endpoints=true . This enables only the v1 metadata API on the instance and disables the \"legacy\" API version v1beta1 . The legacy API does not require a custom HTTP header of Metadata-Flavor: Google to be passed in order to access the metadata API. Applications running on the GCE instance or pods on the GKE worker nodes vulnerable to Server-Side Request Forgery (SSRF) attacks are therefore much less likely to allow exposure of instance credentials via the Metadata API since they need to also control the ability to send the custom HTTP header for the attack to succeed. Recommend: disable-legacy-endpoints=true is set. Oauth Scopes - oauth scopes serve to \"scope\" or \"limit\" the GCP APIs that the credentials of the attached service account can access. It is the intersection or \"overlap\" of the permissions granted by the oauth scopes and the Service Account that defines the actual access. If a service account attached to the node pool is granted Project Owner but is only assigned the https://www.googleapis.com/auth/logging.write oauth scope , then those credentials can only write logs to Stackdriver. Additionally, if the oauth scope was https://www.googleapis.com/auth/cloud-platform (an alias for \"*\" or \"any GCP API\") but the service account was only granted roles/logging.writer , then those credentials can still only be used to write logs to Stackdriver. Avoid granting cloud-platform or compute oauth scopes , especially when paired with Project Editor or Project Owner or Compute Admin IAM roles, or any pod can leverage that access! Recommend: the oauth scopes assigned to new clusters by default in GKE 1.12+ . Sandbox Config (Gvisor) - Disabled by default, this allows the nodes to run pods with the gVisor sandboxing technology. This provides much greater isolation of the container and its ability to interact with the host kernel, but certain pod features are not supported. Disk performance of those pods will be negatively affected, so additional acceptance testing encouraged. Recommend: enabled on dedicated node pools where workloads are running that need additional isolation. Service Account - By default, the \"Default Compute Service Account\" in the project is assigned, and this has Project Editor bound. To give each cluster and/or node pool the ability to use separate and least-privilege permissions, this should be a dedicated service account created for each cluster or node pool , and the minimum permissions assigned to it. Used in conjunction with Oauth Scopes. Recommend: create and specify a dedicated service account and not the default. Workload Metadata - Not set by default (the equivalent of \"UNSPECIFIED\"), this flag enables either the Metadata Concealment Proxy (\"SECURE\") or Workload Identity (\"GKE_METADATA_SERVER\") options. The Workload Identity feature performs all of the same concealment functionality of the Metadata Concealment Proxy buth with the added ability of mapping KSAs to GSAs for dynamic GCP credential access. Recommend: \"GKE_METADATA_SERVER\". Resources Hardening GKE Clusters Private GKE Clusters GCE SSH Access GKE Master Authorized Networks Terraform Cluster Provider Terraform Node Pool Provider GKE Pod Security Policy Open Policy Agent Gatekeeper COS COS_containerd GKE Metadata Concealment GKE Workload Identity GKE OAuth Scopes gVisor Sandboxing VPC/Pod Flow Logs","title":"Cluster Configuration"},{"location":"cluster_configuration/#cluster-configuration","text":"","title":"Cluster Configuration"},{"location":"cluster_configuration/#the-gke-cluster-architecture","text":"Google Kubernetes Engine makes several key architectural decisions for you that may differ from other Kubernetes cluster installations. These choices are made in the interest of ease of use, operational simplicity, and security: There is no direct access to the control plane systems - GKE manages the GCE instances running etcd and the api server and supporting components, and does not expose them to you or an attacker via SSH, for example. In exchange for giving up direct control over all API server configuration flags, your GKE cluster offers very strong protection of the core components and sensitive data in etcd automatically. This means your control over the control plane is restricted to the configuration options on the GKE cluster object. Upgrades are handled via GKE Operations - Upgrades and downgrades are handled via GKE operations through API calls. GKE versions (which map to Kubernetes versions) and worker node OS security and version upgrades are all core features of the built-in upgrade process. The default worker node operating system is COS - Container-optimized OS is a hardened, minimal operating system engineered specifically to run containers and to function as a Kubernetes worker node . While SSH is enabled by default and integrates with GCP SSH functionality, it's goal is to make SSHing into each worker node unnecessary by handling patching and upgrades as a part of the node pool lifecycle. There is an Ubuntu worker node option for supporting unique use cases, but it requires additional administration in terms of upgrades and security patches. The CNI is not configurable - The native GKE CNI is used to connect worker nodes and pods to the GCP Network and is not currently replacable. Network Policy enforcement is done via Calico - Enabling Network Policy enforcement installs a Calico deployment and daemonset as a managed addon for you. Logging and Monitoring is done via Stackdriver - By default, all logs and performance metrics are sent to Stackdriver in the current GCP project .","title":"The GKE Cluster Architecture"},{"location":"cluster_configuration/#resources","text":"GKE Architecture GKE Upgrades Container-Optimized OS GKE CNI GKE Stackdriver","title":"Resources"},{"location":"cluster_configuration/#geographical-placement-and-cluster-types","text":"GKE clusters are by default \"zonal\" clusters. That is, a single GCE instance running the control plane components is deployed in the same GCP zone as the node-pool nodes. \"Regional\" GKE clusters are deployed across three zones in the same region. Three GCE instances running the control plane components are deployed (one per zone ) behind a single IP and load balancer . The node-pools spread evenly across the zones with a minimum of one node per zone . The key benefits to a \"regional\" cluster : The cluster control plane can handle a single GCP zone failure more gracefully. When the control plane is being upgraded, only one instance is down at a time which leaves two remaining instances. Upgrades on \"zonal\" clusters means a 4-10 minute downtime of the API/control plane while that takes place where you can't use kubectl to interact with your cluster . Since there is no additional charge for running a \"regional\" GKE cluster with a highly-available control plane, why use a \"zonal\" cluster ? The two primary reasons are to save on \"cross-zone\" network traffic costs and to support specific GPU node-pool needs. Persistent Disks The default StorageClass defines persistent disks that are \"zonal\". You may want to add a new StorageClass that makes \"regional\" persistent disks available for pod workloads that get rescheduled on another node in a different zone to still be able to access them.","title":"Geographical Placement and Cluster Types"},{"location":"cluster_configuration/#best-practices","text":"Use Regional Clusters - Unless you have specific needs that force you to use a \"zonal\" cluster , using \"regional\" clusters offers the best redundancy and availablility for a minor increase in network traffic costs for the majority of use cases. Offer a Regional Persistent Disk StorageClass - Allows pods to attach and access persistent disk volumes regardless of where they are scheduled inside the cluster. This prevents a zone failure from allowing a pod to be rescheduled and mount that disk on a node in another zone .","title":"Best Practices"},{"location":"cluster_configuration/#resources_1","text":"GKE Regional Clusters","title":"Resources"},{"location":"cluster_configuration/#google-cloud-iam-and-kubernetes-rbac","text":"Google Cloud Identity and Access Management (IAM) - The system in GCP that grants permissions via GCP IAM Roles to users and service accounts to access GCP APIs. Kubernetes Role-Based Access Control (RBAC) - The native system inside Kubernetes that grants permissions via Kubernetes roles and clusterroles to users and service accounts to access the Kubernetes API Server. The two APIs that can be used to interact with the GKE service and a GKE cluster are: GCP GKE API (container.googleapis.com) - Used to create/update/delete the cluster and node pools that comprise the GKE cluster and to obtain connection and credential information for how to access a given cluster . Kubernetes API of the cluster - The unique Kubernetes API Server endpoint running on the GKE Control Plane systems for a specific cluster . It controls resources and access to resources running inside the cluster. This means that there are two main categories of permissions that you have to consider: Cluster Administration - Permissions associated with administering the cluster itself. Cluster Usage - Permissions associated with granting what is allowed to run inside the cluster. There are a couple key points to understand about how Cloud IAM and Kubernetes RBAC can be used to grant permissions in GKE: GCP Cloud IAM is administered at the project level, and the granted permissions apply to all GKE clusters in the project . They remain in-place even if a cluster is deleted from the project . Kubernetes RBAC permissions are administered per- cluster and are stored inside each cluster . When that cluster is destroyed, those permissions are also destroyed. GCP Cloud IAM permissions have no concept of Kubernetes namespaces , so the permissions granted apply for all namespaces . Kubernetes RBAC can be used to grant permissions to access resources in all namespaces or only in specific namespaces . Kubernetes RBAC cannot be used to grant permissions to the GCP GKE API (container.googleapis.com). This is performed solely by Cloud IAM. Both systems are additive in that they only grant permissions. They cannot remove or negate permissions from themselves or each other. IAM and RBAC combine inside the cluster When accessing a resource (e.g. list pods in the default namespace) via the Kubernetes API of a GKE cluster , it can be granted via IAM or RBAC. They are effectively combined. If either grants access, the request is permitted.","title":"Google Cloud IAM and Kubernetes RBAC"},{"location":"cluster_configuration/#best-practices_1","text":"There are several predefined IAM roles for GKE clusters that can ease administration as you are getting started with GKE. However, the permissions they grant are often overly broad and violate the priciple of least privileges. You will want to create custom IAM roles with just the permissions needed, but remember that IAM permissions apply to all namespaces and cannot be limited to a single namespace . To maintain least privilege, it's recommended to leverage a minimal IAM role to gain access to the GKE API and then use Kubernetes RBAC roles and clusterroles to define what resources they can access inside the cluster . In order for users and service accounts to perform a gcloud container clusters get-credentials call to generate a valid kubeconfig for a GKE cluster , they need to have the following permissions: container.apiServices.get container.apiServices.list container.clusters.get container.clusters.list container.clusters.getCredentials If these permissions are grouped into a custom IAM role , that IAM role can be conveniently bound to a Gsuite/Cloud Identity group which includes all users that need access to the cluster . From this point, the users and service accounts can be granted access to resources as needed with cluster -wide or per- namespace granularity. This has the benefit of minimizing IAM changes needed, ensuring access granted is per- cluster , giving the highest granularity, and making troubleshooting permissions an RBAC-only process. With the above approach of deferring nearly all permissions to in- cluster RBAC instead of IAM, there is one exception: assigning the Kubernetes Engine Admin predefined IAM role at the project or folder level to a small number of trusted administrators to ensure that they don't accidentally remove their own access via an RBAC configuration mistake.","title":"Best Practices"},{"location":"cluster_configuration/#resources_2","text":"GKE IAM and RBAC Predefined GKE Roles GKE RBAC","title":"Resources"},{"location":"cluster_configuration/#cluster-access","text":"","title":"Cluster Access"},{"location":"cluster_configuration/#control-plane-access","text":"The GCE instances running the control plane components like the kube-apiserver and etcd are not directly accessible via SSH and the GCE API. They don't appear in gcloud command outputs nor in the GCP console. The GKE control plane is only accessible via the exposed Kubernetes API on tcp/443 . By default, the API is assigned a public IP address and has firewall rules that allow access from 0.0.0.0/0 . While this is conducive to ease of use and access from anywhere, it might not meet your security requirements. Warning Sufficient access to the Kubernetes API equates to root on all the worker nodes . Protecting the Kubernetes API from access misconfiguration, denial-of-service, and exploitation should be high on a security team's priority list. There are controls available to improve on the default configuration, and they are covered in the next section.","title":"Control Plane Access"},{"location":"cluster_configuration/#worker-node-access","text":"Because GKE nodes are simply GCE instances inside instance groups , they are accessible via SSH in accordance with the normal GCE routing and VPC firewall rules. The standard GCE methods for using gcloud compute ssh to gain access to the underlying operating system work as expected. There are two things to consider when granting SSH access to nodes : Does SSH access need to be available publicly? In many cases, additional firewall source restrictions are useful in limiting the allowed subnets from which SSH access can be initiated. For instance, from a set of external office IP addresses or from a specific VPC subnet designated for management purposes. Several primitive IAM roles like Owner , Editor , Dataproc Service Agent , Compute OS Admin Login , Compute Admin , and Compute Instance Admin include the compute.instances.osAdminLogin permission. Users and service accounts with those permissions and network level access can SSH into the worker nodes and gain root permissions at the operating system level. Warning Gaining root access to a GKE worker node allows that user to view all secrets attached to pods running on that node . Those secrets may include tokens that belong to service accounts which have elevated permissions to the Kubernetes API. With that token , that user can access the Kubernetes API as that service account . If that service account has permissions to modify Role-Based Access Control or to view all secrets , that commonly equates to full cluster access (aka \"cluster admin\"). As \"cluster admin\" has full control of the Kubernetes API, that equates to root on all worker nodes .","title":"Worker Node Access"},{"location":"cluster_configuration/#cluster-settings","text":"For a full list of cluster configuration level items, the google_container_cluster terraform provider documentation is a fantastic resource for a list of each setting and its purpose. The list below aims to cover the availability and security-related items with additional supporting guidance: Location - The region (\"us-central1\") or zone (\"us-central1-a\") where the cluster will be located. Clusters cannot span multiple regions . If a region is specified, the control plane will have an instance in each of the three zones . If a zone is specified, the control plane will have a single instance in that zone only. Node Locations - The list of one to three zones where the nodes will be located, and it must be in the same region . Recommend: specifying a region to obtain a fault-tolerant control plane. Mix-and-Match If node locations is specified, it overrides the default behavior. It's possible to have a regional cluster but then use node locations to restrict the zones to only one or two instead of the default of three. It's also possible to create a zonal cluster with a single control plane instance but then use node locations to select one to three zones where the nodes will be located. Typically, this feature is used to help guide clusters into configurations to align with large CPU or GPU quota needs. Addons Config HTTP Load Balancing - Enabled by default, this installs the ingress-gce HTTP/S Load Balancing and Ingress Controller. It can be disabled to allow for installation of a different ingress controller solution or left implemented but unused. Recommend: enabled. Kubernetes Dashboard - Now disabled by default, this installs a Kubernetes dashboard deployment inside the cluster . Historically, the dashboard has been misconfigured and has had a small number of security issues that have led to compromises. With the GCP console providing all the dashboard features needed, this can and should be disabled. Clusters that were originally created in the Kubernetes 1.6 era and upgraded to the present may still have this enabled. Recommend: Disabled. Network Policy - Disabled by default, this installs the Calico CNI components which enables the configuration of networkpolicy resources in the Kubernetes API that can restrict pod'-to- pod communication. It is \"safe\" to enable by default as the default effect of the networkpolicy configuration in Kubernetes is \"allow all traffic\" until a networkpolicy` is defined. Recommend: Enabled. Istio - Disabled by default, this installs the Google-managed deployment of Istio inside the cluster . Istio can provide interesting security-related features like \"mutual TLS\" encryption for traffic between pods in the mesh , deep traffic inspection in the form of network traces, and granular role-based permissions for egress network traffic and Layer 7 access controls. In terms of complexity, Istio adds over 50 custom resource definitions that a security team will need to analyze and understand to have awareness of the security posture of the cluster . Recommend: \"Disabled\" until the features are needed. Database Encryption - Disabled by default, this enables an integration with Cloud KMS to have the API server access a Cloud KMS key to encrypt and decrypt the contents of secrets as they are stored and accessed from etcd . In environments where the control plane nodes are potentially accessible, this makes the compromise of a dedicated etcd system or etcd backup more difficult to extract the secrets in plain-text. In GKE, the control plane systems are not accessible directly by users or attackers, so this adds marginal benefit aside from satisfying regulatory requirements of \"databases must encrypt sensitive records at the row-level\". Recommend: \"Disabled\" unless compliance requirements dictate. Default Max Pods Per Node - Controls how many pods can run on a single node. The default is the hard-coded maximum of 110, but this can be reduced to 64, 32, 16, or 8 if the workload profile is known. This causes the subnet allocated to each GKE node to go from a /24 down to a /25 , /26 , and so on, and it can greatly reduce IP address consumption of precious RFC1918 space. Recommend: the default of 110 unless specific IP space or capacity needs require smaller. Binary Authorization - Disabled by default. This addon enables an admission controller that validates pod specs against a policy that defines which container image repositories are allowed and/or validates that container images have a valid PGP signature before allowing them to run inside the cluster. Can be enabled safely as the default policy is to \"allow all\" until configured. Recommend: Enabled. Kubernetes Alpha - Disabled by default. Do not enable on production clusters as alpha clusters are deleted automatically after 30 days by GKE. Recommend: Disabled. Legacy ABAC - Disabled by default, this is a legacy permissions mechanism that is largely unused in the Kubernetes community. The default role-based access control (RBAC) mechanism is enabled by default and is preferred. Recommend: Disabled. Logging Service - Enabled by default, this installs and manages a daemonset that collects and sends all pod logs to Stackdriver for processing, troubleshooting, and analysis/auditing purposes. Recommend: logging.googleapis.com/kubernetes Maintenance Policy - Defines a preferred 4-hour time window (in UTC) for when GKE should perform automatic or security-related upgrades. Recommend: setting a 4-hour UTC window with the least workload impact and overlap with operational on-call rotations. Master Authorized Networks - By default, the implicit allowed CIDR range is 0.0.0.0/0 for which source IPs can connect to the Kubernetes API. You may specify up to 50 different CIDR ranges. This setting can be changed as needed without affecting the lifecycle of the running cluster , and it is the most common method for reducing the scope of who can reach the Kubernetes cluster API to a smaller set of IPs. In the event of a vulnerability in the API server, enabling a small list of CIDRs to a known set can reduce the risk of an organization delaying maintenance operations to a time that is less busy or risky. Recommend: Configuring a short list of allowed CIDRs. Monitoring Service - Enabled by default, this installs and manages a daemonset that collects and sends all metrics to Stackdriver for troubleshooting and analysis. Recommend: monitoring.googleapis.com/kubernetes Pod Security Policy - Disabled by default, this enables the admission controller used to validate the specifications of pods to prevent insecure or \"privileged\" pods from being created that allow trivial escaping to the underlying host as root and bypassing other security mechanisms like networkpolicy . Recommend: Enabled with extensive testing OR implementing the same features with an Open Policy Agent/Gatekeeper validating admission controller. Authenticator Groups - Disabled by default, this enables RBAC to use Google Groups in RoleBinding and ClusterRoleBinding resources instead of having to specify each user or serviceaccount individually. For example, a Gsuite administrator creates a group called gke-security-groups@mydomain.com and places groups named gke-admins@ and gke-developers@ inside it. Passing gke-security-groups@mydomain.com to this setting allows RBAC to reference/lookup the gke-admins@mydomain.com and/or gke-developers@mydomain.com when evaluating access in the API to resources. Recommend: Enabled if using Gsuite/Google Groups to support easier to manage RBAC policies. Private Cluster - Master IPv4 CIDR Range - The RFC1918 subnet that the control plane should pick an IP address from when assigning a private IP. Recommend: a /28 that does not overlap with any other IP space in use. Enable Private Endpoint - \"false\" by default, setting this to \"true\" instructs the GKE control plane IP to be selected from the master CIDR range and does not expose a public IP address. Recommend: true Enable Private Nodes - \"false\" by default, setting this to \"true\" instructs the GKE nodes to not be assigned a public IP address. Recommend: true Private Clusters There is a common misconception that a \"private cluster\" is one that has worker nodes with private IP addresses. This is only half-correct. A true \"private cluster\" is one that has both the control plane and nodes using private IP addresses, and both of the above settings are necessary to achieve that improved security posture. When these two settings are enabled and combined with a small list of master authorized networks, the attack surface of the Kubernetes API can be significantly reduced. Remove Default Node Pool - By default, GKE deploys a node pool to be able to run pods . However, its lifecycle is tied to that of the cluster object. Meaning, certain changes may cause the entire cluster to be recreated. It's recommended that you disable/remove the default node pool and explicitly declare node pools with the desired settings so that they keep their lifecycles independent from the control plane/cluster. Workload Identity - Disabled by default, this enables the Workload Identity addon and by specifying which Identity \"domain\" to use. Without Workload Identity, pods wanting to reach GCP APIs would be granted the credentials attached to the underlying GCE instance . The underlying GCE instance has permissions to send logs, send metrics, read from Cloud Storage/Container Registries, and more. Historically, the nodes were assigned the default compute service account which was assigned Project Editor permissions. This meant that every pod could potentially have Project Editor access! With Workload Identity, a daemonset on every worker node intercepts all calls for instance credentials to the metadata API. If the Kubernetes service account attached to that pod has been granted a specific binding to a Google service account , then the dynamic crednetials for that specific Google service account will be returned to the pod . In essence, this is a way to map Kubernetes service accounts to Google service accounts and can remove the need for exporting Google service account keys in JSON format and storing them manually inside Kubernetes secrets . Recommend: Enabled. IntraNode Visibility - Disabled by default, this enables the network traffic on the pod network to be viewable by the VPC to be available for capture with VPC flow logs. Recommend: Disabled unless flow logs from pod traffic is needed.","title":"Cluster Settings"},{"location":"cluster_configuration/#node-pool-settings","text":"For a full list of node configuration level items, the google_container_node_pool terraform provider documentation lists each setting and its purpose. The list below aims to cover the availability and security-related items with additional supporting guidance: Auto Repair - If enabled, should the node validate its own health and destroy/recreate the node if it detects a problem. Generally speaking, this can be safely enabled and avoid getting paged for minor issues that can be self-healed. Recommend: Enabled. Auto Upgrade - If enabled, will attempt to keep the node versions in sync with the control plane version during the maintenance window. Depending on how the applications running inside the cluster respond to losing a worker node or how close to maximum capacity the cluster is running, this may or may not affect workload availability. While all applications should be designed to handle this gracefully, it may require additional testing before it is considered appropriate to enable on your production clusters . Recommend: Enabled with proper testing. Node Config Image Type - Default is \"COS\", or \"Container-Optimized OS\". This is a hardened, minimalist distribution of Linux that is designed to run containers securely. Currently, the stable/default version of \"COS\" leverages a full Docker installation. \"COS_containerd\" is available leverages containerd instead. \"Ubuntu\" is available for specific needs where full control over the Operating System and kernel modules is required, but it defers many operational and security-related tasks to the customer. Recommend: \"COS\" until \"COS_containerd\" becomes the default. Metadata - The mechanism for passing attributes to the GCE Instance Metadata of the underlying GCE instances . Perhaps the most important setting which is now default in GKE 1.12+ , is disable-legacy-endpoints=true . This enables only the v1 metadata API on the instance and disables the \"legacy\" API version v1beta1 . The legacy API does not require a custom HTTP header of Metadata-Flavor: Google to be passed in order to access the metadata API. Applications running on the GCE instance or pods on the GKE worker nodes vulnerable to Server-Side Request Forgery (SSRF) attacks are therefore much less likely to allow exposure of instance credentials via the Metadata API since they need to also control the ability to send the custom HTTP header for the attack to succeed. Recommend: disable-legacy-endpoints=true is set. Oauth Scopes - oauth scopes serve to \"scope\" or \"limit\" the GCP APIs that the credentials of the attached service account can access. It is the intersection or \"overlap\" of the permissions granted by the oauth scopes and the Service Account that defines the actual access. If a service account attached to the node pool is granted Project Owner but is only assigned the https://www.googleapis.com/auth/logging.write oauth scope , then those credentials can only write logs to Stackdriver. Additionally, if the oauth scope was https://www.googleapis.com/auth/cloud-platform (an alias for \"*\" or \"any GCP API\") but the service account was only granted roles/logging.writer , then those credentials can still only be used to write logs to Stackdriver. Avoid granting cloud-platform or compute oauth scopes , especially when paired with Project Editor or Project Owner or Compute Admin IAM roles, or any pod can leverage that access! Recommend: the oauth scopes assigned to new clusters by default in GKE 1.12+ . Sandbox Config (Gvisor) - Disabled by default, this allows the nodes to run pods with the gVisor sandboxing technology. This provides much greater isolation of the container and its ability to interact with the host kernel, but certain pod features are not supported. Disk performance of those pods will be negatively affected, so additional acceptance testing encouraged. Recommend: enabled on dedicated node pools where workloads are running that need additional isolation. Service Account - By default, the \"Default Compute Service Account\" in the project is assigned, and this has Project Editor bound. To give each cluster and/or node pool the ability to use separate and least-privilege permissions, this should be a dedicated service account created for each cluster or node pool , and the minimum permissions assigned to it. Used in conjunction with Oauth Scopes. Recommend: create and specify a dedicated service account and not the default. Workload Metadata - Not set by default (the equivalent of \"UNSPECIFIED\"), this flag enables either the Metadata Concealment Proxy (\"SECURE\") or Workload Identity (\"GKE_METADATA_SERVER\") options. The Workload Identity feature performs all of the same concealment functionality of the Metadata Concealment Proxy buth with the added ability of mapping KSAs to GSAs for dynamic GCP credential access. Recommend: \"GKE_METADATA_SERVER\".","title":"Node Pool Settings"},{"location":"cluster_configuration/#resources_3","text":"Hardening GKE Clusters Private GKE Clusters GCE SSH Access GKE Master Authorized Networks Terraform Cluster Provider Terraform Node Pool Provider GKE Pod Security Policy Open Policy Agent Gatekeeper COS COS_containerd GKE Metadata Concealment GKE Workload Identity GKE OAuth Scopes gVisor Sandboxing VPC/Pod Flow Logs","title":"Resources"},{"location":"cluster_lifecycle/","text":"Cluster Lifecycle Infrastructure as Code Creating GKE clusters using gcloud or the UI console is sufficient for testing purposes, but production-ready deployments should be managed with purpose-built tooling that declaratively set the defined state of infrastructure in code form. Terraform handles this well and it provides an easy to read example code base for how the security-related features are configured: provider google-beta { version = 2.13.0 project = my-project-id region = us-central1 } resource google_container_cluster my-cluster-name { provider = google-beta name = my-cluster-name project = my-project-id // Configures a highly-available control plane spread across three // zones in this region location = us-central1 // It s best to create these networks and subnets in terraform // and then reference them here. network = google_compute_network.network.self_link subnetwork = google_compute_subnetwork.subnetwork.self_link // Set this to the version desired. It will become the starting // point version for the cluster. Upgrades of the control plane // can be initiated by simply bumping this version and running // terraform apply. min_master_version = 1.13.7-gke19 // Specify the newer Kubernetes logging and monitoring features // of the Stackdriver integration. // Previously logging.googleapis.com logging_service = logging.googleapis.com/kubernetes monitoring_service = monitoring.googleapis.com/kubernetes // Do not use the default node pool that GKE provides and instead // use the node pool(s) define below explicitly. GKE will actually // provision a 1 node node pool and then remove it before making the // node pools below. remove_default_node_pool = true initial_node_count = 1 // This is false by default. RBAC is enabled by default and preferred. enable_legacy_abac = false // Enable the Binary Authorization admission controller to allow // this cluster to evaluate a BinAuthZ policy when pods are created // to ensure images come from the proper sources. enable_binary_authorization = true // Defines the kubelet setting for how many pods could potentially // run on this node. Depending on instance type, setting this to // 64 would be more appropriate and use half the IP space. default_max_pods_per_node = 110 // Enable transparent application level encryption of etcd secrets // using a KMS keyring/key. Can be a software or HSM-backed KMS key // for extra compliance requirements. database_encryption { key_name = my-existing-kms-key state = ENCRYPTED } addons_config { // Do not deploy the in-cluster K8s dashboard and defer to kubectl // and the GCP UI console. kubernetes_dashboard { disabled = true } // Enable network policy (Calico) as an addon. network_policy_config { disabled = false } // Provide the ability to scale pod replicas based on real-time metrics horizontal_pod_autoscaling { disabled = false } } // Enable intranode visibility to expose pod-to-pod traffic to the VPC // for flow logging potential. Requires enabling VPC Flow Logging // on the subnet first enable_intranode_visibility = true // Enables the PSP admission controller in the cluster. DO NOT enable this // on an existing cluster without first configuring the necessary pod security // policies and RBAC role bindings or you will inhibit pods from running // until those are correctly configured. Recommend success in a test env first. pod_security_policy_config { enabled = true } // Enable the VPA addon in the cluster to track actual usage vs requests/limits. // Safe to enable at any time. vertical_pod_autoscaling { enabled = true } // Configure the workload identity identity namespace . Requires additional // configuration on the node pool for workload identity to function. workload_identity_config { identity_namespace = my-project-id.svc.goog.id } // Disable basic authentication and cert-based authentication. // Empty fields for username and password are how to disable the // credentials from being generated. master_auth { username = password = client_certificate_config { issue_client_certificate = false } } // Enable network policy configurations via Calico. Must be configured with // the block in the addons section. network_policy { enabled = true } // The Google Security group that contains the allowed list of other Google // Security groups that can be referenced via in-cluster RBAC bindings instead // of having to specify Users one by one. // e.g. // gke-security-groups // - groupA // - groupB // And now, an RBAC RoleBinding/ClusterRoleBinding can reference Group: groupA authenticator_groups_config { security_group = gke-security-groups@mydomain.com } // Give GKE a 4 hour window each day in which to perform maintenance operations // and required security patches. In UTC. maintenance_policy { daily_maintenance_window { start_time = TODO } } // Use VPC Aliasing to improve performance and reduce network hops between nodes and load balancers. References the secondary ranges specified in the VPC subnet. ip_allocation_policy { use_ip_aliases = true cluster_secondary_range_name = google_compute_subnetwork.subnetwork.secondary_ip_range.0.range_name services_secondary_range_name = google_compute_subnetwork.subnetwork.secondary_ip_range.1.range_name } // Specify the list of CIDRs which can access the master s API. This can be // a list of up to 50 CIDRs. It s basically a control plane firewall rulebase. // Nodes automatically/always have access, so these are for users and automation // systems. master_authorized_networks_config { cidr_blocks { cidr_block = 10.0.0.0/8 display_name = VPC Subnet } cidr_blocks { cidr_block = 192.168.0.0/24 display_name = Admin Subnet } } // Configure the cluster to have private nodes and private control plane access only private_cluster_config { // Enables the control plane to not be exposed via public IP and which subnet to // use an IP from. enable_private_endpoint = true master_ipv4_cidr_block = 172.20.20.0/28 // Specifies that all nodes in all node pools for this cluster should not have a // public IP automatically assigned. enable_private_nodes = true } } resource google_container_node_pool my-node-pool { provider = google-beta name = my-node-pool // Spread nodes in this node pool evenly across the three zones in this region. location = us-central1 cluster = google_container_cluster.my-cluster-name.name // Must be at or below the control plane version. Bump this field to trigger a // rolling node pool upgrade. version = 1.13.7-gke19 // Because this is a regional cluster and a regional node pool, this is the // number of nodes per-zone to create. 1 will create 3 total nodes. node_count = 1 // Overrides the cluster setting on a per-node-pool basis. max_pods_per_node = 110 // The min and max number of nodes (per-zone) to scale to. This defines a three // to 30 node cluster. autoscaling { min_node_count = 1 max_node_count = 10 } // Fix broken nodes automatically and keep them updated with the control plane. management { auto_repair = true auto_upgrade = true } node_config { machine_type = n1-standard-1 // pd-standard is often too slow, so using pd-ssd s is recommended for pods // that do any scratch disk operations. disk_type = pd-ssd // COS or COS_containerd are ideal here. Ubuntu if specific kernel features or // disk drivers are necessary. image_type = COS // Use a custom service account for this node pool. Be sure to grant it // a minimal amount of IAM roles and not Project Editor like the default SA. service_account = dedicated-sa-name@my-project-id.iam.gserviceaccount.com // Use the default/minimal oauth scopes to help restrict the permissions to // only those needed for GCR and stackdriver logging/monitoring/tracing needs. oauth_scopes = [ https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring , https://www.googleapis.com/auth/servicecontrol , https://www.googleapis.com/auth/service.management.readonly , https://www.googleapis.com/auth/trace.append ] // Enable GKE Sandbox (Gvisor) on this node pool. This cannot be set on the // only node pool in the cluster as system workloads are not all compatible // with the restrictions and protections offered by gvisor. sandbox_config { sandbox_type = gvisor } // Protect node metadata and enable Workload Identity // for this node pool. SECURE just protects the metadata. // EXPOSE or not set allows for cluster takeover. // GKE_METADATA_SERVER specifies that each pod s requests to the metadata // API for credentials should be intercepted and given the specific // credentials for that pod only and not the node s. workload_metadata_config { node_metadata = GKE_METADATA_SERVER } metadata = { // Set metadata on the VM to supply more entropy google-compute-enable-virtio-rng = true // Explicitly remove GCE legacy metadata API endpoint to prevent most SSRF // bugs in apps running on pods inside the cluster from giving attackers // a path to pull the GKE metadata/bootstrapping credentials. disable-legacy-endpoints = true } } } Resources Terraform Terraform Cluster Provider Terraform Node Pool Provider Upgrades and Security Fixes Control Plane Upgrades One of the most important problems that GKE helps solve is maintaining the control plane components for you. Using gcloud or the UI console, upgrading the control plane version is a single API call. There are a few important concepts to understand with this feature: The version of the control plane can and should be kept updated. - GKE supports the current \"Generally Available\" (GA) GKE version back to two older minor revisions. For instance, if the GA version is 1.13.7-gke19 , then 1.11.x and 1.12.x are supported. GKE versions tend to trail Kubernetes OSS slightly. - Kubernetes OSS releases a minor revision approximately every 90-120 days, and GKE is quick to offer \"Alpha\" releases with those newest versions, but it may take some time to graduate to \"GA\" in GKE. Control Plane upgrades of Zonal clusters incurs a few minutes of downtime. - Upgrading to a newer GKE version causes each control plane instance to be recreated in-place. On zonal clusters, there is a few minutes where that single instance is unavailable while it is being recreated. The data in etcd is kept as-is and all workloads that are running on the node pools remain in place. However, access to the API with kubectl will be temporarily halted. Regional clusters perform a \"rolling upgrade\" approach to upgrading the three control plane instances, and so the upgrade happens one instance at a time. This leaves the control plane available on the other instances and should incur no noticable downtime. For the sake of a highly-available control plane and the bonus that is no additional cost, it's recommended to run regional clusters. If inter-regional traffic costs are a concern, use the node locations setting to use a single zone but still keep the regional control plane. Node Pool Upgrades Upgrades of node pools are handled per- node pool , and this gives the operator a lot of control over the behavior. If auto-upgrades are enabled, the node pools will be scheduled for \"rolling upgrade\" style upgrades during the configured maintenance window time block. While it's possible to have node pools trail the control plane version by two minor revisions, it's best to not trail more than one. As node pools are where workloads are running, a few points are important to understand: Node Pools are resource boundaries - If certain workloads are resource-intensive or prone to over-consuming resources in large bursts, it may make sense to dedicate a node pool to them to reduce the potential for negatively affecting the performance and availability of other workloads. Each Node Pool adds operational maintenance overhead - Quite simply, each node pool adds more work to the operations team. Either to ensure is automatically upgraded or to perform the upgrades manually if auto-upgrades are disabled. Separating workloads to different Node Pools can help support security goals - While not a perfect solution, ensuring that workloads handling certain sensitive data are scheduled on separate node pools (using node taints and tolerations on workloads) can help reduce the chance of compromising secrets in a container \"escape-to-the-host\" situation. For example, placing PCI-related workloads on a separate node pool from other non-PCI workloads means that a compromise of a non-PCI pod that allows access to the underlying host will be less likely to have access to secrets that the PCI workloads are using. Security Bulletins Another benefit of a managed service like GKE is the reduced operational security burden on having to triage, test, and manage upgrades due to security issues. The GKE Security Bulletins website and Atom/RSS feed provide a consolidated view of all things security-related in GKE. It is strongly recommended that teams subscribe to and receive automatic updates from the feed as the information is timely and typically very clear on which actions GKE the service is taking and which actions are left to you. Resources GKE Cluster Upgrades GKE Node Pool Upgrades Automatic Node Upgrades GKE Security Bulletins GKE Release Notes Scaling While scaling isn't a purely security-related topic, there are a few security and resource availability concerns with each of the common scaling patterns: Node Sizing - The resources available to GKE worker nodes are shared by all pods that are scheduled on them. While there are solid limits on CPU and RAM usage, there are not strong resource isolation mechanisms (yet) for disk usage that lands on the node . It is possible for a single pod to consume enough disk space and/or inodes and disrupt the health of the node . Depending on the workload profile you plan on running in the cluster , it might make sense to have more nodes of a smaller instance type with the faster pd-ssd disk type than fewer large instances sharing the slower pd-hdd disk type , for example. When workload resource profiles drastically differ, it might make sense to use separate node pools or even separate clusters . Horizontal Pod Autoscaler - The built-in capability of scaling replicas in a deployment based on CPU usage may or may not be granular enough for your needs. Ensure that the total capacity of the cluster (capacity of the max number of nodes allowed by autoscaling) is greater than the resources used by the max number of replicas in the deployment . Also, consider using \"external\" metrics from Stackdriver like \"number of tasks in a pub/sub queue\" or \"web requests per pod\" to be a more accurate and efficient basis for scaling your workloads. Vertical Pod Autoscaler - Horizontal Pod Autoscaling adds more replicas as needed, but that is assuming the pod has properly set resource requests and limits for CPU and RAM set. Each pod should be configured with the proper requests for how much CPU and RAM it uses plus 5-10% at idle for its requests setting. The limit should be the maximum CPU and RAM that the pod will ever use. Having accurate requests and limits set for every pod gives the scheduler the correct information for how to place workloads, increases efficiency of resource usage, and reduces the risk of \"noisy neighbor\" issues from pods that consume more than their set share of resources. VPA runs inside the cluster and can monitor workloads for configured requests and limits in comparison with actual usage, and can either \"audit\" (just make recommendations) or actually modify the deployments dynamically. It's recommended to run VPA in a cluster in \"audit\" mode to help find misconfigured pods before they cause cluster -wide outages. Node Autoprovisioner - The GKE cluster-autoscaler adds or removes nodes from existing node pools based on their available capacity. If a deployment needs more replicas , there are not enough running nodes to handle them, the node pool is configured to autoscale, and there are more nodes left before the maximum is reached, the autoscaler will add nodes until the workloads are scheduled. But what if a pod is asking for resources that a new node can't handle? For example, a pod that asks for 16 cpu cores but the nodes are only 8 core instances? Or if a pod needs a GPU but the node pool doesn't attach GPUs? The node autoprovisioner can be configured to dynamically manage node pools for you to help satisfy these situations. Instead of waiting for an administrator to add a new node pool , the cluster autoscaler can do that for you. Resources GKE Cluster Sizing GKE Cluster Resizing Horizontal Pod Autoscaler Vertical Pod Autoscaler GKE Node Auto-Provisioner","title":"Cluster Lifecycle"},{"location":"cluster_lifecycle/#cluster-lifecycle","text":"","title":"Cluster Lifecycle"},{"location":"cluster_lifecycle/#infrastructure-as-code","text":"Creating GKE clusters using gcloud or the UI console is sufficient for testing purposes, but production-ready deployments should be managed with purpose-built tooling that declaratively set the defined state of infrastructure in code form. Terraform handles this well and it provides an easy to read example code base for how the security-related features are configured: provider google-beta { version = 2.13.0 project = my-project-id region = us-central1 } resource google_container_cluster my-cluster-name { provider = google-beta name = my-cluster-name project = my-project-id // Configures a highly-available control plane spread across three // zones in this region location = us-central1 // It s best to create these networks and subnets in terraform // and then reference them here. network = google_compute_network.network.self_link subnetwork = google_compute_subnetwork.subnetwork.self_link // Set this to the version desired. It will become the starting // point version for the cluster. Upgrades of the control plane // can be initiated by simply bumping this version and running // terraform apply. min_master_version = 1.13.7-gke19 // Specify the newer Kubernetes logging and monitoring features // of the Stackdriver integration. // Previously logging.googleapis.com logging_service = logging.googleapis.com/kubernetes monitoring_service = monitoring.googleapis.com/kubernetes // Do not use the default node pool that GKE provides and instead // use the node pool(s) define below explicitly. GKE will actually // provision a 1 node node pool and then remove it before making the // node pools below. remove_default_node_pool = true initial_node_count = 1 // This is false by default. RBAC is enabled by default and preferred. enable_legacy_abac = false // Enable the Binary Authorization admission controller to allow // this cluster to evaluate a BinAuthZ policy when pods are created // to ensure images come from the proper sources. enable_binary_authorization = true // Defines the kubelet setting for how many pods could potentially // run on this node. Depending on instance type, setting this to // 64 would be more appropriate and use half the IP space. default_max_pods_per_node = 110 // Enable transparent application level encryption of etcd secrets // using a KMS keyring/key. Can be a software or HSM-backed KMS key // for extra compliance requirements. database_encryption { key_name = my-existing-kms-key state = ENCRYPTED } addons_config { // Do not deploy the in-cluster K8s dashboard and defer to kubectl // and the GCP UI console. kubernetes_dashboard { disabled = true } // Enable network policy (Calico) as an addon. network_policy_config { disabled = false } // Provide the ability to scale pod replicas based on real-time metrics horizontal_pod_autoscaling { disabled = false } } // Enable intranode visibility to expose pod-to-pod traffic to the VPC // for flow logging potential. Requires enabling VPC Flow Logging // on the subnet first enable_intranode_visibility = true // Enables the PSP admission controller in the cluster. DO NOT enable this // on an existing cluster without first configuring the necessary pod security // policies and RBAC role bindings or you will inhibit pods from running // until those are correctly configured. Recommend success in a test env first. pod_security_policy_config { enabled = true } // Enable the VPA addon in the cluster to track actual usage vs requests/limits. // Safe to enable at any time. vertical_pod_autoscaling { enabled = true } // Configure the workload identity identity namespace . Requires additional // configuration on the node pool for workload identity to function. workload_identity_config { identity_namespace = my-project-id.svc.goog.id } // Disable basic authentication and cert-based authentication. // Empty fields for username and password are how to disable the // credentials from being generated. master_auth { username = password = client_certificate_config { issue_client_certificate = false } } // Enable network policy configurations via Calico. Must be configured with // the block in the addons section. network_policy { enabled = true } // The Google Security group that contains the allowed list of other Google // Security groups that can be referenced via in-cluster RBAC bindings instead // of having to specify Users one by one. // e.g. // gke-security-groups // - groupA // - groupB // And now, an RBAC RoleBinding/ClusterRoleBinding can reference Group: groupA authenticator_groups_config { security_group = gke-security-groups@mydomain.com } // Give GKE a 4 hour window each day in which to perform maintenance operations // and required security patches. In UTC. maintenance_policy { daily_maintenance_window { start_time = TODO } } // Use VPC Aliasing to improve performance and reduce network hops between nodes and load balancers. References the secondary ranges specified in the VPC subnet. ip_allocation_policy { use_ip_aliases = true cluster_secondary_range_name = google_compute_subnetwork.subnetwork.secondary_ip_range.0.range_name services_secondary_range_name = google_compute_subnetwork.subnetwork.secondary_ip_range.1.range_name } // Specify the list of CIDRs which can access the master s API. This can be // a list of up to 50 CIDRs. It s basically a control plane firewall rulebase. // Nodes automatically/always have access, so these are for users and automation // systems. master_authorized_networks_config { cidr_blocks { cidr_block = 10.0.0.0/8 display_name = VPC Subnet } cidr_blocks { cidr_block = 192.168.0.0/24 display_name = Admin Subnet } } // Configure the cluster to have private nodes and private control plane access only private_cluster_config { // Enables the control plane to not be exposed via public IP and which subnet to // use an IP from. enable_private_endpoint = true master_ipv4_cidr_block = 172.20.20.0/28 // Specifies that all nodes in all node pools for this cluster should not have a // public IP automatically assigned. enable_private_nodes = true } } resource google_container_node_pool my-node-pool { provider = google-beta name = my-node-pool // Spread nodes in this node pool evenly across the three zones in this region. location = us-central1 cluster = google_container_cluster.my-cluster-name.name // Must be at or below the control plane version. Bump this field to trigger a // rolling node pool upgrade. version = 1.13.7-gke19 // Because this is a regional cluster and a regional node pool, this is the // number of nodes per-zone to create. 1 will create 3 total nodes. node_count = 1 // Overrides the cluster setting on a per-node-pool basis. max_pods_per_node = 110 // The min and max number of nodes (per-zone) to scale to. This defines a three // to 30 node cluster. autoscaling { min_node_count = 1 max_node_count = 10 } // Fix broken nodes automatically and keep them updated with the control plane. management { auto_repair = true auto_upgrade = true } node_config { machine_type = n1-standard-1 // pd-standard is often too slow, so using pd-ssd s is recommended for pods // that do any scratch disk operations. disk_type = pd-ssd // COS or COS_containerd are ideal here. Ubuntu if specific kernel features or // disk drivers are necessary. image_type = COS // Use a custom service account for this node pool. Be sure to grant it // a minimal amount of IAM roles and not Project Editor like the default SA. service_account = dedicated-sa-name@my-project-id.iam.gserviceaccount.com // Use the default/minimal oauth scopes to help restrict the permissions to // only those needed for GCR and stackdriver logging/monitoring/tracing needs. oauth_scopes = [ https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring , https://www.googleapis.com/auth/servicecontrol , https://www.googleapis.com/auth/service.management.readonly , https://www.googleapis.com/auth/trace.append ] // Enable GKE Sandbox (Gvisor) on this node pool. This cannot be set on the // only node pool in the cluster as system workloads are not all compatible // with the restrictions and protections offered by gvisor. sandbox_config { sandbox_type = gvisor } // Protect node metadata and enable Workload Identity // for this node pool. SECURE just protects the metadata. // EXPOSE or not set allows for cluster takeover. // GKE_METADATA_SERVER specifies that each pod s requests to the metadata // API for credentials should be intercepted and given the specific // credentials for that pod only and not the node s. workload_metadata_config { node_metadata = GKE_METADATA_SERVER } metadata = { // Set metadata on the VM to supply more entropy google-compute-enable-virtio-rng = true // Explicitly remove GCE legacy metadata API endpoint to prevent most SSRF // bugs in apps running on pods inside the cluster from giving attackers // a path to pull the GKE metadata/bootstrapping credentials. disable-legacy-endpoints = true } } }","title":"Infrastructure as Code"},{"location":"cluster_lifecycle/#resources","text":"Terraform Terraform Cluster Provider Terraform Node Pool Provider","title":"Resources"},{"location":"cluster_lifecycle/#upgrades-and-security-fixes","text":"","title":"Upgrades and Security Fixes"},{"location":"cluster_lifecycle/#control-plane-upgrades","text":"One of the most important problems that GKE helps solve is maintaining the control plane components for you. Using gcloud or the UI console, upgrading the control plane version is a single API call. There are a few important concepts to understand with this feature: The version of the control plane can and should be kept updated. - GKE supports the current \"Generally Available\" (GA) GKE version back to two older minor revisions. For instance, if the GA version is 1.13.7-gke19 , then 1.11.x and 1.12.x are supported. GKE versions tend to trail Kubernetes OSS slightly. - Kubernetes OSS releases a minor revision approximately every 90-120 days, and GKE is quick to offer \"Alpha\" releases with those newest versions, but it may take some time to graduate to \"GA\" in GKE. Control Plane upgrades of Zonal clusters incurs a few minutes of downtime. - Upgrading to a newer GKE version causes each control plane instance to be recreated in-place. On zonal clusters, there is a few minutes where that single instance is unavailable while it is being recreated. The data in etcd is kept as-is and all workloads that are running on the node pools remain in place. However, access to the API with kubectl will be temporarily halted. Regional clusters perform a \"rolling upgrade\" approach to upgrading the three control plane instances, and so the upgrade happens one instance at a time. This leaves the control plane available on the other instances and should incur no noticable downtime. For the sake of a highly-available control plane and the bonus that is no additional cost, it's recommended to run regional clusters. If inter-regional traffic costs are a concern, use the node locations setting to use a single zone but still keep the regional control plane.","title":"Control Plane Upgrades"},{"location":"cluster_lifecycle/#node-pool-upgrades","text":"Upgrades of node pools are handled per- node pool , and this gives the operator a lot of control over the behavior. If auto-upgrades are enabled, the node pools will be scheduled for \"rolling upgrade\" style upgrades during the configured maintenance window time block. While it's possible to have node pools trail the control plane version by two minor revisions, it's best to not trail more than one. As node pools are where workloads are running, a few points are important to understand: Node Pools are resource boundaries - If certain workloads are resource-intensive or prone to over-consuming resources in large bursts, it may make sense to dedicate a node pool to them to reduce the potential for negatively affecting the performance and availability of other workloads. Each Node Pool adds operational maintenance overhead - Quite simply, each node pool adds more work to the operations team. Either to ensure is automatically upgraded or to perform the upgrades manually if auto-upgrades are disabled. Separating workloads to different Node Pools can help support security goals - While not a perfect solution, ensuring that workloads handling certain sensitive data are scheduled on separate node pools (using node taints and tolerations on workloads) can help reduce the chance of compromising secrets in a container \"escape-to-the-host\" situation. For example, placing PCI-related workloads on a separate node pool from other non-PCI workloads means that a compromise of a non-PCI pod that allows access to the underlying host will be less likely to have access to secrets that the PCI workloads are using.","title":"Node Pool Upgrades"},{"location":"cluster_lifecycle/#security-bulletins","text":"Another benefit of a managed service like GKE is the reduced operational security burden on having to triage, test, and manage upgrades due to security issues. The GKE Security Bulletins website and Atom/RSS feed provide a consolidated view of all things security-related in GKE. It is strongly recommended that teams subscribe to and receive automatic updates from the feed as the information is timely and typically very clear on which actions GKE the service is taking and which actions are left to you.","title":"Security Bulletins"},{"location":"cluster_lifecycle/#resources_1","text":"GKE Cluster Upgrades GKE Node Pool Upgrades Automatic Node Upgrades GKE Security Bulletins GKE Release Notes","title":"Resources"},{"location":"cluster_lifecycle/#scaling","text":"While scaling isn't a purely security-related topic, there are a few security and resource availability concerns with each of the common scaling patterns: Node Sizing - The resources available to GKE worker nodes are shared by all pods that are scheduled on them. While there are solid limits on CPU and RAM usage, there are not strong resource isolation mechanisms (yet) for disk usage that lands on the node . It is possible for a single pod to consume enough disk space and/or inodes and disrupt the health of the node . Depending on the workload profile you plan on running in the cluster , it might make sense to have more nodes of a smaller instance type with the faster pd-ssd disk type than fewer large instances sharing the slower pd-hdd disk type , for example. When workload resource profiles drastically differ, it might make sense to use separate node pools or even separate clusters . Horizontal Pod Autoscaler - The built-in capability of scaling replicas in a deployment based on CPU usage may or may not be granular enough for your needs. Ensure that the total capacity of the cluster (capacity of the max number of nodes allowed by autoscaling) is greater than the resources used by the max number of replicas in the deployment . Also, consider using \"external\" metrics from Stackdriver like \"number of tasks in a pub/sub queue\" or \"web requests per pod\" to be a more accurate and efficient basis for scaling your workloads. Vertical Pod Autoscaler - Horizontal Pod Autoscaling adds more replicas as needed, but that is assuming the pod has properly set resource requests and limits for CPU and RAM set. Each pod should be configured with the proper requests for how much CPU and RAM it uses plus 5-10% at idle for its requests setting. The limit should be the maximum CPU and RAM that the pod will ever use. Having accurate requests and limits set for every pod gives the scheduler the correct information for how to place workloads, increases efficiency of resource usage, and reduces the risk of \"noisy neighbor\" issues from pods that consume more than their set share of resources. VPA runs inside the cluster and can monitor workloads for configured requests and limits in comparison with actual usage, and can either \"audit\" (just make recommendations) or actually modify the deployments dynamically. It's recommended to run VPA in a cluster in \"audit\" mode to help find misconfigured pods before they cause cluster -wide outages. Node Autoprovisioner - The GKE cluster-autoscaler adds or removes nodes from existing node pools based on their available capacity. If a deployment needs more replicas , there are not enough running nodes to handle them, the node pool is configured to autoscale, and there are more nodes left before the maximum is reached, the autoscaler will add nodes until the workloads are scheduled. But what if a pod is asking for resources that a new node can't handle? For example, a pod that asks for 16 cpu cores but the nodes are only 8 core instances? Or if a pod needs a GPU but the node pool doesn't attach GPUs? The node autoprovisioner can be configured to dynamically manage node pools for you to help satisfy these situations. Instead of waiting for an administrator to add a new node pool , the cluster autoscaler can do that for you.","title":"Scaling"},{"location":"cluster_lifecycle/#resources_2","text":"GKE Cluster Sizing GKE Cluster Resizing Horizontal Pod Autoscaler Vertical Pod Autoscaler GKE Node Auto-Provisioner","title":"Resources"},{"location":"cluster_observability/","text":"Cluster Observability Note This section only refers to GKE Clusters using the newer \"Stackdriver Kubernetes Engine Logging\" and \"Stackdriver Kubernetes Engine Monitoring\" mechanism. If you are using Terraform, the value for the logging_service should be logging.googleapis.com/kubernetes and monitoring_service should be monitoring.googleapis.com/kubernetes . Logging GKE Logs When logging.googleapis.com/kubernetes is enabled for a cluster in a project , the following resource_type s now appear in Stackdriver: GKE Cluster Operations - resource.type=\"gke_cluster\" Captures actions like CreateCluster and DeleteCluster similar to what is shown via gcloud container operations list . Use this to know when clusters are created/updated/deleted (including the full object specification details/settings), who performed that action, if it was authorized, and more. Add protoPayload.authorizationInfo.granted!=true to the filter to see all unauthorized attempts. GKE NodePool Operations - resource.type=\"gke_nodepool\" Similar to GKE Cluster Operations but specific to operations on node pools . gcloud container operations list mixes both cluster and node pool operations into a single output for ease of monitoring upgrades from one gcloud command. Kubernetes Cluster - resource.type=\"k8s_cluster\" Captures actions against the Kubernetes API Server/Control Plane of all clusters in the project . Add the filter resource.labels.cluster_name=\"standard-cluster-2\" resource.labels.location=\"us-central1-a\" to limit the logs to a specific cluster in a specific location (region or zone). Use this to know when Kubernetes resources are created/updated/deleted (including the full object specification details), who performed that action, if it was authorized, and more. Add labels.\"authorization.k8s.io/decision\"=\"forbid\" or \"allow\" to the filter to see which actions were permitted by RBAC inside the cluster. When paired with protoPayload.authenticationInfo.principalEmail=\"myemail@domain.com\" , this filter can help with troubleshooting RBAC failures during the development of RBAC ClusterRoles and Roles . You might want to configure filters and sinks/exports for all RBAC failures and look for anomalies/failures of service accounts. For instance, if protoPayload.authenticationInfo.principalEmail=\"kubelet\" does not have a UserAgent like: protoPayload.requestMetadata.callerSuppliedUserAgent=\"kubelet/v1.13.7 (linux/amd64) kubernetes/7d3d6f1\" , but instead one corresponding to curl or kubectl , it's a near-guarantee that a malicious actor has stolen and is using the kubelet 's credentials. Kubernetes Node - resource.type=\"k8s_node\" Captures all logs exported from the GCE Instance operating system for all nodes in all clusters in the project . In the case of COS , these are systemd logs. Add the filter jsonPayload._SYSTEMD_UNIT=\"kubelet.service\" to see logs from the kubelet systemd unit, and add a filter like resource.labels.node_name=\"gke-standard-cluster-1-default-pool-de92f9ef-qwpq\" to limit the scope to a single node . Kubernetes Pod - resource.type=\"k8s_pod\" Captures pod \"events\" logs that describe the lifecycle/operations of pods . If you run kubectl describe pod podname and view the \"Events\" section manually during troubleshooting, these will be familiar. It's important to note that the \"Events\" logs in Kubernetes do not persist after a certain period of time, so shipping these to Stackdriver allows for diagnosing issues after the fact. Adding the filter resource.labels.pod_name=\"mynginx-5b97b974b6-6x589\" resource.labels.cluster_name=\"standard-cluster-2\" resource.labels.location=\"us-central1-a\" resource.labels.namespace_name=\"default\" allows for narrowing things down to a specific pod in a specific namespace in a specific cluster . Kubernetes Container - resource.type=\"k8s_container\" Captures the per- container logs that your application emits. The same output as if you run kubectl logs mypod -c mycontainer . In addition to the cluster / namespace / pod filters also used by the pod logs, add resource.labels.container_name=\"mycontainer\" to get the logs from a specific container . GCE Logs for GKE Clusters GCE Instance Group - resource.type=\"gce_instance_group\" Logs when GKE worker instances are added/removed from the node pool instance group. GCE Instance Group Manager - resource.type=\"\" Logs for GCE Instance group operations success/failure. GCE Instance Template - resource.type=\"\" Logs for when the instanceTemplate for a node pool instance group is inserted/updated/deleted. GCE VM Instance - resource.type=\"gce_instance\" Logs when GKE worker instances are inserted/deleted, their secure boot attestation output status, and more. GCE Disk - resource.type=\"gce_disk\" Shows GKE specific operations on the underlying GCE VM Disks. GCE Subnetwork - resource.type=\"gce_subnetwork\" Shows GKE specific operations on the subnets when clusters and node pools are modified. Resources GKE Audit Logging Monitoring Node Metrics Kubernetes State Metrics Workload Metrics","title":"Cluster Observability"},{"location":"cluster_observability/#cluster-observability","text":"Note This section only refers to GKE Clusters using the newer \"Stackdriver Kubernetes Engine Logging\" and \"Stackdriver Kubernetes Engine Monitoring\" mechanism. If you are using Terraform, the value for the logging_service should be logging.googleapis.com/kubernetes and monitoring_service should be monitoring.googleapis.com/kubernetes .","title":"Cluster Observability"},{"location":"cluster_observability/#logging","text":"","title":"Logging"},{"location":"cluster_observability/#gke-logs","text":"When logging.googleapis.com/kubernetes is enabled for a cluster in a project , the following resource_type s now appear in Stackdriver: GKE Cluster Operations - resource.type=\"gke_cluster\" Captures actions like CreateCluster and DeleteCluster similar to what is shown via gcloud container operations list . Use this to know when clusters are created/updated/deleted (including the full object specification details/settings), who performed that action, if it was authorized, and more. Add protoPayload.authorizationInfo.granted!=true to the filter to see all unauthorized attempts. GKE NodePool Operations - resource.type=\"gke_nodepool\" Similar to GKE Cluster Operations but specific to operations on node pools . gcloud container operations list mixes both cluster and node pool operations into a single output for ease of monitoring upgrades from one gcloud command. Kubernetes Cluster - resource.type=\"k8s_cluster\" Captures actions against the Kubernetes API Server/Control Plane of all clusters in the project . Add the filter resource.labels.cluster_name=\"standard-cluster-2\" resource.labels.location=\"us-central1-a\" to limit the logs to a specific cluster in a specific location (region or zone). Use this to know when Kubernetes resources are created/updated/deleted (including the full object specification details), who performed that action, if it was authorized, and more. Add labels.\"authorization.k8s.io/decision\"=\"forbid\" or \"allow\" to the filter to see which actions were permitted by RBAC inside the cluster. When paired with protoPayload.authenticationInfo.principalEmail=\"myemail@domain.com\" , this filter can help with troubleshooting RBAC failures during the development of RBAC ClusterRoles and Roles . You might want to configure filters and sinks/exports for all RBAC failures and look for anomalies/failures of service accounts. For instance, if protoPayload.authenticationInfo.principalEmail=\"kubelet\" does not have a UserAgent like: protoPayload.requestMetadata.callerSuppliedUserAgent=\"kubelet/v1.13.7 (linux/amd64) kubernetes/7d3d6f1\" , but instead one corresponding to curl or kubectl , it's a near-guarantee that a malicious actor has stolen and is using the kubelet 's credentials. Kubernetes Node - resource.type=\"k8s_node\" Captures all logs exported from the GCE Instance operating system for all nodes in all clusters in the project . In the case of COS , these are systemd logs. Add the filter jsonPayload._SYSTEMD_UNIT=\"kubelet.service\" to see logs from the kubelet systemd unit, and add a filter like resource.labels.node_name=\"gke-standard-cluster-1-default-pool-de92f9ef-qwpq\" to limit the scope to a single node . Kubernetes Pod - resource.type=\"k8s_pod\" Captures pod \"events\" logs that describe the lifecycle/operations of pods . If you run kubectl describe pod podname and view the \"Events\" section manually during troubleshooting, these will be familiar. It's important to note that the \"Events\" logs in Kubernetes do not persist after a certain period of time, so shipping these to Stackdriver allows for diagnosing issues after the fact. Adding the filter resource.labels.pod_name=\"mynginx-5b97b974b6-6x589\" resource.labels.cluster_name=\"standard-cluster-2\" resource.labels.location=\"us-central1-a\" resource.labels.namespace_name=\"default\" allows for narrowing things down to a specific pod in a specific namespace in a specific cluster . Kubernetes Container - resource.type=\"k8s_container\" Captures the per- container logs that your application emits. The same output as if you run kubectl logs mypod -c mycontainer . In addition to the cluster / namespace / pod filters also used by the pod logs, add resource.labels.container_name=\"mycontainer\" to get the logs from a specific container .","title":"GKE Logs"},{"location":"cluster_observability/#gce-logs-for-gke-clusters","text":"GCE Instance Group - resource.type=\"gce_instance_group\" Logs when GKE worker instances are added/removed from the node pool instance group. GCE Instance Group Manager - resource.type=\"\" Logs for GCE Instance group operations success/failure. GCE Instance Template - resource.type=\"\" Logs for when the instanceTemplate for a node pool instance group is inserted/updated/deleted. GCE VM Instance - resource.type=\"gce_instance\" Logs when GKE worker instances are inserted/deleted, their secure boot attestation output status, and more. GCE Disk - resource.type=\"gce_disk\" Shows GKE specific operations on the underlying GCE VM Disks. GCE Subnetwork - resource.type=\"gce_subnetwork\" Shows GKE specific operations on the subnets when clusters and node pools are modified.","title":"GCE Logs for GKE Clusters"},{"location":"cluster_observability/#resources","text":"GKE Audit Logging","title":"Resources"},{"location":"cluster_observability/#monitoring","text":"","title":"Monitoring"},{"location":"cluster_observability/#node-metrics","text":"","title":"Node Metrics"},{"location":"cluster_observability/#kubernetes-state-metrics","text":"","title":"Kubernetes State Metrics"},{"location":"cluster_observability/#workload-metrics","text":"","title":"Workload Metrics"},{"location":"container_images/","text":"Container Images Dockerfile Best Practices Base Images Package Vulnerabilities Application Vulnerabilities","title":"Container Images"},{"location":"container_images/#container-images","text":"","title":"Container Images"},{"location":"container_images/#dockerfile-best-practices","text":"","title":"Dockerfile Best Practices"},{"location":"container_images/#base-images","text":"","title":"Base Images"},{"location":"container_images/#package-vulnerabilities","text":"","title":"Package Vulnerabilities"},{"location":"container_images/#application-vulnerabilities","text":"","title":"Application Vulnerabilities"},{"location":"detection_and_response/","text":"Detection and Response Runtime Detection Incident Response Forensics Credential Revocation and Rotation","title":"Detection and Response"},{"location":"detection_and_response/#detection-and-response","text":"","title":"Detection and Response"},{"location":"detection_and_response/#runtime-detection","text":"","title":"Runtime Detection"},{"location":"detection_and_response/#incident-response","text":"","title":"Incident Response"},{"location":"detection_and_response/#forensics","text":"","title":"Forensics"},{"location":"detection_and_response/#credential-revocation-and-rotation","text":"","title":"Credential Revocation and Rotation"},{"location":"project_organization/","text":"Project Organization Project and Environment Separation Using the Hipster Shop as an example workload of eleven coordinating microservices that form a \"service offering\", there are a few different approaches for how to organize GCP projects and GKE clusters . Each has pros and cons from a security perspective. Single GCP Project, Single GKE Cluster, Namespaces per Environment - A single project named my-hipster-shop with a GKE cluster named gke-hipster-shop and three Kubernetes namespaces : hipster-shop-dev , hipster-shop-test , and hipster-shop-prod . Pros Simplest project strategy Simplest network strategy Least expensive cluster strategy Cons Weakest \"isolation\" strategy Weakest \"defense in depth\" strategy Weakest \"resource contention\" strategy Single GCP Project, Three GKE Clusters - A single project named my-hipster-shop with three GKE clusters named gke-hipster-shop-dev , gke-hipster-shop-test , and gke-hipster-shop-prod . A single namespace named hipster-shop is in each cluster . Pros Good \"isolation\" strategy Good \"defense in depth\" strategy Good \"resource contention\" strategy Cons Most complex project strategy Simple network strategy Most expensive cluster strategy Three GCP Projects, Single GKE Cluster per Project - Three projects named my-hipster-shop-dev , my-hipster-shop-test , and my-hipster-shop-prod . In each project , a GKE cluster named gke-hipster-shop- env , and a single namespace named hipster-shop in each cluster . Pros Strongest \"isolation\" strategy Strongest \"defense in depth\" strategy Strongest \"resource contention\" strategy Cons Most complex project strategy Most complex network strategy Most expensive cluster strategy Strategy Descriptions Project Complexity - Although GCP projects are \"free\", this refers to the ongoing maintenance and operational overhead of managing and caring for GCP projects . Managing 5 projects vs 15 vs 150 has challenges that require more sophisticated tooling and processes to do well. Network Complexity - Having all GKE clusters on the same shared VPC network or in separate VPC networks will vary the amount of ongoing maintenance of IP CIDR allocation/consumption, Interconnect/Cloud VPN configuration complexity, and complexity of collecting network related flow logs. Cluster Cost - While the control plane of GKE is free, the additional GKE worker nodes in three clusters vs one plus the additional maintenance cost of managing (upgrading, securing, monitoring) more clusters increases the overall cost. Isolation - Does the configuration leverage harder boundary mechanisms like GCP projects , separate GKE clusters on separate GCE instances , or does it rely on softer boundaries like Kubernetes namespaces ? Defense in Depth - Should a security incident occur, which strategy serves to reduce the available attack surface by default, make lateral movement more difficult to perform successfully, and make malicious activity easier to identify vs normal activity? Resource Contention - Are workloads competing for resources on the same nodes , clusters , or projects ? Is it possible for a single workload to over-consume resources such that the nodes evict pods ? If the cluster autoscales to add more nodes , does that consume all available cpu / memory / disk quota in the project and prevent other clusters from having available resources to autoscale if they need to grow? Best Practices Standardize Early - Understand that you will be building tools and processes implicitly and explicitly around your approach, so choose the one that meets your requirements and be consistent across all your GKE deployments. Automation with infrastructure-as-code tools like Terraform for standardizing configuration and naming conventions of projects and clusters is strongly encouraged. One Cluster per Project - While it is the most expensive approach to operate, it offers the best permissions isolation, defense-in-depth, and resource contention strategy by default. If you are serious about service offering and environment separation and have any compliance requirements, this is the best way to achieve those objectives. One Cluster per Service Offering - Running one cluster per colocated set of services with similar data gravity and redundancy needs is ideal for reducing the \"blast radius\" of an incident. It might seem convenient to place three or four smaller \"production\" services into a single GKE cluster , but consider the scope of an investigation should a compromise occur. All workloads in that cluster and all the data they touch would have to be \"in scope\" for the remediation efforts. Resources Preparing GKE Environments for Production GCP Cost Calculator GKE Pricing Hipster Shop Terraform Separating Tenants The previous section covers the \"cluster per project\" approaches, and this section attempts to guide you through the \"workloads per cluster\" decisions. Two important definitions to cover first are Hard and Soft tenancy as written by Jessie Frazelle: Soft multi-tenancy - multiple users within the same organization in the same cluster . Soft multi-tenancy could have possible bad actors such as people leaving the company, etc. Users are not thought to be actively malicious since they are within the same organization, but potential for accidents or \"evil leaving employees.\" A large focus of soft multi-tenancy is to prevent accidents. Hard multi-tenancy - multiple users, from various places, in the same cluster . Hard multi-tenancy means that anyone on the cluster is thought to be potentially malicious and therefore should not have access to any other tenants resources. From experience, building and operating a cluster with a hard-tenancy use case in mind is very difficult. The tools and capabilties are improving in this area, but it requires extreme attention to detail, careful planning, 100% visibility into activity, and near-hyper active monitoring. For these reasons, your journey with Kubernetes and GKE should first solve for the soft-tenancy use case. The understanding and lessons learned will overlap nearly 100% if you decide to go for hard-tenancy and will absolutely give you the proper frame of reference to decide if your organization can tackle the added challenges. There is no such thing as a \"single-tenant\" cluster When it comes to workloads, there are always a minimum of two classes: \"System\" and \"User\" workloads. Workloads that are responsible for the operation of the cluster (CNI, log export, metrics export, etc) should be isolated from workloads that run actual applications and vice versa. In Kubernetes, the default separation between these workload types is likely not sufficient for production needs. System components run on the same physical resources as user workloads, share a common administrative mechanism, share a common layer 3 network with no default access controls, and often run with higher privileges. Even in GKE, you will want to take steps to address these concerns. API Isolation - Using separate Kubernetes namespaces to segment workloads for different purposes when it comes to how those resources interact with the Kubernetes API only. Service accounts per namespace tied to granular RBAC policies are the primary approach. Network Isolation - Using Kubernetes namespaces as an anchor point, defining which pods are allowed to talk with each other explicitly via NetworkPolicy objects is the primary approach. For instance, preventing all ingress traffic from non- kube-system namespaces with the exception of udp/53 for kube-dns . Privilege Isolation - Leveraging well-formed containers running as non-privileged users in combination with PodSecurityPolicies to prevent user workloads from being able to access sensitive or privileged resources on the worker node and undermining the security of all workloads. Resource Isolation - Using features like ResourceQuotas to cap overal cpu/memory/persistent disk resource consumption, resources requests and limits to ensure pods are given the resources they need without overcrowding other workloads, and separating security or performance sensitive workloads on separate Node Pools . Whether you are a single developer running a couple microservices in one cluster or a large organization with many teams sharing large clusters , these concerns are important and should not be overlooked. The remainder of this guide will attempt to show you how to implement these features in combination and give you confidence in the decisions you make for your use case. Best Practices Tenants per Cluster - From a security perspective, having a single tenant per cluster provides the highest degree of separation among tenants, but it is common to allow multiple workloads from different users/teams of similar trust levels to share a cluster for cost and operational efficiency. Multiple Untrusted Tenants in a Single Cluster - This approach is not generally recommended as the level of effort to sufficiently isolate workloads is high and the risk of a vulnerability or mistake leading to a tenant escape is much higher. Separate the System from the Workloads - No matter which approach is taken, you should take steps to properly isolate the pods and services that control and manage your cluster from the workloads that operate in them. The system components can have permissions to GCP resources outside your cluster , and it's important that an incident with a user workload can't escape to the system workloads and then escape \"outside\" the cluster . Resources Hard Multi-Tenancy in Kubernetes Multi-Tenancy Design Space GKE Cluster Multi-Tenancy Project Quotas GCP projects have quotas or limits on how many resources are available for potential use. This doesn't mean that there is available capacity to fulfill the request, and on rare occasions, a particular zone might not be able to provide another GKE worker right away when your cluster workloads need more capacity and node-pool autoscaling kicks in. GKE uses several GCE related quotas like cpu , memory , disk , and gpu of a particular type, but it also uses lesser known quotas like \"Number of secondary ranges per VPC\". The resource related quotas are per region and sometimes per zone, so it's important to monitor your quota usage to avoid quota-capped resource exhaustion scenarios from negatively affecting your application's performance during a scale-up event or from preventing certain types of upgrade scenarios. Best Practices Monitor Quota Consumption - Smaller GKE clusters will most likely fit into the project quota defaults, but clusters of a dozen nodes or more may start to bump into the limits. Request Quota Increases Ahead of Time - Quota increases can take as much as 48 hrs to be approved, so it's best to plan ahead and ask early. Aim for 110% - If the single GKE cluster in a project uses 10, 32-core nodes , the total cpu cores needed is 320 or more. To give enough headroom to perform a \"blue/green\" cluster upgrade if needed (bringing up an identical cluster in parallel), the project quota should be at least 640 cpu cores in that region to facilitate that approach. Following the 110% guideline, this would actually be more like 700 cpus . This allows for two, full-sized clusters to be possible for a short duration while the workloads are migrated between the clusters . GCP APIs have Rate Limits - If your application makes 100s of requests per second or more to say, GCS or GCR, you may run into rate limits designed to protect overuse of the GCP APIs from a single customer affecting all customers. If you run into these, you may or may not be able to get them increased. Consider working with GCP support and implementing a different approach with your application. Resources GCE Quotas and Limits Working with GCP Quotas","title":"Project Organization"},{"location":"project_organization/#project-organization","text":"","title":"Project Organization"},{"location":"project_organization/#project-and-environment-separation","text":"Using the Hipster Shop as an example workload of eleven coordinating microservices that form a \"service offering\", there are a few different approaches for how to organize GCP projects and GKE clusters . Each has pros and cons from a security perspective. Single GCP Project, Single GKE Cluster, Namespaces per Environment - A single project named my-hipster-shop with a GKE cluster named gke-hipster-shop and three Kubernetes namespaces : hipster-shop-dev , hipster-shop-test , and hipster-shop-prod . Pros Simplest project strategy Simplest network strategy Least expensive cluster strategy Cons Weakest \"isolation\" strategy Weakest \"defense in depth\" strategy Weakest \"resource contention\" strategy Single GCP Project, Three GKE Clusters - A single project named my-hipster-shop with three GKE clusters named gke-hipster-shop-dev , gke-hipster-shop-test , and gke-hipster-shop-prod . A single namespace named hipster-shop is in each cluster . Pros Good \"isolation\" strategy Good \"defense in depth\" strategy Good \"resource contention\" strategy Cons Most complex project strategy Simple network strategy Most expensive cluster strategy Three GCP Projects, Single GKE Cluster per Project - Three projects named my-hipster-shop-dev , my-hipster-shop-test , and my-hipster-shop-prod . In each project , a GKE cluster named gke-hipster-shop- env , and a single namespace named hipster-shop in each cluster . Pros Strongest \"isolation\" strategy Strongest \"defense in depth\" strategy Strongest \"resource contention\" strategy Cons Most complex project strategy Most complex network strategy Most expensive cluster strategy","title":"Project and Environment Separation"},{"location":"project_organization/#strategy-descriptions","text":"Project Complexity - Although GCP projects are \"free\", this refers to the ongoing maintenance and operational overhead of managing and caring for GCP projects . Managing 5 projects vs 15 vs 150 has challenges that require more sophisticated tooling and processes to do well. Network Complexity - Having all GKE clusters on the same shared VPC network or in separate VPC networks will vary the amount of ongoing maintenance of IP CIDR allocation/consumption, Interconnect/Cloud VPN configuration complexity, and complexity of collecting network related flow logs. Cluster Cost - While the control plane of GKE is free, the additional GKE worker nodes in three clusters vs one plus the additional maintenance cost of managing (upgrading, securing, monitoring) more clusters increases the overall cost. Isolation - Does the configuration leverage harder boundary mechanisms like GCP projects , separate GKE clusters on separate GCE instances , or does it rely on softer boundaries like Kubernetes namespaces ? Defense in Depth - Should a security incident occur, which strategy serves to reduce the available attack surface by default, make lateral movement more difficult to perform successfully, and make malicious activity easier to identify vs normal activity? Resource Contention - Are workloads competing for resources on the same nodes , clusters , or projects ? Is it possible for a single workload to over-consume resources such that the nodes evict pods ? If the cluster autoscales to add more nodes , does that consume all available cpu / memory / disk quota in the project and prevent other clusters from having available resources to autoscale if they need to grow?","title":"Strategy Descriptions"},{"location":"project_organization/#best-practices","text":"Standardize Early - Understand that you will be building tools and processes implicitly and explicitly around your approach, so choose the one that meets your requirements and be consistent across all your GKE deployments. Automation with infrastructure-as-code tools like Terraform for standardizing configuration and naming conventions of projects and clusters is strongly encouraged. One Cluster per Project - While it is the most expensive approach to operate, it offers the best permissions isolation, defense-in-depth, and resource contention strategy by default. If you are serious about service offering and environment separation and have any compliance requirements, this is the best way to achieve those objectives. One Cluster per Service Offering - Running one cluster per colocated set of services with similar data gravity and redundancy needs is ideal for reducing the \"blast radius\" of an incident. It might seem convenient to place three or four smaller \"production\" services into a single GKE cluster , but consider the scope of an investigation should a compromise occur. All workloads in that cluster and all the data they touch would have to be \"in scope\" for the remediation efforts.","title":"Best Practices"},{"location":"project_organization/#resources","text":"Preparing GKE Environments for Production GCP Cost Calculator GKE Pricing Hipster Shop Terraform","title":"Resources"},{"location":"project_organization/#separating-tenants","text":"The previous section covers the \"cluster per project\" approaches, and this section attempts to guide you through the \"workloads per cluster\" decisions. Two important definitions to cover first are Hard and Soft tenancy as written by Jessie Frazelle: Soft multi-tenancy - multiple users within the same organization in the same cluster . Soft multi-tenancy could have possible bad actors such as people leaving the company, etc. Users are not thought to be actively malicious since they are within the same organization, but potential for accidents or \"evil leaving employees.\" A large focus of soft multi-tenancy is to prevent accidents. Hard multi-tenancy - multiple users, from various places, in the same cluster . Hard multi-tenancy means that anyone on the cluster is thought to be potentially malicious and therefore should not have access to any other tenants resources. From experience, building and operating a cluster with a hard-tenancy use case in mind is very difficult. The tools and capabilties are improving in this area, but it requires extreme attention to detail, careful planning, 100% visibility into activity, and near-hyper active monitoring. For these reasons, your journey with Kubernetes and GKE should first solve for the soft-tenancy use case. The understanding and lessons learned will overlap nearly 100% if you decide to go for hard-tenancy and will absolutely give you the proper frame of reference to decide if your organization can tackle the added challenges. There is no such thing as a \"single-tenant\" cluster When it comes to workloads, there are always a minimum of two classes: \"System\" and \"User\" workloads. Workloads that are responsible for the operation of the cluster (CNI, log export, metrics export, etc) should be isolated from workloads that run actual applications and vice versa. In Kubernetes, the default separation between these workload types is likely not sufficient for production needs. System components run on the same physical resources as user workloads, share a common administrative mechanism, share a common layer 3 network with no default access controls, and often run with higher privileges. Even in GKE, you will want to take steps to address these concerns. API Isolation - Using separate Kubernetes namespaces to segment workloads for different purposes when it comes to how those resources interact with the Kubernetes API only. Service accounts per namespace tied to granular RBAC policies are the primary approach. Network Isolation - Using Kubernetes namespaces as an anchor point, defining which pods are allowed to talk with each other explicitly via NetworkPolicy objects is the primary approach. For instance, preventing all ingress traffic from non- kube-system namespaces with the exception of udp/53 for kube-dns . Privilege Isolation - Leveraging well-formed containers running as non-privileged users in combination with PodSecurityPolicies to prevent user workloads from being able to access sensitive or privileged resources on the worker node and undermining the security of all workloads. Resource Isolation - Using features like ResourceQuotas to cap overal cpu/memory/persistent disk resource consumption, resources requests and limits to ensure pods are given the resources they need without overcrowding other workloads, and separating security or performance sensitive workloads on separate Node Pools . Whether you are a single developer running a couple microservices in one cluster or a large organization with many teams sharing large clusters , these concerns are important and should not be overlooked. The remainder of this guide will attempt to show you how to implement these features in combination and give you confidence in the decisions you make for your use case.","title":"Separating Tenants"},{"location":"project_organization/#best-practices_1","text":"Tenants per Cluster - From a security perspective, having a single tenant per cluster provides the highest degree of separation among tenants, but it is common to allow multiple workloads from different users/teams of similar trust levels to share a cluster for cost and operational efficiency. Multiple Untrusted Tenants in a Single Cluster - This approach is not generally recommended as the level of effort to sufficiently isolate workloads is high and the risk of a vulnerability or mistake leading to a tenant escape is much higher. Separate the System from the Workloads - No matter which approach is taken, you should take steps to properly isolate the pods and services that control and manage your cluster from the workloads that operate in them. The system components can have permissions to GCP resources outside your cluster , and it's important that an incident with a user workload can't escape to the system workloads and then escape \"outside\" the cluster .","title":"Best Practices"},{"location":"project_organization/#resources_1","text":"Hard Multi-Tenancy in Kubernetes Multi-Tenancy Design Space GKE Cluster Multi-Tenancy","title":"Resources"},{"location":"project_organization/#project-quotas","text":"GCP projects have quotas or limits on how many resources are available for potential use. This doesn't mean that there is available capacity to fulfill the request, and on rare occasions, a particular zone might not be able to provide another GKE worker right away when your cluster workloads need more capacity and node-pool autoscaling kicks in. GKE uses several GCE related quotas like cpu , memory , disk , and gpu of a particular type, but it also uses lesser known quotas like \"Number of secondary ranges per VPC\". The resource related quotas are per region and sometimes per zone, so it's important to monitor your quota usage to avoid quota-capped resource exhaustion scenarios from negatively affecting your application's performance during a scale-up event or from preventing certain types of upgrade scenarios.","title":"Project Quotas"},{"location":"project_organization/#best-practices_2","text":"Monitor Quota Consumption - Smaller GKE clusters will most likely fit into the project quota defaults, but clusters of a dozen nodes or more may start to bump into the limits. Request Quota Increases Ahead of Time - Quota increases can take as much as 48 hrs to be approved, so it's best to plan ahead and ask early. Aim for 110% - If the single GKE cluster in a project uses 10, 32-core nodes , the total cpu cores needed is 320 or more. To give enough headroom to perform a \"blue/green\" cluster upgrade if needed (bringing up an identical cluster in parallel), the project quota should be at least 640 cpu cores in that region to facilitate that approach. Following the 110% guideline, this would actually be more like 700 cpus . This allows for two, full-sized clusters to be possible for a short duration while the workloads are migrated between the clusters . GCP APIs have Rate Limits - If your application makes 100s of requests per second or more to say, GCS or GCR, you may run into rate limits designed to protect overuse of the GCP APIs from a single customer affecting all customers. If you run into these, you may or may not be able to get them increased. Consider working with GCP support and implementing a different approach with your application.","title":"Best Practices"},{"location":"project_organization/#resources_2","text":"GCE Quotas and Limits Working with GCP Quotas","title":"Resources"},{"location":"weaknesses_and_misconfigurations/","text":"Weaknesses and Misconfigurations Legacy Credentials Default Service Account and Project Editor IAM Role Exposed Metadata API Lack of Pod Spec Enforcement Images Allowed from Any Repository Overgranting IAM Permissions","title":"Common Weaknesses and Misconfigurations"},{"location":"weaknesses_and_misconfigurations/#weaknesses-and-misconfigurations","text":"","title":"Weaknesses and Misconfigurations"},{"location":"weaknesses_and_misconfigurations/#legacy-credentials","text":"","title":"Legacy Credentials"},{"location":"weaknesses_and_misconfigurations/#default-service-account-and-project-editor-iam-role","text":"","title":"Default Service Account and Project Editor IAM Role"},{"location":"weaknesses_and_misconfigurations/#exposed-metadata-api","text":"","title":"Exposed Metadata API"},{"location":"weaknesses_and_misconfigurations/#lack-of-pod-spec-enforcement","text":"","title":"Lack of Pod Spec Enforcement"},{"location":"weaknesses_and_misconfigurations/#images-allowed-from-any-repository","text":"","title":"Images Allowed from Any Repository"},{"location":"weaknesses_and_misconfigurations/#overgranting-iam-permissions","text":"","title":"Overgranting IAM Permissions"},{"location":"workload_configuration/","text":"Workload Configuration Trusted Image Repos Deployment Settings and Best Practices Pod Security Policy Network Policy RBAC Dynamic Admission Control Secrets Management Pod Disruption Budgets LimitRanges Resource Control","title":"Workload Configuration"},{"location":"workload_configuration/#workload-configuration","text":"","title":"Workload Configuration"},{"location":"workload_configuration/#trusted-image-repos","text":"","title":"Trusted Image Repos"},{"location":"workload_configuration/#deployment-settings-and-best-practices","text":"","title":"Deployment Settings and Best Practices"},{"location":"workload_configuration/#pod-security-policy","text":"","title":"Pod Security Policy"},{"location":"workload_configuration/#network-policy","text":"","title":"Network Policy"},{"location":"workload_configuration/#rbac","text":"","title":"RBAC"},{"location":"workload_configuration/#dynamic-admission-control","text":"","title":"Dynamic Admission Control"},{"location":"workload_configuration/#secrets-management","text":"","title":"Secrets Management"},{"location":"workload_configuration/#pod-disruption-budgets","text":"","title":"Pod Disruption Budgets"},{"location":"workload_configuration/#limitranges","text":"","title":"LimitRanges"},{"location":"workload_configuration/#resource-control","text":"","title":"Resource Control"}]}